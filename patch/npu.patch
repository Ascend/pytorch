diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/aten/src/ATen/autocast_mode.h patch/aten/src/ATen/autocast_mode.h
--- pytorch-v1.8.1/aten/src/ATen/autocast_mode.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/aten/src/ATen/autocast_mode.h	2022-03-07 18:32:02.084336916 +0800
@@ -5,7 +5,7 @@
 
 namespace {
   bool is_autocast_eligible(const Tensor& tensor) {
-    return (tensor.is_cuda() || tensor.is_xla()) && tensor.is_floating_point();
+    return (tensor.is_cuda() || tensor.is_xla() || tensor.is_npu()) && tensor.is_floating_point();
   }
 } // namespace
 
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/aten/src/ATen/core/VariableFallbackKernel.cpp patch/aten/src/ATen/core/VariableFallbackKernel.cpp
--- pytorch-v1.8.1/aten/src/ATen/core/VariableFallbackKernel.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/aten/src/ATen/core/VariableFallbackKernel.cpp	2022-03-07 18:32:02.084336916 +0800
@@ -48,4 +48,8 @@
   m.fallback(torch::CppFunction::makeFallthrough());
 }
 
+TORCH_LIBRARY_IMPL(_, AutogradNPU, m) {
+  m.fallback(torch::CppFunction::makeFallthrough());
+}
+
 }
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/aten/src/ATen/templates/TensorBody.h patch/aten/src/ATen/templates/TensorBody.h
--- pytorch-v1.8.1/aten/src/ATen/templates/TensorBody.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/aten/src/ATen/templates/TensorBody.h	2022-03-07 18:32:02.192337599 +0800
@@ -342,6 +342,9 @@
 
   /// Returns if a `Tensor` has CUDA backend.
   bool is_cuda() const;
+  
+  /// Returns if a `Tensor` has NPU backend.
+  bool is_npu() const;
 
   /// Returns if a `Tensor` has XPU backend.
   bool is_xpu() const;
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/aten/src/ATen/templates/TensorMethods.cpp patch/aten/src/ATen/templates/TensorMethods.cpp
--- pytorch-v1.8.1/aten/src/ATen/templates/TensorMethods.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/aten/src/ATen/templates/TensorMethods.cpp	2022-03-07 18:32:02.192337599 +0800
@@ -81,6 +81,10 @@
   return impl_->is_cuda();
 }
 
+bool Tensor::is_npu() const {
+  return impl_->is_npu();
+}
+
 bool Tensor::is_xpu() const {
   // NB: this is not a native function to avoid dispatching overhead.
   return impl_->is_xpu();
@@ -116,6 +120,10 @@
   return self.is_cuda();
 }
 
+bool is_npu(Tensor self) {
+  return self.is_npu();
+}
+
 bool is_xla(Tensor self) {
     return self.is_xla();
 }
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/build.sh patch/build.sh
--- pytorch-v1.8.1/build.sh	1970-01-01 08:00:00.000000000 +0800
+++ patch/build.sh	2022-05-10 16:41:28.213285304 +0800
@@ -0,0 +1,124 @@
+CUR_DIR=$(dirname $(readlink -f $0))
+SUPPORTED_PY_VERSION=(3.7 3.8 3.9)
+PY_VERSION='3.7'                     # Default supported python version is 3.7
+DEFAULT_SCRIPT_ARGS_NUM=1            # Default supported input parameters
+
+# Parse arguments inside script
+function parse_script_args() {
+    local args_num=0
+    if [[ "x${1}" = "x" ]]; then
+        # default: bash build.sh (python3.7)
+        return 0
+    fi
+
+    while true; do
+        if [[ "x${1}" = "x" ]]; then
+            break
+        fi
+        if [[ "$(echo "${1}"|cut -b1-|cut -b-2)" == "--" ]]; then
+            args_num=$((args_num+1))
+        fi
+        if [[ ${args_num} -eq ${DEFAULT_SCRIPT_ARGS_NUM} ]]; then
+            break
+        fi
+        shift
+    done
+
+    # if num of args are not fully parsed, throw an error.
+    if [[ ${args_num} -lt ${DEFAULT_SCRIPT_ARGS_NUM} ]]; then
+        return 1
+    fi
+
+    while true; do
+        case "${1}" in
+        --python=*)
+            PY_VERSION=$(echo "${1}"|cut -d"=" -f2)
+            args_num=$((args_num-1))
+            shift
+            ;;
+        -*)
+            echo "ERROR Unsupported parameters: ${1}"
+            return 1
+            ;;
+        *)
+            if [ "x${1}" != "x" ]; then
+                echo "ERROR Unsupported parameters: ${1}"
+                return 1
+            fi
+            break
+            ;;
+        esac
+    done
+
+    # if some "--param=value" are not parsed correctly, throw an error.
+    if [[ ${args_num} -ne 0 ]]; then
+        return 1
+    fi
+}
+
+function check_python_version() {
+    matched_py_version='false'
+    for ver in ${SUPPORTED_PY_VERSION[*]}; do
+        if [ "${PY_VERSION}" = "${ver}" ]; then
+            matched_py_version='true'
+            return 0
+        fi
+    done
+    if [ "${matched_py_version}" = 'false' ]; then
+        echo "${PY_VERSION} is an unsupported python version, we suggest ${SUPPORTED_PY_VERSION[*]}"
+        exit 1
+    fi
+}
+
+function main()
+{
+    if ! parse_script_args "$@"; then
+        echo "Failed to parse script args. Please check your inputs."
+        exit 1
+    fi
+    check_python_version
+
+    # Find matched dependent Python libraries to current Python version in HCCL compiling
+    hccl_file2=${CUR_DIR}/third_party/acl/libs/build_stub.sh
+    if [[ ${PY_VERSION} = '3.7' ]]; then
+        dst_py_ver='3.7m'
+    else
+        dst_py_ver=${PY_VERSION}
+    fi
+    for src_py_ver in ${SUPPORTED_PY_VERSION[*]}; do
+        if [[ ${src_py_ver} = '3.7' ]]; then
+            src_py_ver='3.7m'
+        fi
+
+        if [[ $(grep -c "${src_py_ver}" ${hccl_file2}) -ne 0 && ${src_py_ver} != ${dst_py_ver} ]]; then
+            sed -i "s/libpython${src_py_ver}/libpython${dst_py_ver}/g" ${hccl_file2}
+        fi
+    done
+
+    cd ${CUR_DIR}/third_party/acl/libs
+    # stub
+    dos2unix build_stub.sh
+    chmod +x build_stub.sh
+    ./build_stub.sh
+
+    cd ${CUR_DIR}
+    # if you add or delete file/files in the project, you need to remove the following comment
+    # make clean
+    export TORCH_PACKAGE_NAME=torch
+    export PYTORCH_BUILD_VERSION='1.8.1+ascend'
+    export PYTORCH_BUILD_NUMBER=2
+
+    USE_FBGEMM=0 DEBUG=0 USE_DISTRIBUTED=1 USE_QNNPACK=0 USE_HCCL=1 USE_MKLDNN=0 USE_CUDA=0 USE_NPU=1 BUILD_TEST=0 USE_NNPACK=0 USE_XNNPACK=0 python"${PY_VERSION}" setup.py build bdist_wheel
+    if [ $? != 0 ]; then
+        USE_FBGEMM=0 DEBUG=0 USE_DISTRIBUTED=1 USE_QNNPACK=0 USE_HCCL=1 USE_MKLDNN=0 USE_CUDA=0 USE_NPU=1 BUILD_TEST=0 USE_NNPACK=0 USE_XNNPACK=0 python"${PY_VERSION}" setup.py build bdist_wheel
+        if [ $? != 0 ]; then
+            echo "Failed to compile the wheel file. Please check the source code by yourself."
+            exit 1
+        fi
+        exit 0
+    fi
+
+    exit 0
+}
+
+main "$@"
\ No newline at end of file
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/Backend.h patch/c10/core/Backend.h
--- pytorch-v1.8.1/c10/core/Backend.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/Backend.h	2022-03-08 15:25:36.520231820 +0800
@@ -45,6 +45,7 @@
   QuantizedXPU,
   Undefined,
   MkldnnCPU,
+  NPU,
   NumOptions
 };
 
@@ -64,6 +65,8 @@
       return Backend::SparseCUDA;
     case Backend::SparseHIP:
       return Backend::SparseHIP;
+    case Backend::NPU:
+      throw std::runtime_error("NPU is not support sparse tensor");
     default:
       throw std::runtime_error("Unknown backend");
   }
@@ -71,6 +74,8 @@
 
 static inline Backend toDense(Backend b) {
   switch (b) {
+    case Backend::NPU:
+      return Backend::NPU;
     case Backend::CPU:
       return Backend::CPU;
     case Backend::CUDA:
@@ -105,7 +110,9 @@
 }
 
 static inline Backend dispatchKeyToBackend(DispatchKey t) {
-  if (t == DispatchKey::CPU || t == DispatchKey::AutogradCPU) {
+  if (t == DispatchKey::NPU || t == DispatchKey::AutogradNPU) {
+    return Backend::NPU;
+  } else if (t == DispatchKey::CPU || t == DispatchKey::AutogradCPU) {
     return Backend::CPU;
   } else if (t == DispatchKey::CUDA || t == DispatchKey::AutogradCUDA) {
     return Backend::CUDA;
@@ -148,6 +155,8 @@
 
 static inline DispatchKey backendToDispatchKey(Backend b) {
   switch (b) {
+    case Backend::NPU:
+      return DispatchKey::NPUTensorId;
     case Backend::CPU:
       return DispatchKey::CPU;
     case Backend::CUDA:
@@ -189,6 +198,8 @@
 
 static inline DeviceType backendToDeviceType(Backend b) {
   switch (b) {
+    case Backend::NPU:
+      return DeviceType::NPU;
     case Backend::CPU:
       return DeviceType::CPU;
     case Backend::CUDA:
@@ -229,6 +240,8 @@
 
 static inline Backend backendToCPU(Backend b) {
   switch (b) {
+    case Backend::NPU:
+      return Backend::NPU;
     case Backend::CPU:
       return Backend::CPU;
     case Backend::CUDA:
@@ -340,6 +353,8 @@
 // TODO: This probably shouldn't actually be static inline
 static inline const char* toString(Backend b) {
   switch (b) {
+    case Backend::NPU:
+      return "NPU";
     case Backend::CPU:
       return "CPU";
     case Backend::CUDA:
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/Device.cpp patch/c10/core/Device.cpp
--- pytorch-v1.8.1/c10/core/Device.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/Device.cpp	2022-03-07 18:32:02.216337752 +0800
@@ -46,6 +46,7 @@
           {"msnpu", DeviceType::MSNPU},
           {"xla", DeviceType::XLA},
           {"vulkan", DeviceType::Vulkan},
+		      {"npu", DeviceType::NPU},
       }};
   auto device = std::find_if(
       types.begin(),
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/Device.h patch/c10/core/Device.h
--- pytorch-v1.8.1/c10/core/Device.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/Device.h	2022-03-07 18:32:02.216337752 +0800
@@ -85,6 +85,11 @@
   bool is_xpu() const noexcept {
     return type_ == DeviceType::XPU;
   }
+  
+  /// Return true if the device is of NPU type.
+  bool is_npu() const noexcept {
+    return type_ == DeviceType::NPU;
+  }
 
   /// Return true if the device is of CPU type.
   bool is_cpu() const noexcept {
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/DeviceType.cpp patch/c10/core/DeviceType.cpp
--- pytorch-v1.8.1/c10/core/DeviceType.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/DeviceType.cpp	2022-03-07 18:32:02.216337752 +0800
@@ -33,6 +33,8 @@
       return lower_case ? "metal" : "METAL";
     case DeviceType::XPU:
       return lower_case ? "xpu" : "XPU";
+	  case DeviceType::NPU:
+      return lower_case ? "npu" : "NPU";
     default:
       AT_ERROR(
           "Unknown device: ",
@@ -68,6 +70,7 @@
     case DeviceType::Vulkan:
     case DeviceType::Metal:
     case DeviceType::XPU:
+	  case DeviceType::NPU:
       return true;
     default:
       return false;
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/DeviceType.h patch/c10/core/DeviceType.h
--- pytorch-v1.8.1/c10/core/DeviceType.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/DeviceType.h	2022-03-07 18:32:02.216337752 +0800
@@ -26,15 +26,17 @@
   Vulkan = 10, // Vulkan
   Metal = 11, // Metal
   XPU = 12, // XPU
+  NPU = 13, // NPU
   // NB: If you add more devices:
   //  - Change the implementations of DeviceTypeName and isValidDeviceType
   //    in DeviceType.cpp
   //  - Change the number below
-  COMPILE_TIME_MAX_DEVICE_TYPES = 13,
+  COMPILE_TIME_MAX_DEVICE_TYPES = 14,
 };
 
 constexpr DeviceType kCPU = DeviceType::CPU;
 constexpr DeviceType kCUDA = DeviceType::CUDA;
+constexpr DeviceType kNPU = DeviceType::NPU;
 constexpr DeviceType kHIP = DeviceType::HIP;
 constexpr DeviceType kFPGA = DeviceType::FPGA;
 constexpr DeviceType kMSNPU = DeviceType::MSNPU;
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/DispatchKey.cpp patch/c10/core/DispatchKey.cpp
--- pytorch-v1.8.1/c10/core/DispatchKey.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/DispatchKey.cpp	2022-03-08 15:25:36.520231820 +0800
@@ -6,7 +6,8 @@
   switch (t) {
     case DispatchKey::Undefined:
       return "Undefined";
-
+    case DispatchKey::NPU:
+      return "NPU";
     case DispatchKey::CPU:
       return "CPU";
     case DispatchKey::CUDA:
@@ -80,6 +81,8 @@
       return "AutogradCUDA";
     case DispatchKey::AutogradXLA:
       return "AutogradXLA";
+    case DispatchKey::AutogradNPU:
+      return "AutogradNPU";
     case DispatchKey::AutogradNestedTensor:
       return "AutogradNestedTensor";
     case DispatchKey::AutogradPrivateUse1:
@@ -143,6 +146,8 @@
       return DispatchKey::AutogradCUDA;
     case DispatchKey::XLA:
       return DispatchKey::AutogradXLA;
+    case DispatchKey::NPU:
+      return DispatchKey::AutogradNPU;
     case DispatchKey::NestedTensor:
       return DispatchKey::AutogradNestedTensor;
     case DispatchKey::PrivateUse1:
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/DispatchKey.h patch/c10/core/DispatchKey.h
--- pytorch-v1.8.1/c10/core/DispatchKey.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/DispatchKey.h	2022-03-07 18:32:02.216337752 +0800
@@ -114,6 +114,7 @@
   // Here are reserved backends for user-defined backends, see Note [Private use
   // DispatchKey]
   // To see some example about how to use this, check out MSNPU
+  NPU,
   PrivateUse1,
   PrivateUse2,
   PrivateUse3,
@@ -224,6 +225,7 @@
   AutogradCPU,
   AutogradCUDA,
   AutogradXLA,
+  AutogradNPU,
   AutogradNestedTensor, // lives out of tree at
                         // https://github.com/pytorch/nestedtensor
   AutogradXPU,
@@ -297,6 +299,7 @@
   // be used
   CPUTensorId = CPU,
   CUDATensorId = CUDA,
+  NPUTensorId = NPU,
   PrivateUse1_PreAutograd = AutogradPrivateUse1,
   PrivateUse2_PreAutograd = AutogradPrivateUse2,
   PrivateUse3_PreAutograd = AutogradPrivateUse3,
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/DispatchKeySet.cpp patch/c10/core/DispatchKeySet.cpp
--- pytorch-v1.8.1/c10/core/DispatchKeySet.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/DispatchKeySet.cpp	2022-03-07 18:32:02.216337752 +0800
@@ -11,6 +11,7 @@
         DispatchKey::XLA,
         DispatchKey::NestedTensor,
         DispatchKey::XPU,
+		    DispatchKey::NPU,
         DispatchKey::PrivateUse1,
         DispatchKey::PrivateUse2,
         DispatchKey::PrivateUse3,
@@ -48,6 +49,8 @@
       return DispatchKeySet(DispatchKey::CUDA);
     case DispatchKey::AutogradXLA:
       return DispatchKeySet(DispatchKey::XLA);
+    case DispatchKey::AutogradNPU:
+      return DispatchKeySet(DispatchKey::NPU);
     case DispatchKey::AutogradNestedTensor:
       return DispatchKeySet(DispatchKey::NestedTensor);
     case DispatchKey::AutogradXPU:
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/DispatchKeySet.h patch/c10/core/DispatchKeySet.h
--- pytorch-v1.8.1/c10/core/DispatchKeySet.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/DispatchKeySet.h	2022-03-07 18:32:02.216337752 +0800
@@ -193,6 +193,7 @@
     DispatchKey::AutogradCPU,
     DispatchKey::AutogradCUDA,
     DispatchKey::AutogradXLA,
+    DispatchKey::AutogradNPU,
     DispatchKey::AutogradNestedTensor,
     DispatchKey::AutogradXPU,
     DispatchKey::AutogradPrivateUse1,
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/StorageImpl.h patch/c10/core/StorageImpl.h
--- pytorch-v1.8.1/c10/core/StorageImpl.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/StorageImpl.h	2022-04-28 18:51:47.202663489 +0800
@@ -7,7 +7,7 @@
 
 namespace c10 {
 
-struct C10_API StorageImpl final : public c10::intrusive_ptr_target {
+struct C10_API StorageImpl : public c10::intrusive_ptr_target {
  public:
   struct use_byte_size_t {};
 
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/TensorImpl.h patch/c10/core/TensorImpl.h
--- pytorch-v1.8.1/c10/core/TensorImpl.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/TensorImpl.h	2022-03-08 15:25:36.520231820 +0800
@@ -508,6 +508,10 @@
         key_set_.has(DispatchKey::SparseCUDA) ||
         key_set_.has(DispatchKey::QuantizedCUDA);
   }
+  
+  bool is_npu() const {
+    return key_set_.has(DispatchKey::NPU);
+  }
 
   bool is_xpu() const {
     // NB: This method is not virtual and avoid dispatches for performance
@@ -997,7 +1001,7 @@
    */
   inline bool has_compatible_shallow_copy_type(DispatchKeySet from) {
     auto is_dense = [](DispatchKeySet ts) {
-      return ts.has(DispatchKey::CPU) || ts.has(DispatchKey::CUDA) ||
+      return ts.has(DispatchKey::NPU) || ts.has(DispatchKey::CPU) || ts.has(DispatchKey::CUDA) ||
           ts.has(DispatchKey::HIP) || ts.has(DispatchKey::XPU);
     };
     auto is_sparse = [](DispatchKeySet ts) {
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/c10/core/TensorOptions.h patch/c10/core/TensorOptions.h
--- pytorch-v1.8.1/c10/core/TensorOptions.h	2022-03-05 10:10:10.000000000 +0800
+++ patch/c10/core/TensorOptions.h	2022-03-08 15:25:36.520231820 +0800
@@ -595,6 +595,8 @@
       case Layout::Strided: {
         const auto dtype_ = dtype_or_default(dtype);
         switch (device_.type()) {
+          case DeviceType::NPU:
+            return DispatchKey::NPU;
           case DeviceType::CPU: {
             if (isQIntType(dtype_)) {
               return DispatchKey::QuantizedCPU;
@@ -665,7 +667,9 @@
 // We deliberately ignore handling AutogradCPU/CUDA/XLA... keys to
 // avoid adding asymmetry in device <--> Autograd dispatch key mapping.
 inline DeviceType computeDeviceType(DispatchKey tid) {
-  if (tid == DispatchKey::CPU) {
+  if (tid == DispatchKey::NPU) {
+    return DeviceType::NPU;
+  } else if (tid == DispatchKey::CPU) {
     return DeviceType::CPU;
   } else if (tid == DispatchKey::CUDA) {
     return DeviceType::CUDA;
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/cmake/BuildVariables.cmake patch/cmake/BuildVariables.cmake
--- pytorch-v1.8.1/cmake/BuildVariables.cmake	2022-03-05 10:10:10.000000000 +0800
+++ patch/cmake/BuildVariables.cmake	2022-03-07 18:32:02.340338536 +0800
@@ -11,6 +11,7 @@
 # CMakeLists.txt files under each folder respectively.
 set(Caffe2_CPU_SRCS)
 set(Caffe2_GPU_SRCS)
+set(Caffe2_NPU_SRCS)
 
 # Caffe2_{CPU,GPU}_TEST_SRCS is the list that will have all the related source
 # files for CPU and GPU tests respectively.
@@ -21,10 +22,12 @@
 # directories for CPU and GPU respectively.
 set(Caffe2_CPU_INCLUDE)
 set(Caffe2_GPU_INCLUDE)
+set(Caffe2_NPU_INCLUDE)
 
 # Lists for Caffe2 dependency libraries, for CPU and CUDA respectively.
 set(Caffe2_DEPENDENCY_LIBS "")
 set(Caffe2_CUDA_DEPENDENCY_LIBS "")
+set(Caffe2_NPU_DEPENDENCY_LIBS "")
 # This variable contains dependency libraries of Caffe2 which requires whole
 # symbol linkage. One example is the onnx lib where we need all its schema
 # symbols. However, if the lib is whole linked in caffe2 lib, we don't want
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/cmake/Codegen.cmake patch/cmake/Codegen.cmake
--- pytorch-v1.8.1/cmake/Codegen.cmake	2022-03-05 10:10:10.000000000 +0800
+++ patch/cmake/Codegen.cmake	2022-03-07 18:32:02.340338536 +0800
@@ -214,7 +214,7 @@
   file(MAKE_DIRECTORY ${CMAKE_BINARY_DIR}/aten/src/ATen)
   file(MAKE_DIRECTORY ${CMAKE_BINARY_DIR}/aten/src/ATen/core)
 
-  add_custom_command(OUTPUT ${generated_cpp} ${cuda_generated_cpp} ${core_generated_cpp}
+  add_custom_command(OUTPUT ${generated_cpp} ${cuda_generated_cpp} ${core_generated_cpp} ${npu_generated_cpp}
     COMMAND ${GEN_COMMAND}
     DEPENDS ${all_python} ${all_templates}
       ${CMAKE_CURRENT_LIST_DIR}/../aten/src/ATen/native/native_functions.yaml
@@ -226,10 +226,13 @@
   # on building the generated ATen files to workaround.
   add_custom_target(ATEN_CPU_FILES_GEN_TARGET DEPENDS ${generated_cpp} ${core_generated_cpp})
   add_custom_target(ATEN_CUDA_FILES_GEN_TARGET DEPENDS ${cuda_generated_cpp})
+  add_custom_target(ATEN_NPU_FILES_GEN_TARGET DEPENDS ${npu_generated_cpp})
   add_library(ATEN_CPU_FILES_GEN_LIB INTERFACE)
   add_library(ATEN_CUDA_FILES_GEN_LIB INTERFACE)
+  add_library(ATEN_NPU_FILES_GEN_LIB INTERFACE)
   add_dependencies(ATEN_CPU_FILES_GEN_LIB ATEN_CPU_FILES_GEN_TARGET)
   add_dependencies(ATEN_CUDA_FILES_GEN_LIB ATEN_CUDA_FILES_GEN_TARGET)
+  add_dependencies(ATEN_NPU_FILES_GEN_LIB ATEN_NPU_FILES_GEN_TARGET)
 endif()
 
 function(append_filelist name outputvar)
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/cmake/Dependencies.cmake patch/cmake/Dependencies.cmake
--- pytorch-v1.8.1/cmake/Dependencies.cmake	2022-03-05 10:10:10.000000000 +0800
+++ patch/cmake/Dependencies.cmake	2022-05-10 16:41:28.213285304 +0800
@@ -1771,6 +1771,11 @@
   endif(NOT C_HAS_THREAD)
 endif()
 
+# ---[ NPU
+if(USE_NPU)
+  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/../third_party/acl)
+endif()
+
 #
 # End ATen checks
 #
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/cmake/Summary.cmake patch/cmake/Summary.cmake
--- pytorch-v1.8.1/cmake/Summary.cmake	2022-03-05 10:10:10.000000000 +0800
+++ patch/cmake/Summary.cmake	2022-03-07 18:32:02.344338561 +0800
@@ -127,6 +127,7 @@
     message(STATUS "  USE_MKLDNN_CBLAS      : ${USE_MKLDNN_CBLAS}")
   endif()
   message(STATUS "  USE_NCCL              : ${USE_NCCL}")
+  message(STATUS "  USE_HCCL              : ${USE_HCCL}")
   if(${USE_NCCL})
     message(STATUS "    USE_SYSTEM_NCCL     : ${USE_SYSTEM_NCCL}")
   endif()
@@ -162,6 +163,7 @@
   if(NOT "${SELECTED_OP_LIST}" STREQUAL "")
     message(STATUS "  SELECTED_OP_LIST    : ${SELECTED_OP_LIST}")
   endif()
+  message(STATUS "  USE_NPU              : ${USE_NPU}")
   message(STATUS "  USE_DEPLOY           : ${USE_DEPLOY}")
   message(STATUS "  Public Dependencies  : ${Caffe2_PUBLIC_DEPENDENCY_LIBS}")
   message(STATUS "  Private Dependencies : ${Caffe2_DEPENDENCY_LIBS}")
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/CMakeLists.txt patch/CMakeLists.txt
--- pytorch-v1.8.1/CMakeLists.txt	2022-03-05 10:10:10.000000000 +0800
+++ patch/CMakeLists.txt	2022-05-10 16:41:28.213285304 +0800
@@ -261,6 +261,10 @@
     "USE_DISTRIBUTED" OFF)
 option(USE_TBB "Use TBB" OFF)
 option(ONNX_ML "Enable traditional ONNX ML API." ON)
+# TODO: need to add options to disable NPU on other platforms
+option(USE_NPU "Use NPU" ON)
+option(USE_HCCL "Use HCCL" ON)
+
 option(HAVE_SOVERSION "Whether to add SOVERSION to the shared objects" OFF)
 cmake_dependent_option(
     USE_DEPLOY "Build embedded torch::deploy interpreter" OFF
@@ -748,7 +752,19 @@
   else()
     message(ERROR "Code coverage for compiler ${CMAKE_CXX_COMPILER_ID} is unsupported")
   endif()
+endif()
+
+if (USE_NPU)
+    add_definitions(-DUSE_NPU=1)
+endif()
+
+if (USE_HCCL)
+  link_directories(${CMAKE_BINARY_DIR}/../third_party/acl/libs)
+  add_definitions(-DUSE_HCCL=1)
+endif()
 
+if ($ENV{NPU_LOG_ENABLE})
+    add_definitions(-NPU_LOG_ENABLE=1)
 endif()
 
 if(APPLE)
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/setup.py patch/setup.py
--- pytorch-v1.8.1/setup.py	2022-03-05 10:10:10.000000000 +0800
+++ patch/setup.py	2022-05-10 16:41:28.213285304 +0800
@@ -901,6 +901,7 @@
                 'include/caffe2/**/*.h',
                 'include/torch/*.h',
                 'include/torch/csrc/*.h',
+                'include/torch/csrc/generic/*.h',
                 'include/torch/csrc/api/include/torch/*.h',
                 'include/torch/csrc/api/include/torch/data/*.h',
                 'include/torch/csrc/api/include/torch/data/dataloader/*.h',
@@ -940,6 +941,10 @@
                 'include/torch/csrc/jit/tensorexpr/*.h',
                 'include/torch/csrc/onnx/*.h',
                 'include/torch/csrc/utils/*.h',
+                'include/torch/csrc/tensor/*.h',
+                'include/torch/csrc/distributed/c10d/*.h',
+                'include/torch/csrc/distributed/rpc/*.h',
+                'include/torch/csrc/distributed/autograd/**/*.h',
                 'include/pybind11/*.h',
                 'include/pybind11/detail/*.h',
                 'include/TH/*.h*',
@@ -952,6 +957,12 @@
                 'include/THH/*.cuh',
                 'include/THH/*.h*',
                 'include/THH/generic/*.h',
+                # TODO(ascend): the following two acl directories should be removed after the NPU API is enhanced.
+                'include/third_party/acl/inc/acl/*.h',
+                'include/third_party/acl/inc/acl/ops/*.h',
+                'include/third_party/acl/inc/ge/*.h',
+                'include/third_party/acl/inc/graph/*.h',
+                'include/third_party/acl/inc/op_proto/*.h',
                 'share/cmake/ATen/*.cmake',
                 'share/cmake/Caffe2/*.cmake',
                 'share/cmake/Caffe2/public/*.cmake',
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/tools/generate_torch_version.py patch/tools/generate_torch_version.py
--- pytorch-v1.8.1/tools/generate_torch_version.py	2022-03-05 10:10:10.000000000 +0800
+++ patch/tools/generate_torch_version.py	2022-03-07 18:32:03.076343192 +0800
@@ -17,10 +17,10 @@
 
     if os.getenv('PYTORCH_BUILD_VERSION'):
         assert os.getenv('PYTORCH_BUILD_NUMBER') is not None
-        build_number = int(os.getenv('PYTORCH_BUILD_NUMBER', ""))
+        build_number = int(os.getenv('PYTORCH_BUILD_NUMBER', "0"))
         version = os.getenv('PYTORCH_BUILD_VERSION', "")
-        if build_number > 1:
-            version += '.post' + str(build_number)
+        if build_number > 0:
+            version += '.rc' + str(build_number)
     elif sha != 'Unknown':
         if sha is None:
             sha = get_sha(pytorch_root)
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/_C/_autograd.pyi patch/torch/_C/_autograd.pyi
--- pytorch-v1.8.1/torch/_C/_autograd.pyi	2022-03-05 10:10:10.000000000 +0800
+++ patch/torch/_C/_autograd.pyi	2022-03-07 18:32:03.080343218 +0800
@@ -9,14 +9,17 @@
     CUDA = ...
     NVTX = ...
     KINETO = ...
+    NPU = ...
 
 class ProfilerActivity(Enum):
     CPU = ...
     CUDA = ...
+    NPU = ...
 
 class DeviceType(Enum):
     CPU = ...
     CUDA = ...
+    NPU = ...
     ...
 
 class ProfilerConfig:
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/csrc/autograd/functions/tensor.cpp patch/torch/csrc/autograd/functions/tensor.cpp
--- pytorch-v1.8.1/torch/csrc/autograd/functions/tensor.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/torch/csrc/autograd/functions/tensor.cpp	2022-03-07 18:32:03.104343370 +0800
@@ -26,7 +26,7 @@
       at::DeviceGuard device_guard(src_device);
       // TODO: What if !grad.is_cuda(), but src_device is CUDA?
       // This code is kind of weirdly asymmetric.
-      if (grad.is_cuda() && grad.device() != src_device) {
+    if ((grad.is_cuda() || grad.is_npu()) && grad.device() != src_device) {
         grad_inputs[1] = grad.to(
             src_options,
             /*non_blocking=*/false,
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/csrc/DynamicTypes.cpp patch/torch/csrc/DynamicTypes.cpp
--- pytorch-v1.8.1/torch/csrc/DynamicTypes.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/torch/csrc/DynamicTypes.cpp	2022-05-05 14:32:51.351071624 +0800
@@ -61,9 +61,14 @@
     const at::Storage& storage,
     const caffe2::TypeMeta dtype) {
   at::ScalarType scalarType = at::typeMetaToScalarType(dtype);
-  auto attype = &at::getDeprecatedTypeProperties(
-      at::dispatchKeyToBackend(c10::computeDispatchKey(scalarType, c10::nullopt, storage.device_type())),
-      scalarType);
+  auto backend = at::dispatchKeyToBackend(c10::computeDispatchKey(scalarType, c10::nullopt, storage.device_type()));
+  
+#ifdef USE_NPU
+  if (backend == c10::Backend::NPU) {
+    backend = c10::Backend::CPU;
+  }
+#endif
+  auto attype = &at::getDeprecatedTypeProperties(backend, scalarType);
   auto it = attype_to_py_storage_type.find(attype);
   if (it != attype_to_py_storage_type.end()) {
     return it->second;
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/csrc/tensor/python_tensor.cpp patch/torch/csrc/tensor/python_tensor.cpp
--- pytorch-v1.8.1/torch/csrc/tensor/python_tensor.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/torch/csrc/tensor/python_tensor.cpp	2022-05-05 14:32:51.351071624 +0800
@@ -185,6 +185,7 @@
     case Backend::CUDA: return "torch.cuda";
     case Backend::SparseCPU: return "torch.sparse";
     case Backend::SparseCUDA: return "torch.cuda.sparse";
+    case Backend::NPU: return "torch.npu";
     default: AT_ERROR("invalid backend: ", toString(backend));
   }
 }
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/csrc/utils/tensor_new.cpp patch/torch/csrc/utils/tensor_new.cpp
--- pytorch-v1.8.1/torch/csrc/utils/tensor_new.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/torch/csrc/utils/tensor_new.cpp	2022-05-05 14:32:51.351071624 +0800
@@ -32,6 +32,7 @@
 using at::IntArrayRef;
 using at::kCPU;
 using at::kCUDA;
+using at::kNPU;
 using at::kLong;
 using at::Scalar;
 using at::ScalarType;
@@ -47,6 +48,8 @@
 
 Backend backendToBackendOfDeviceType(Backend b, DeviceType d) {
   switch (d) {
+    case DeviceType::NPU:
+      return Backend::NPU;
     case DeviceType::CPU:
       return backendToCPU(b);
     case DeviceType::CUDA:
@@ -341,6 +344,7 @@
             dispatch_key == c10::DispatchKey::CUDA ||
             dispatch_key == c10::DispatchKey::HIP ||
             dispatch_key == c10::DispatchKey::XLA ||
+            dispatch_key == c10::DispatchKey::NPU ||
             dispatch_key == c10::DispatchKey::XPU,
         "new(): expected DispatchKey: ",
         c10::DispatchKey::CPU,
@@ -351,6 +355,8 @@
         " or ",
         c10::DispatchKey::XLA,
         " or ",
+		    c10::DispatchKey::NPU,
+        " or ",
         c10::DispatchKey::XPU,
         " but got: ",
         dispatch_key);
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/csrc/utils/tensor_types.cpp patch/torch/csrc/utils/tensor_types.cpp
--- pytorch-v1.8.1/torch/csrc/utils/tensor_types.cpp	2022-03-05 10:10:10.000000000 +0800
+++ patch/torch/csrc/utils/tensor_types.cpp	2022-03-07 18:32:03.176343826 +0800
@@ -19,6 +19,7 @@
   switch (backend) {
     case at::Backend::CPU: return "torch";
     case at::Backend::CUDA: return "torch.cuda";
+	case at::Backend::NPU: return "torch.npu";
     case at::Backend::XPU: return "torch.xpu";
     case at::Backend::SparseCPU: return "torch.sparse";
     case at::Backend::SparseCUDA: return "torch.cuda.sparse";
@@ -82,7 +83,7 @@
 std::vector<std::pair<Backend, ScalarType>> all_declared_types() {
   std::vector<std::pair<Backend, ScalarType>> ret;
   // can't easily iterate over enum classes
-  std::vector<Backend> backends = { Backend::CPU, Backend::CUDA, Backend::SparseCPU, Backend::SparseCUDA };
+  std::vector<Backend> backends = { Backend::CPU, Backend::CUDA, Backend::SparseCPU, Backend::SparseCUDA, Backend::NPU };
   std::vector<ScalarType> scalar_types = { ScalarType::Byte, ScalarType::Char, ScalarType::Double, ScalarType::Float,
                                            ScalarType::Int, ScalarType::Long, ScalarType::Short, ScalarType::Half,
                                            ScalarType::Bool, ScalarType::BFloat16};
@@ -92,6 +93,9 @@
       if (scalar_type == ScalarType::Bool && (backend == Backend::SparseCUDA || backend == Backend::SparseCPU)) {
         continue;
       }
+      if (scalar_type == ScalarType::BFloat16 && backend == Backend::NPU) {
+        continue;
+      }
       ret.emplace_back(std::make_pair(backend, scalar_type));
     }
   }
diff -Nur '--exclude=.git*' '--exclude=OWNERS' '--exclude=access_control_test.py' '--exclude=third_party' '--exclude=README*' -Nur pytorch-v1.8.1/torch/npu/__init__.py patch/torch/npu/__init__.py
--- pytorch-v1.8.1/torch/npu/__init__.py	1970-01-01 08:00:00.000000000 +0800
+++ patch/torch/npu/__init__.py	2022-03-07 18:32:03.212344053 +0800
@@ -0,0 +1 @@
+# Empty Module for Npu-type Tensors.
\ No newline at end of file
