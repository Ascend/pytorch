import torch
from torch import distributed as dist
from torch.distributed.fsdp._fully_shard._fsdp_param_group import FSDPParamGroup

import torch_npu


def _patched_finalize_backward(self):
    self._wait_for_post_backward()
    for fsdp_param in self.fsdp_params:
        if fsdp_param.grad_offload_event is not None:
            fsdp_param.grad_offload_event.synchronize()
            fsdp_param.grad_offload_event = None
    if self._all_gather_result is not None:
        # If there was a mistargeted unshard without a corresponding wait,
        # then we wait here and clear the unshard
        if (event := self._all_gather_result.all_gather_event) is not None:
            torch.npu.current_stream().wait_event(event)
        work = self._all_gather_result.all_gather_work
        if isinstance(work, dist.distributed_c10d.Work):
            work.wait()
        self._all_gather_result = None
    self._post_forward_indices.clear()


def _apply_fsdp_patch():
    FSDPParamGroup.finalize_backward = _patched_finalize_backward
