backend: XLA
cpp_namespace: at_npu::native
supported:
  - _cast_Byte
  - _cast_Char
  - _cast_Double
  - _cast_Float
  - _cast_Int
  - _cast_Long
  - _cast_Short
  - _cast_Half
  - _backward
  - set_data
  - data
  - is_leaf
  - output_nr
  - _version
  - requires_grad_
  - retain_grad
  - _fw_primal
  - _make_dual
  - _unpack_dual
  - rename_
  - rename
  - align_to
  - align_to.ellipsis_idx
  - align_as
  - align_tensors
  - ceil
  - ceil_
  - ceil.out
  - refine_names
  - _use_cudnn_ctc_loss
  - _cudnn_ctc_loss
  - _use_cudnn_rnn_flatten_weight
  - _cudnn_rnn_flatten_weight
  - _cudnn_rnn
  - _cudnn_rnn_backward
  - _cudnn_init_dropout_state
  - _debug_has_internal_overlap
  - _fused_dropout
  - _masked_scale
  - _sobol_engine_draw
  - _sobol_engine_ff_
  - _sobol_engine_scramble_
  - _sobol_engine_initialize_state_
  - _reshape_from_tensor
  - _shape_as_tensor
  - dropout
  - dropout_
  - feature_dropout
  - feature_dropout_
  - alpha_dropout
  - alpha_dropout_
  - feature_alpha_dropout
  - feature_alpha_dropout_
  - abs
  - abs_
  - abs.out
  - absolute
  - absolute_
  - absolute.out
  - angle
  - angle.out
  - view_as_real
  - view_as_complex
  - sgn
  - sgn_
  - sgn.out
  - real
  - imag
  - acos
  - acos_
  - acos.out
  - arccos
  - arccos_
  - arccos.out
  - adaptive_avg_pool1d
  - adaptive_max_pool1d
  - adaptive_avg_pool2d.out
  - native_batch_norm
  - native_batch_norm.out
  - native_batch_norm_backward
  - record_stream
  - slow_conv_transpose2d.out
  - slow_conv_transpose2d
  - slow_conv3d.out
  - slow_conv3d
  - slow_conv_transpose3d.out
  - slow_conv_transpose3d
  - add.Tensor
  - add_.Tensor
  - add.out
  - _add_relu.Tensor
  - _add_relu_.Tensor
  - _add_relu.out
  - add.Scalar
  - add_.Scalar
  - addmv
  - addmv_
  - addmv.out
  - set_.source_Storage
  - set_.source_Storage_storage_offset
  - set_.source_Tensor
  - set_
  - _local_scalar_dense
  - resize_
  - as_strided
  - as_strided_
  - cat
  - cat.out
  - cat.names
  - cat.names_out
  - _cat
  - _cat.out
  - clone
  - contiguous
  - convolution_overrideable
  - convolution_backward_overrideable
  - copy_
  - _copy_from
  - div.Tensor
  - div_.Tensor
  - div.out
  - div.Scalar
  - div_.Scalar
  - resize_as_
  - empty.memory_format
  - new_full
  - new_zeros
  - resize_
  - empty.out
  - empty_like
  - empty_strided
  - fill_.Scalar
  - fill_.Tensor
  - hann_window
  - hann_window.periodic
  - hamming_window
  - hamming_window.periodic
  - hamming_window.periodic_alpha
  - hamming_window.periodic_alpha_beta
  - bartlett_window
  - bartlett_window.periodic
  - blackman_window
  - log_softmax.int
  - log_softmax.Dimname
  - _log_softmax
  - _log_softmax_backward_data
  - amin
  - amin.out
  - max
  - max.dim
  - max.dim_max
  - max.names_dim
  - max.names_dim_max
  - amax
  - amax.out
  - mean
  - mean.dim
  - mean.out
  - mean.names_dim
  - mean.names_out
  - min.dim
  - min.dim_min
  - min.names_dim
  - min.names_dim_min
  - min
  - min.out
  - minimum
  - minimum.out
  - blackman_window.periodic
  - mul.Tensor
  - mul_.Tensor
  - mul.out
  - mul.Scalar
  - mul_.Scalar
  - neg
  - neg_
  - neg.out
  - reciprocal
  - reciprocal_
  - reciprocal.out
  - mse_loss.out
  - mse_loss
  - mse_loss_backward.grad_input
  - mse_loss_backward
  - relu
  - relu_
  - slice.Tensor
  - slice_backward
  - softmax.int
  - softmax.Dimname
  - _softmax
  - _softmax_backward_data
  - sum
  - sum.dim_IntList
  - sum.dim_DimnameList
  - sum.IntList_out
  - sum.DimnameList_out
  - sqrt
  - sqrt_
  - sqrt.out
  - threshold
  - threshold_
  - threshold.out
  - threshold_backward
  - one_hot
  - resize_as_
  - zero_
  - sub.out
  - sub.Tensor
  - sub_.Tensor
  - sub.Scalar
  - sub_.Scalar
  - addcdiv_
  - addcdiv.out
  - addcdiv
  - addcmul.out
  - addcmul
  - addcmul_
  - rsub.Tensor
  - rsub.Scalar
  - addmm.out
  - addmm
  - addmm_
  - set_.source_Storage
  - set_.source_Storage_storage_offset
  - set_.source_Tensor
  - set_
  - view
  - copy_
  - max.other
  - max.out
  - topk.values
  - topk
  - pow.Tensor_Tensor_out
  - pow.Tensor_Tensor
  - pow.Scalar_out
  - pow.Scalar
  - pow.Tensor_Scalar_out
  - pow.Tensor_Scalar
  - pow_.Scalar
  - pow_.Tensor
  - normal_
  - normal.Tensor_float_out
  - normal.Tensor_float
  - normal.float_Tensor_out
  - normal.float_Tensor
  - normal.Tensor_Tensor_out
  - normal.Tensor_Tensor
  - normal.float_float
  - normal.float_float_out
  - nll_loss.out
  - nll_loss
  - nll_loss_forward.output
  - nll_loss_forward
  - nll_loss_backward.grad_input
  - nll_loss_backward
  - nll_loss2d.out
  - nll_loss2d
  - nll_loss2d_forward.output
  - nll_loss2d_forward
  - nll_loss2d_backward.grad_input
  - nll_loss2d_backward
  - ones.names
  - ones
  - ones.out
  - ones_like
  - zeros_like
  - equal
  - index.Tensor
  - index_copy_
  - index_copy
  - index_copy_.dimname
  - index_copy.dimname
  - index_put_
  - index_put
  - _index_put_impl_
  - bitwise_and.Tensor_out
  - bitwise_and.Scalar_out
  - bitwise_and.Scalar
  - bitwise_and.Tensor
  - bitwise_and_.Scalar
  - bitwise_and_.Tensor
  - conv_transpose2d.input
  - conv_transpose3d.input
  - addcmul
  - addcmul.out
  - addcmul_
  - avg_pool2d_backward
  - avg_pool2d_backward.grad_input
  - avg_pool2d
  - avg_pool2d.out
  - _adaptive_avg_pool2d
  - _adaptive_avg_pool2d_backward
  - max_pool2d_with_indices.out
  - max_pool2d_with_indices
  - max_pool2d_with_indices_backward.grad_input
  - max_pool2d_with_indices_backward
  - _pin_memory
  - is_pinned
  - _reshape_alias
  - eq.Scalar_out
  - eq.Scalar
  - eq.Tensor_out
  - eq.Tensor
  - ge.Scalar_out
  - ge.Scalar
  - ge.Tensor_out
  - ge.Tensor
  - ge_.Scalar
  - ge_.Tensor
  - abs
  - abs_
  - abs.out
  - ne.Scalar_out
  - ne.Scalar
  - ne.Tensor_out
  - ne.Tensor
  - ne_.Scalar
  - ne_.Tensor
  - not_equal.Scalar_out
  # - not_equal.Scalar
  - not_equal.Tensor_out
  # - not_equal.Tensor
  - not_equal_.Scalar
  - not_equal_.Tensor
  - gather.out
  - gather
  - gather.dimname_out
  - gather.dimname
  - gelu
  - gelu.out
  - gelu_backward
  - gt.Scalar_out
  - gt.Scalar
  - gt.Tensor_out
  - gt.Tensor
  - gt_.Scalar
  - gt_.Tensor
  - masked_select.out
  - masked_select
  - masked_select_backward
  - _convolution_double_backward
  - embedding
  - embedding_dense_backward
  - erf
  - erf_
  - erf.out
  - exp
  - exp_
  - exp.out
  - index_select.out
  - index_select
  - index_select.dimname_out
  - index_select.dimname
  - native_layer_norm
  - native_layer_norm_backward
  - lt.Scalar_out
  - lt.Scalar
  - lt.Tensor_out
  - lt.Tensor
  - lt_.Scalar
  - lt_.Tensor
  - stack
  - stack.out
  - where.self
  - where
  - _s_where
  - mm.out
  - mm
  - kl_div
  - kl_div_backward
  - unsqueeze
  - unsqueeze_
  - isnan
  - unfold
  - le.Scalar_out
  - le.Scalar
  - le.Tensor_out
  - le.Tensor
  - le_.Scalar
  - le_.Tensor
  - less_equal.Scalar_out
  - less_equal.Scalar
  - less_equal.Tensor_out
  - less_equal.Tensor
  - less_equal_.Scalar
  - less_equal_.Tensor

  #- full

custom:
  - func: npu_transpose_to_contiguous(Tensor self) -> Tensor
    variants: function, method
  - func: npu_transpose(Tensor self, int[] perm) -> Tensor
    variants: function, method
  - func: npu_transpose.out(Tensor self, int[] perm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
    variants: function, method
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
    variants: function, method
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    variants: method
  - func: npu_alloc_float_status(Tensor self) -> Tensor
    variants: function, method
  - func: npu_get_float_status(Tensor self) -> Tensor
    variants: function, method
  - func: npu_clear_float_status(Tensor self) -> Tensor
    variants: function, method
  - func: one_(Tensor(a!) self) -> Tensor(a!)
    variants: method, function
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
    variants: function, method
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
    variants: function, method
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
    variants: function, method
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
custom_autograd:
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
  - func: get_storage_size(Tensor self) -> int
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
    variants: function, method
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)