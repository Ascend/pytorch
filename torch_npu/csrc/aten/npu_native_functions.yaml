backend: NPU
cpp_namespace: at_npu::native
supported:
 - __and__.Scalar
 - __and__.Tensor
 - __ilshift__.Scalar
 - __ilshift__.Tensor
 - __ior__.Scalar
 - __ior__.Tensor
 - __irshift__.Scalar
 - __irshift__.Tensor
 - __lshift__.Scalar
 - __lshift__.Tensor
 - __or__.Scalar
 - __or__.Tensor
 - __rshift__.Scalar
 - __rshift__.Tensor
 - __xor__.Scalar
 - __xor__.Tensor
 - func: _adaptive_avg_pool2d
   op_api: True
 - func: _adaptive_avg_pool2d_backward
   op_api: True
 - _adaptive_avg_pool3d
 - _adaptive_avg_pool3d_backward
 - _add_relu.Tensor
 - _add_relu.out
 - _add_relu_.Tensor
 - _aminmax
 - _aminmax.dim
 - aminmax.out
 - _amp_foreach_non_finite_check_
 - _amp_foreach_non_finite_check_and_unscale_
 - _batch_norm_impl_index
 - _batch_norm_impl_index_backward
 - func: _cat
   op_api: True
 - func: _cat.out
   op_api: True
 - _cdist_backward
 - _cdist_forward
 - _conv_depthwise2d
 - _conv_depthwise2d.out
 - _conv_depthwise2d_backward
 - convolution
 - _convolution
 - _ctc_loss
 - _ctc_loss_backward
 - _cummax_helper
 - _cummin_helper
 - _dim_arange
 - _embedding_bag
 - _embedding_bag_backward
 - _embedding_bag_forward_only
 - _index_copy_
 - _index_put_impl_
 - _linalg_svd.U
 - _local_scalar_dense
 - func: _log_softmax
   op_api: True
 - func: _log_softmax.out
   op_api: True
 - func: _log_softmax_backward_data
   op_api: True
 - func: _log_softmax_backward_data.out
   op_api: True
 - _nnpack_spatial_convolution
 - _pack_padded_sequence
 - _pad_packed_sequence
 - _pdist_forward
 - _pin_memory
 - _s_where
 - _slow_conv2d_backward.output_mask
 - _slow_conv2d_forward
 - _slow_conv2d_forward.output
 - func: _softmax
   op_api: True
 - func: _softmax.out
   op_api: True
 - _softmax_backward_data
 - _softmax_backward_data.out
 - _symeig_helper
 - _unique2
 - _reshape_alias
 - func: abs
   op_api: True
 - func: abs.out
   op_api: True
 - abs_
 - acos
 - acos.out
 - acos_
 - acosh
 - acosh.out
 - acosh_
 - adaptive_avg_pool1d
 - func: adaptive_avg_pool2d
   op_api: True
 - func: adaptive_avg_pool2d.out
   op_api: True
 - adaptive_avg_pool3d
 - adaptive_avg_pool3d.out
 - adaptive_avg_pool3d_backward.grad_input
 - adaptive_max_pool2d
 - adaptive_max_pool2d.out
 - adaptive_max_pool2d_backward
 - adaptive_max_pool2d_backward.grad_input
 - func: add.Scalar
   op_api: True
 - func: add.Tensor
   op_api: True
 - func: add.out
   op_api: True
 - add_.Scalar
 - add_.Tensor
 - addbmm
 - addbmm.out
 - addbmm_
 - func: addcdiv
   op_api: True
 - func: addcdiv.out
   op_api: True
 - func: addcdiv_
   op_api: True
 - func: addcmul
   op_api: True
 - func: addcmul.out
   op_api: True
 - addcmul_
 - addmm
 - func: addmm.out
   op_api: True
 - addmm_
 - func: addmv
   op_api: True
 - func: addmv.out
   op_api: True
 - func: addmv_
   op_api: True
 - addr
 - addr.out
 - addr_
 - affine_grid_generator
 - affine_grid_generator_backward
 - func: all
   op_api: True
 - func: all.dim
   op_api: True
 - func: all.out
   op_api: True
 - amax
 - amax.out
 - amin
 - amin.out
 - any
 - any.dim
 - any.out
 - func: arange
   op_api: True
 - func: arange.out
   op_api: True
 - func: arange.start
   op_api: True
 - func: arange.start_out
   op_api: True
 - func: arange.start_step
   op_api: True
 - argsort
 - argsort.dimname
 - as_strided
 - as_strided_
 - asin
 - asin.out
 - asin_
 - asinh
 - asinh.out
 - asinh_
 - atan
 - atan.out
 - atan2
 - atan2.out
 - atan2_
 - atan_
 - atanh
 - atanh.out
 - atanh_
 - avg_pool2d
 - avg_pool2d.out
 - avg_pool2d_backward
 - avg_pool2d_backward.grad_input
 - avg_pool3d
 - avg_pool3d.out
 - avg_pool3d_backward
 - avg_pool3d_backward.grad_input
 - baddbmm
 - baddbmm.out
 - baddbmm_
 - bartlett_window
 - bartlett_window.periodic
 - batch_norm
 - batch_norm_backward_elemt
 - batch_norm_backward_reduce
 - batch_norm_elemt
 - batch_norm_elemt.out
 - batch_norm_gather_stats_update
 - batch_norm_gather_stats_with_counts
 - batch_norm_reduce
 - batch_norm_stats
 - func: bernoulli
   op_api: True
 - func: bernoulli.out
   op_api: True
 - func: bernoulli.p
   op_api: True
 - func: bernoulli_.Tensor
   op_api: True
 - func: bernoulli_.float
   op_api: True
 - func: binary_cross_entropy
   op_api: True
 - func: binary_cross_entropy.out
   op_api: True
 - binary_cross_entropy_backward
 - binary_cross_entropy_backward.grad_input
 - binary_cross_entropy_with_logits
 - bincount
 - bitwise_and.Scalar
 - bitwise_and.Scalar_out
 - func: bitwise_and.Tensor
   op_api: True
 - func: bitwise_and.Tensor_out
   op_api: True
 - bitwise_and_.Scalar
 - func: bitwise_and_.Tensor
   op_api: True
 - func: bitwise_not
   op_api: True
 - func: bitwise_not.out
   op_api: True
 - func: bitwise_not_
   op_api: True
 - bitwise_or.Scalar
 - bitwise_or.Scalar_out
 - bitwise_or.Tensor
 - bitwise_or.Tensor_out
 - bitwise_xor.Scalar
 - bitwise_xor.Scalar_out
 - bitwise_xor.Tensor
 - bitwise_xor.Tensor_out
 - bitwise_xor_.Scalar
 - bitwise_xor_.Tensor
 - blackman_window
 - blackman_window.periodic
 - bmm
 - bmm.out
 - func: cat
   op_api: True
 - func: cat.names
   op_api: True
 - func: cat.names_out
   op_api: True
 - func: cat.out
   op_api: True
 - cdist
 - func: ceil
   op_api: True
 - func: ceil.out
   op_api: True
 - ceil_
 - channel_shuffle
 - func: clamp
   op_api: True
 - func: clamp.out
   op_api: True
 - func: clamp_
   op_api: True
 - clamp_max
 - clamp_max.out
 - clamp_max_
 - func: clamp_min
   op_api: True
 - func: clamp_min.out
   op_api: True
 - func: clamp_min_
   op_api: True
 - clamp.Tensor
 - clamp.Tensor_out
 - clamp_.Tensor
 - clamp_max.Tensor
 - clamp_max.Tensor_out
 - clamp_max_.Tensor
 - clamp_min.Tensor
 - clamp_min.Tensor_out
 - clamp_min_.Tensor
 - clone
 - col2im
 - col2im.out
 - col2im_backward
 - col2im_backward.grad_input
 - constant_pad_nd
 - contiguous
 - conv_tbc
 - conv_tbc_backward
 - conv_transpose2d.input
 - conv_transpose3d.input
 - convolution_backward
 - convolution_backward_overrideable
 - convolution_overrideable
 - count_nonzero
 - count_nonzero.dim_IntList
 - copy_
 - copy_memory_
 - cos
 - cos.out
 - cos_
 - cosh
 - cosh.out
 - cosh_
 - crop_and_resize
 - ctc_loss.IntList
 - ctc_loss.Tensor
 - cumprod.dimname_out
 - cumprod.out
 - cumprod_
 - cumprod_.dimname
 - func: cumsum.dimname_out
   op_api: True
 - func: cumsum.out
   op_api: True
 - func: cumsum
   op_api: True
 - decode_jpeg
 - diag
 - diag.out
 - div.Scalar
 - div.Scalar_mode
 - func: div.Tensor
   op_api: True
 - func: div.Tensor_mode
   op_api: True
 - func: div.out
   op_api: True
 - func: div.out_mode
   op_api: True
 - div_.Scalar
 - div_.Scalar_mode
 - func: div_.Tensor
   op_api: True
 - div_.Tensor_mode
 - func: dot
   op_api: True
 - func: dot.out
   op_api: True
 - dropout
 - dropout_with_byte_mask
 - embedding
 - embedding_backward
 - embedding_dense_backward
 - func: embedding_renorm_
   op_api: True
 - empty.memory_format
 - empty_like
 - empty_strided
 - empty_with_format
 - empty_with_format.names
 - func: eq.Scalar
   op_api: True
 - func: eq.Scalar_out
   op_api: True
 - func: eq.Tensor
   op_api: True
 - func: eq.Tensor_out
   op_api: True
 - eq_.Scalar
 - eq_.Tensor
 - func: equal
   op_api: True
 - func: erf
   op_api: True
 - func: erf.out
   op_api: True
 - func: erf_
   op_api: True
 - erfc
 - erfc.out
 - erfc_
 - erfinv
 - erfinv.out
 - erfinv_
 - func: exp
   op_api: True
 - func: exp.out
   op_api: True
 - exp2
 - exp2.out
 - exp2_
 - func: exp_
   op_api: True
 - expm1
 - expm1.out
 - expm1_
 - eye
 - eye.m
 - eye.m_out
 - eye.out
 - func: fill_.Tensor
   op_api: True
 - func: fill_.Scalar
   op_api: True
 - fill_diagonal_
 - func: flip
   op_api: True
 - func: floor
   op_api: True
 - func: floor.out
   op_api: True
 - floor_
 - floor_divide
 - floor_divide.Scalar
 - floor_divide.out
 - floor_divide_.Scalar
 - floor_divide_.Tensor
 - fmod.Scalar
 - fmod.Scalar_out
 - fmod.Tensor
 - fmod.Tensor_out
 - fmod_.Scalar
 - fmod_.Tensor
 - frac
 - frac.out
 - frac_
 - full
 - full.names
 - full.out
 - func: gather
   op_api: False
 - func: gather.dimname
   op_api: False
 - func: gather.dimname_out
   op_api: False
 - func: gather.out
   op_api: False
 - func: ge.Scalar
   op_api: True
 - func: ge.Scalar_out
   op_api: True
 - func: ge.Tensor
   op_api: True
 - func: ge.Tensor_out
   op_api: True
 - ge_.Scalar
 - ge_.Tensor
 - func: gelu
   op_api: True
 - func: gelu.out
   op_api: True
 - gelu_backward
 - ger
 - ger.out
 - glu
 - glu.out
 - glu_backward
 - glu_backward.grad_input
 - grid_sampler_2d
 - grid_sampler_2d_backward
 - grid_sampler_3d
 - grid_sampler_3d_backward
 - gru.input
 - func: gt.Scalar
   op_api: True
 - func: gt.Scalar_out
   op_api: True
 - func: gt.Tensor
   op_api: True
 - func: gt.Tensor_out
   op_api: True
 - gt_.Scalar
 - gt_.Tensor
 - hamming_window
 - hamming_window.periodic
 - hamming_window.periodic_alpha
 - hamming_window.periodic_alpha_beta
 - hann_window
 - hann_window.periodic
 - hardshrink
 - hardshrink_backward
 - hardsigmoid
 - hardsigmoid.out
 - hardsigmoid_
 - hardsigmoid_backward
 - func: hardswish
   op_api: True
 - func: hardswish.out
   op_api: True
 - func: hardswish_
   op_api: True
 - func: hardswish_backward
   op_api: True
 - hardtanh
 - hardtanh.out
 - hardtanh_
 - hardtanh_backward
 - hardtanh_backward.grad_input
 - histc
 - histc.out
 - im2col
 - im2col.out
 - im2col_backward
 - im2col_backward.grad_input
 - image_normalize
 - image_normalize_
 - img_to_tensor
 - index.Tensor
 - index_add
 - index_add.dimname
 - index_add.out
 - index_fill.int_Scalar
 - index_fill.int_Tensor
 - index_fill_.int_Scalar
 - index_fill_.int_Tensor
 - func: index_select
   op_api: True
 - index_put
 - index_put_
 - index_select.dimname
 - index_select.dimname_out
 - func: index_select.out
   op_api: True
 - inverse
 - inverse.out
 - is_pinned
 - isclose
 - isfinite
 - isnan
 - kl_div
 - kl_div_backward
 - kthvalue
 - kthvalue.dimname
 - kthvalue.dimname_out
 - kthvalue.values
 - l1_loss
 - l1_loss.out
 - l1_loss_backward
 - l1_loss_backward.grad_input
 - le.Scalar
 - le.Scalar_out
 - le.Tensor
 - le.Tensor_out
 - le_.Scalar
 - le_.Tensor
 - leaky_relu
 - leaky_relu.out
 - leaky_relu_
 - leaky_relu_backward
 - lerp.Scalar
 - lerp.Scalar_out
 - lerp.Tensor
 - lerp.Tensor_out
 - lerp_.Scalar
 - lerp_.Tensor
 - linalg_cross
 - linalg_cross.out
 - linspace
 - linspace.out
 - func: log
   op_api: True
 - func: log.out
   op_api: True
 - log10
 - log10.out
 - log10_
 - log1p
 - log1p.out
 - log1p_
 - log2
 - log2.out
 - log2_
 - log_
 - log_sigmoid
 - log_sigmoid.out
 - log_sigmoid_backward
 - log_sigmoid_backward.grad_input
 - log_sigmoid_forward
 - log_sigmoid_forward.output
 - log_softmax.Dimname
 - log_softmax.int
 - logaddexp
 - logaddexp.out
 - logaddexp2
 - logaddexp2.out
 - func: logical_and
   op_api: True
 - func: logical_and.out
   op_api: True
 - logical_and_
 - logical_not
 - logical_not.out
 - logical_not_
 - func: logical_or
   op_api: True
 - func: logical_or.out
   op_api: True
 - logical_or_
 - logspace
 - logspace.out
 - logsumexp
 - logsumexp.names
 - logsumexp.names_out
 - logsumexp.out
 - lstm.data
 - lstm.input
 - lstm_cell
 - lt.Scalar
 - lt.Scalar_out
 - func: lt.Tensor
   op_api: True
 - func: lt.Tensor_out
   op_api: True
 - lt_.Scalar
 - lt_.Tensor
 - func: masked_fill_.Scalar
   op_api: True
 - func: masked_fill_.Tensor
   op_api: True
 - masked_scatter_
 - masked_select
 - masked_select.out
 - matmul
 - matmul.out
 - max
 - max.dim
 - max.dim_max
 - max.names_dim
 - max.names_dim_max
 - max.out
 - max_pool2d
 - max_pool2d_with_indices
 - max_pool2d_with_indices.out
 - max_pool2d_with_indices_backward
 - max_pool2d_with_indices_backward.grad_input
 - max_pool3d_with_indices
 - max_pool3d_with_indices.out
 - max_pool3d_with_indices_backward
 - max_pool3d_with_indices_backward.grad_input
 - max_unpool2d
 - max_unpool2d.out
 - max_unpool2d_backward
 - max_unpool2d_backward.grad_input
 - max_unpool3d
 - max_unpool3d.out
 - max_unpool3d_backward
 - max_unpool3d_backward.grad_input
 - func: maximum
   op_api: True
 - func: maximum.out
   op_api: True
 - func: mean
   op_api: True
 - mean.dim
 - mean.names_dim
 - mean.names_out
 - func: mean.out
   op_api: True
 - median
 - median.dim
 - median.dim_values
 - median.names_dim
 - median.names_dim_values
 - min
 - min.dim
 - min.dim_min
 - min.names_dim
 - min.names_dim_min
 - min.out
 - minimum
 - minimum.out
 - mish
 - mish.out
 - mish_
 - mish_backward
 - mm
 - mm.out
 - mse_loss
 - mse_loss.out
 - mse_loss_backward
 - mse_loss_backward.grad_input
 - func: mul.Scalar
   op_api: True
 - func: mul.Tensor
   op_api: True
 - func: mul.out
   op_api: True
 - func: mul_.Scalar
   op_api: True
 - func: mul_.Tensor
   op_api: True
 - multilabel_margin_loss
 - multilabel_margin_loss.out
 - multilabel_margin_loss_forward
 - multilabel_margin_loss_forward.output
 - multinomial
 - multinomial.out
 - mv
 - mv.out
 - func: native_batch_norm
   op_api: True
 - func: native_batch_norm_backward
   op_api: True
 - native_dropout
 - native_dropout_backward
 - func: native_layer_norm
   op_api: True
 - func: native_layer_norm_backward
   op_api: True
 - func: ne.Scalar
   op_api: True
 - func: ne.Scalar_out
   op_api: True
 - func: ne.Tensor
   op_api: True
 - func: ne.Tensor_out
   op_api: True
 - ne_.Scalar
 - ne_.Tensor
 - func: neg
   op_api: True
 - func: neg.out
   op_api: True
 - neg_
 - nll_loss
 - nll_loss.out
 - nll_loss2d
 - nll_loss2d.out
 - func: nll_loss2d_backward
   op_api: True
 - func: nll_loss2d_backward.grad_input
   op_api: True
 - func: nll_loss2d_forward
   op_api: True
 - func: nll_loss2d_forward.output
   op_api: True
 - func: nll_loss_backward
   op_api: True
 - func: nll_loss_backward.grad_input
   op_api: True
 - func: nll_loss_forward
   op_api: True
 - func: nll_loss_forward.output
   op_api: True
 - nonzero
 - nonzero.out
 - norm.Scalar
 - norm.ScalarOpt_dim
 - norm.ScalarOpt_dim_dtype
 - norm.ScalarOpt_dtype
 - norm.dtype_out
 - norm.out
 - normal.Tensor_Tensor
 - normal.Tensor_Tensor_out
 - normal.Tensor_float
 - normal.Tensor_float_out
 - normal.float_Tensor
 - normal.float_Tensor_out
 - normal.float_float
 - normal.float_float_out
 - func: normal_
   op_api: True
 - one_
 - one_hot
 - ones
 - ones.names
 - ones.out
 - ones_like
 - pdist
 - pow.Scalar
 - pow.Scalar_out
 - func: pow.Tensor_Scalar
   op_api: True
 - func: pow.Tensor_Scalar_out
   op_api: True
 - func: pow.Tensor_Tensor
   op_api: True
 - func: pow.Tensor_Tensor_out
   op_api: True
 - pow_.Scalar
 - pow_.Tensor
 - func: prelu
   op_api: True
 - prelu_backward
 - prod
 - prod.Dimname_out
 - prod.dim_Dimname
 - prod.dim_int
 - prod.int_out
 - put_
 - qr
 - qr.Q
 - quantize_per_channel
 - quantize_per_tensor
 - func: random_
   op_api: True
 - func: random_.from
   op_api: True
 - func: random_.to
   op_api: True
 - func: randperm
   op_api: True
 - func: randperm.generator
   op_api: True
 - func: randperm.generator_out
   op_api: True
 - func: randperm.out
   op_api: True
 - range
 - range.out
 - range.step
 - reciprocal
 - reciprocal.out
 - reciprocal_
 - reflection_pad1d
 - reflection_pad1d.out
 - reflection_pad1d_backward
 - reflection_pad1d_backward.grad_input
 - reflection_pad2d
 - reflection_pad2d.out
 - func: reflection_pad2d_backward
   op_api: True
 - func: reflection_pad2d_backward.grad_input
   op_api: True
 - func: relu
   op_api: True
 - func: relu_
   op_api: True
 - remainder.Scalar
 - remainder.Scalar_out
 - remainder.Tensor
 - remainder.Tensor_out
 - remainder_.Scalar
 - remainder_.Tensor
 - renorm
 - renorm.out
 - renorm_
 - repeat
 - repeat_interleave.self_Tensor
 - repeat_interleave.self_int
 - replication_pad1d
 - replication_pad1d.out
 - replication_pad1d_backward
 - replication_pad1d_backward.grad_input
 - replication_pad2d
 - replication_pad2d.out
 - func: replication_pad2d_backward
   op_api: True
 - func: replication_pad2d_backward.grad_input
   op_api: True
 - resize_
 - resize_as_
 - reverse
 - func: roll
   op_api: True
 - round
 - round.out
 - round_
 - func: rrelu_with_noise
   op_api: True
 - func: rrelu_with_noise.out
   op_api: True
 - func: rrelu_with_noise_
   op_api: True
 - rrelu_with_noise_backward
 - rsqrt
 - rsqrt.out
 - rsqrt_
 - rsub.Scalar
 - rsub.Tensor
 - scalar_tensor
 - func: scatter.src_out
   op_api: True
 - func: scatter.value_out
   op_api: True
 - func: scatter.reduce_out
   op_api: True
 - func: scatter.value_reduce_out
   op_api: True
 - scatter_add
 - scatter_add.dimname
 - scatter_add_
 - searchsorted.Scalar
 - searchsorted.Tensor
 - searchsorted.Tensor_out
 - set_
 - set_.source_Storage
 - set_.source_Storage_storage_offset
 - set_.source_Tensor
 - sgn.out
 - func: sigmoid
   op_api: True
 - func: sigmoid.out
   op_api: True
 - func: sigmoid_
   op_api: True
 - func: sigmoid_backward
   op_api: True
 - func: sigmoid_backward.grad_input
   op_api: True
 - sign
 - sign.out
 - sign_
 - sin
 - sin.out
 - sin_
 - sinh
 - sinh.out
 - sinh_
 - slogdet
 - slow_conv3d
 - slow_conv3d.out
 - slow_conv3d_forward
 - slow_conv3d_forward.output
 - slow_conv_dilated2d
 - slow_conv_dilated2d_backward
 - slow_conv_transpose2d
 - slow_conv_transpose2d.out
 - slow_conv_transpose2d_backward
 - smooth_l1_loss
 - smooth_l1_loss.out
 - smooth_l1_loss_backward
 - smooth_l1_loss_backward.grad_input
 - soft_margin_loss
 - soft_margin_loss.out
 - soft_margin_loss_backward
 - soft_margin_loss_backward.grad_input
 - softmax.Dimname
 - softmax.int
 - softplus
 - softplus.out
 - softplus_backward.grad_input
 - softshrink
 - softshrink.out
 - softshrink_backward
 - softshrink_backward.grad_input
 - func: sort
   op_api: True
 - func: sort.stable
   op_api: True
 - func: sort.dimname
   op_api: True
 - func: sort.dimname_values
   op_api: True
 - func: sort.values
   op_api: True
 - func: sort.values_stable
   op_api: True
 - func: sqrt
   op_api: True
 - func: sqrt.out
   op_api: True
 - func: sqrt_
   op_api: True
 - squeeze
 - squeeze.dim
 - func: stack
   op_api: True
 - func: stack.out
   op_api: True
 - std.correction
 - std.correction_out
 - std.correction_names
 - std.correction_names_out
 - std.names_out
 - std.out
 - std_mean.correction
 - std_mean.correction_names
 - func: sub.Scalar
   op_api: True
 - func: sub.Tensor
   op_api: True
 - func: sub.out
   op_api: True
 - sub_.Scalar
 - sub_.Tensor
 - func: sum
   op_api: True
 - func: sum.DimnameList_out
   op_api: True
 - func: sum.IntList_out
   op_api: True
 - func: sum.dim_DimnameList
   op_api: True
 - func: sum.dim_IntList
   op_api: True
 - take
 - take.out
 - tan
 - tan.out
 - tan_
 - tanh
 - tanh.out
 - tanh_
 - tanh_backward
 - tanh_backward.grad_input
 - threshold
 - threshold.out
 - threshold_
 - threshold_backward
 - to.device
 - func: to.dtype
   op_api: True
 - to.dtype_layout
 - to.other
 - func: topk
   op_api: True
 - func: topk.values
   op_api: True
 - triangular_solve.X
 - func: tril
   op_api: True
 - func: tril.out
   op_api: True
 - func: tril_
   op_api: True
 - tril_indices
 - triu
 - triu.out
 - triu_
 - triu_indices
 - true_divide.Scalar
 - true_divide.Tensor
 - true_divide.out
 - true_divide_.Scalar
 - true_divide_.Tensor
 - trunc
 - trunc.out
 - trunc_
 - unfold
 - func: uniform_
   op_api: True
 - unique_consecutive
 - unsqueeze
 - upsample_bicubic2d
 - upsample_bicubic2d.out
 - upsample_bicubic2d.vec
 - upsample_bicubic2d_backward
 - upsample_bicubic2d_backward.grad_input
 - upsample_bicubic2d_backward.vec
 - func: upsample_bilinear2d
   op_api: True
 - func: upsample_bilinear2d.out
   op_api: True
 - upsample_bilinear2d.vec
 - upsample_bilinear2d_backward
 - upsample_bilinear2d_backward.grad_input
 - upsample_bilinear2d_backward.vec
 - upsample_linear1d
 - upsample_linear1d.out
 - upsample_linear1d.vec
 - upsample_linear1d_backward
 - upsample_linear1d_backward.vec
 - upsample_nearest1d
 - upsample_nearest1d.out
 - upsample_nearest1d.vec
 - upsample_nearest1d_backward
 - upsample_nearest1d_backward.grad_input
 - upsample_nearest1d_backward.vec
 - upsample_nearest2d
 - upsample_nearest2d.out
 - upsample_nearest2d.vec
 - upsample_nearest2d_backward
 - upsample_nearest2d_backward.grad_input
 - upsample_nearest2d_backward.vec
 - upsample_nearest3d
 - upsample_nearest3d.out
 - upsample_nearest3d.vec
 - upsample_nearest3d_backward
 - upsample_nearest3d_backward.grad_input
 - upsample_nearest3d_backward.vec
 - upsample_trilinear3d
 - upsample_trilinear3d.out
 - upsample_trilinear3d.vec
 - upsample_trilinear3d_backward
 - upsample_trilinear3d_backward.grad_input
 - upsample_trilinear3d_backward.vec
 - var.correction
 - var.correction_names
 - var.out
 - var.names_out
 - var.correction_out
 - var.correction_names_out
 - var_mean.correction
 - var_mean.correction_names
 - view
 - where
 - where.self
 - xlogy.OutScalar_Other
 - xlogy.OutScalar_Self
 - xlogy.OutTensor
 - xlogy.Scalar_Other
 - xlogy.Scalar_Self
 - xlogy.Tensor
 - xlogy_.Scalar_Other
 - xlogy_.Tensor
 - func: zero_
   op_api: True
 - zeros
 - zeros.names
 - zeros.out
 - zeros_like

autograd:
  - celu
  - celu_
  - elu.out
  - elu
  - elu_
  - silu
  - silu_
  - silu.out
  - binary_cross_entropy_with_logits_backward
  - selu
  - selu_

tocpu:
  - _cholesky_helper
  - _cholesky_solve_helper
  - _compute_linear_combination
  - _compute_linear_combination.out
  - _dirichlet_grad
  - _embedding_bag_per_sample_weights_backward
  - _fft_r2c
  - _fft_r2c.out
  - _fft_c2r
  - _fft_c2r.out
  - _fft_c2c
  - _fft_c2c.out
  - _foreach_add.Scalar
  - _foreach_add_.Scalar
  - _foreach_sub.Scalar
  - _foreach_sub_.Scalar
  - _foreach_mul.Scalar
  - _foreach_mul_.Scalar
  - _foreach_div.Scalar
  - _foreach_div_.Scalar
  - _foreach_add.List
  - _foreach_add_.List
  - _foreach_sub.List
  - _foreach_sub_.List
  - _foreach_mul.List
  - _foreach_mul_.List
  - _foreach_div.List
  - _foreach_div_.List
  - _foreach_add.ScalarList
  - _foreach_add_.ScalarList
  - _foreach_sub.ScalarList
  - _foreach_sub_.ScalarList
  - _foreach_div.ScalarList
  - _foreach_div_.ScalarList
  - _foreach_mul.ScalarList
  - _foreach_mul_.ScalarList
  - _foreach_exp
  - _foreach_zero_
  - _foreach_exp_
  - _foreach_sqrt
  - _foreach_sqrt_
  - _foreach_abs
  - _foreach_abs_
  - _foreach_acos
  - _foreach_acos_
  - _foreach_asin
  - _foreach_asin_
  - _foreach_atan
  - _foreach_atan_
  - _foreach_ceil
  - _foreach_ceil_
  - _foreach_cos
  - _foreach_cos_
  - _foreach_cosh
  - _foreach_cosh_
  - _foreach_erf
  - _foreach_erf_
  - _foreach_erfc
  - _foreach_erfc_
  - _foreach_expm1
  - _foreach_expm1_
  - _foreach_floor
  - _foreach_floor_
  - _foreach_log
  - _foreach_log_
  - _foreach_log10
  - _foreach_log10_
  - _foreach_log1p
  - _foreach_log1p_
  - _foreach_log2
  - _foreach_log2_
  - _foreach_neg
  - _foreach_neg_
  - _foreach_tan
  - _foreach_tan_
  - _foreach_tanh
  - _foreach_tanh_
  - _foreach_sin
  - _foreach_sin_
  - _foreach_sinh
  - _foreach_sinh_
  - _foreach_round
  - _foreach_round_
  - _foreach_lgamma
  - _foreach_lgamma_
  - _foreach_frac
  - _foreach_frac_
  - _foreach_reciprocal
  - _foreach_reciprocal_
  - _foreach_sigmoid
  - _foreach_sigmoid_
  - _foreach_trunc
  - _foreach_trunc_
  - _foreach_addcdiv_.Scalar
  - _foreach_addcmul_.Scalar
  - _foreach_addcdiv_.ScalarList
  - _foreach_addcmul_.ScalarList
  - _foreach_addcdiv.Scalar
  - _foreach_addcmul.Scalar
  - _foreach_addcdiv.ScalarList
  - _foreach_addcmul.ScalarList
  - _foreach_maximum.List
  - _foreach_minimum.List
  - _linalg_inv_out_helper_
  - _linalg_qr_helper
  - _logcumsumexp
  - _logcumsumexp.out
  - _pdist_backward
  - _sample_dirichlet
  - _standard_gamma_grad
  - _standard_gamma
  - _test_optional_intlist
  - _test_optional_filled_intlist
  - _test_optional_floatlist
  - _solve_helper
  - _unique
  - angle
  - angle.out
  - binomial
  - bucketize.Tensor
  - bucketize.Tensor_out
  - bucketize.Scalar
  - cauchy_
  - cholesky_inverse
  - cholesky_inverse.out
  - complex.out
  - exponential_
  - geometric_
  - is_set_to
  - linalg_vector_norm
  - linalg_vector_norm.out
  - linalg_slogdet
  - linalg_slogdet.out
  - logical_xor.out
  - log_normal_
  - logit
  - logit_
  - logit.out
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_backward
  - multi_margin_loss.out
  - multi_margin_loss
  - multi_margin_loss_backward.grad_input
  - multi_margin_loss_backward
  - mode
  - nanmedian
  - nanmedian.dim_values
  - nansum
  - nansum.dim_IntList
  - nansum.IntList_out
  - native_group_norm_backward
  - narrow_copy.out
  - poisson
  - polar.out
  - replication_pad3d_backward.grad_input
  - replication_pad3d_backward
  - repeat_interleave.Tensor
  - slow_conv_dilated3d
  - slow_conv_transpose3d.out
  - slow_conv_transpose3d
  - sspaddmm.out
  - tensordot.out
  - trace
  - to_sparse.sparse_dim
  - to_sparse
  - to_mkldnn
  - unfold_backward
  - unique_dim
  - unique_dim_consecutive
  - vdot
  - view_as_complex
  - view_as_real
  - sinc.out
  - gcd.out
  - lcm.out
  - isposinf.out
  - isneginf.out
  - fmin
  - fmin.out
  - fmax
  - fmax.out
  - copysign.Tensor
  - copysign_.Tensor
  - copysign.out
  - copysign.Scalar
  - copysign_.Scalar
  - matrix_exp
  - _empty_affine_quantized
  - _empty_per_channel_affine_quantized
  - lgamma.out
  - lgamma_
  - lgamma
  - digamma.out
  - digamma
  - polygamma.out
  - igamma.out
  - igamma
  - igamma_
  - igammac.out
  - igammac
  - igammac_
  - i0.out
  - signbit.out
  - hypot.out
  - hypot
  - nextafter.out
  - nextafter
  - batch_norm_update_stats
  - heaviside.out
  - scatter_.reduce
  - scatter_.value_reduce
  - digamma_
  - adaptive_max_pool3d.out
  - adaptive_max_pool3d
  - adaptive_max_pool3d_backward.grad_input
  - adaptive_max_pool3d_backward
  - fractional_max_pool3d.output
  - fractional_max_pool3d
  - fractional_max_pool3d_backward.grad_input
  - fractional_max_pool3d_backward
  - fractional_max_pool2d.output
  - fractional_max_pool2d
  - fractional_max_pool2d_backward.grad_input
  - fractional_max_pool2d_backward
  - replication_pad3d.out
  - replication_pad3d
  - logit_backward.grad_input
  - logit_backward
  - orgqr.out
  - orgqr
  - _lu_with_info

unsupported:
  - _conj
  - conj
  - bitwise_left_shift.Tensor
  - bitwise_left_shift_.Tensor
  - bitwise_left_shift.Tensor_out
  - bitwise_left_shift.Tensor_Scalar
  - bitwise_left_shift_.Tensor_Scalar
  - bitwise_left_shift.Tensor_Scalar_out
  - bitwise_left_shift.Scalar_Tensor
  - bitwise_right_shift.Tensor
  - bitwise_right_shift_.Tensor
  - bitwise_right_shift.Tensor_out
  - bitwise_right_shift.Tensor_Scalar
  - bitwise_right_shift_.Tensor_Scalar
  - bitwise_right_shift.Tensor_Scalar_out
  - bitwise_right_shift.Scalar_Tensor
  - _conj_physical
  - conj_physical
  - conj_physical.out
  - conj_physical_
  - frexp.Tensor
  - frexp.Tensor_out
  - isin.Tensor_Tensor_out
  - isin.Tensor_Tensor
  - isin.Tensor_Scalar_out
  - isin.Tensor_Scalar
  - isin.Scalar_Tensor_out
  - isin.Scalar_Tensor
  - cholesky.out
  - cholesky
  - geqrf.a
  - geqrf
  - logdet
  - linalg_lu_factor_ex
  - linalg_lu_factor_ex.out
  - lu_solve.out
  - lu_solve
  - lu_unpack
  - lu_unpack.out
  - _det_lu_based_helper
  - linalg_det
  - linalg_det.out
  - linalg_cholesky_ex
  - linalg_cholesky_ex.L
  - linalg_eig
  - linalg_eig.out
  - linalg_eigh
  - linalg_eigh.eigvals
  - linalg_eigvalsh
  - linalg_eigvalsh.out
  - linalg_solve_triangular.out
  - linalg_solve_triangular
  - linalg_lstsq
  - linalg_lstsq.out
  - special_entr
  - special_entr.out
  - special_erfcx
  - special_erfcx.out
  - special_zeta
  - special_zeta.self_scalar
  - special_zeta.other_scalar
  - special_zeta.out
  - special_zeta.self_scalar_out
  - special_zeta.other_scalar_out

custom:
  - func: npu_change_data_ptr(Tensor dst, Tensor src, int index) -> int
  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: npu_alloc_float_status(Tensor self) -> Tensor
  - func: npu_get_float_status(Tensor self) -> Tensor
  - func: npu_clear_float_status(Tensor self) -> Tensor
  - func: one_(Tensor(a!) self) -> Tensor(a!)
  - func: fast_gelu(Tensor self) -> Tensor
  - func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
  - func: _amp_foreach_non_finite_check_(Tensor[] scaled_grads) -> bool
  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: unsafe_empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> Tensor[]
  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3, bool try_recover_truncated=False) -> Tensor
  - func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
  - func: reverse(Tensor self, int[] axis) -> Tensor
  - func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor
  - func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)
  - func: img_to_tensor(Tensor self) -> Tensor
  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)
  - func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
  - func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]
  - func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]
  - func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)
  - func: format_contiguous(Tensor self) -> Tensor
  - func: check_match(Tensor self) -> bool
  - func: check_memory_overlaps(Tensor[] inputs, Tensor[] outputs) -> ()
  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
  - func: get_storage_size(Tensor self) -> int
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_enque_tensor(Tensor[] tensors, str format_string, int capacity=3) -> ()
  - func: npu_hcom_allgather(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allgather.out(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: bscpp_add(Tensor self, Tensor other) -> Tensor
    bscpp_op: True

custom_autograd:
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
  - func: npu_silu(Tensor self) -> Tensor
  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> Tensor[]
  - func: npu_mish(Tensor self) -> Tensor
  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seqMask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flagSeq, bool direction) -> Tensor[]
  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor[]
  - func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor
