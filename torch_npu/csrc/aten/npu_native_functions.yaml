backend: NPU
cpp_namespace: at_npu::native
supported:
 - __ilshift__.Scalar
 - __ilshift__.Tensor
 - func: __ior__.Scalar
   op_api: False
 - func: __ior__.Tensor
   op_api: False
 - __irshift__.Scalar
 - __irshift__.Tensor
 - __lshift__.Scalar
 - __lshift__.Tensor
 - __rshift__.Scalar
 - __rshift__.Tensor
 - __xor__.Scalar
 - __xor__.Tensor
 - func: _adaptive_avg_pool2d
   op_api: True
 - func: _adaptive_avg_pool2d_backward
   op_api: True
 - func: argmin.out
   op_api: True
 - _adaptive_avg_pool3d
 - _adaptive_avg_pool3d_backward
 - _add_relu.Tensor
 - _add_relu.out
 - _add_relu_.Tensor
 - _aminmax
 - _aminmax.dim
 - func: aminmax
   op_api: False
 - func: aminmax.out
   op_api: False
 - _amp_foreach_non_finite_check_
 - func: _amp_foreach_non_finite_check_and_unscale_
   op_api: True
 - _batch_norm_impl_index
 - _batch_norm_impl_index_backward
 - func: _cat
   op_api: True
 - func: _cat.out
   op_api: True
 - _cdist_backward
 - _cdist_forward
 - _conv_depthwise2d
 - _conv_depthwise2d.out
 - _conv_depthwise2d_backward
 - func: convolution
   op_api: True
 - func: _convolution
   op_api: False
 - func: _ctc_loss
   op_api: True
 - func: _ctc_loss_backward
   op_api: True
 - func: _cummax_helper
   op_api: False
 - func: _cummin_helper
   op_api: False
 - _dim_arange
 - _embedding_bag
 - _embedding_bag_backward
 - _embedding_bag_forward_only
 - func: _index_copy_
   op_api: False
 - func: _index_put_impl_
   op_api: True
 - _linalg_svd.U
 - _local_scalar_dense
 - func: _log_softmax
   op_api: True
 - func: _log_softmax.out
   op_api: True
 - func: _log_softmax_backward_data
   op_api: True
 - func: _log_softmax_backward_data.out
   op_api: True
 - _nnpack_spatial_convolution
 - _pack_padded_sequence
 - _pad_packed_sequence
 - _pdist_forward
 - _pin_memory
 - func: _s_where
   op_api: True
 - _slow_conv2d_backward.output_mask
 - _slow_conv2d_forward
 - _slow_conv2d_forward.output
 - func: _softmax
   op_api: True
 - func: _softmax.out
   op_api: True
 - func: _softmax_backward_data
   op_api: True
 - func: _softmax_backward_data.out
   op_api: True
 - _symeig_helper
 - func: _unique2
   op_api: True
 - _reshape_alias
 - func: abs
   op_api: True
 - func: abs.out
   op_api: True
 - func: abs_
   op_api: True
 - func: acos
   op_api: True
 - func: acos.out
   op_api: True
 - func: acos_
   op_api: True
 - func: acosh
   op_api: False
 - func: acosh.out
   op_api: False
 - func: acosh_
   op_api: False
 - adaptive_avg_pool1d
 - func: adaptive_avg_pool2d
   op_api: True
 - func: adaptive_avg_pool2d.out
   op_api: True
 - adaptive_avg_pool3d
 - adaptive_avg_pool3d.out
 - adaptive_avg_pool3d_backward.grad_input
 - func: adaptive_max_pool2d
   op_api: True
 - func: adaptive_max_pool2d.out
   op_api: True
 - adaptive_max_pool2d_backward
 - adaptive_max_pool2d_backward.grad_input
 - func: add.Scalar
   op_api: True
 - func: add.Tensor
   op_api: True
 - func: add.out
   op_api: True
 - func: add_.Scalar
   op_api: True
 - func: add_.Tensor
   op_api: True
 - func: addbmm
   op_api: False
 - func: addbmm.out
   op_api: False
 - func: addbmm_
   op_api: False
 - func: addcdiv
   op_api: True
 - func: addcdiv.out
   op_api: True
 - func: addcdiv_
   op_api: True
 - func: addcmul
   op_api: True
 - func: addcmul.out
   op_api: True
 - func: addcmul_
   op_api: True
 - func: addmm
   op_api: True
 - func: addmm.out
   op_api: True
 - func: addmm_
   op_api: True
 - func: addmv
   op_api: True
 - func: addmv.out
   op_api: True
 - func: addmv_
   op_api: True
 - func: addr
   op_api: False
 - func: addr.out
   op_api: False
 - func: addr_
   op_api: False
 - affine_grid_generator
 - affine_grid_generator_backward
 - func: all
   op_api: True
 - func: all.dim
   op_api: True
 - func: all.out
   op_api: True
 - func: all.all_out
   op_api: True
 - func: amax
   op_api: False
 - func: amax.out
   op_api: False
 - func: amin
   op_api: False
 - func: amin.out
   op_api: False
 - func: any
   op_api: True
 - func: any.all_out
   op_api: True
 - func: any.dim
   op_api: True
 - func: any.out
   op_api: True
 - func: arange
   op_api: True
 - func: arange.out
   op_api: True
 - func: arange.start
   op_api: True
 - func: arange.start_out
   op_api: True
 - func: arange.start_step
   op_api: True
 - func: argsort
   op_api: False
 - func: argsort.dimname
   op_api: False
 - as_strided
 - as_strided_
 - asin
 - asin.out
 - asin_
 - func: asinh
   op_api: False
 - func: asinh.out
   op_api: False
 - func: asinh_
   op_api: False
 - func: atan
   op_api: True
 - func: atan.out
   op_api: True
 - func: atan_
   op_api: True
 - atan2
 - atan2.out
 - atan2_
 - func: atanh
   op_api: False
 - func: atanh.out
   op_api: False
 - func: atanh_
   op_api: False
 - func: avg_pool2d
   op_api: True
 - func: avg_pool2d.out
   op_api: True
 - func: avg_pool2d_backward
   op_api: False
 - func: avg_pool2d_backward.grad_input
   op_api: False
 - avg_pool3d
 - avg_pool3d.out
 - avg_pool3d_backward
 - avg_pool3d_backward.grad_input
 - func: baddbmm
   op_api: True
 - func: baddbmm.out
   op_api: True
 - func: baddbmm_
   op_api: True
 - bartlett_window
 - bartlett_window.periodic
 - batch_norm
 - func: batch_norm_backward_elemt
   op_api: True
 - func: batch_norm_backward_reduce
   op_api: True
 - func: batch_norm_elemt
   op_api: True
 - func: batch_norm_elemt.out
   op_api: True
 - batch_norm_gather_stats_update
 - func: batch_norm_gather_stats_with_counts
   op_api: True
 - batch_norm_reduce
 - func: batch_norm_stats
   op_api: True
 - func: bernoulli
   op_api: True
 - func: bernoulli.out
   op_api: True
 - func: bernoulli.p
   op_api: True
 - func: bernoulli_.Tensor
   op_api: True
 - func: bernoulli_.float
   op_api: True
 - func: binary_cross_entropy
   op_api: False
 - func: binary_cross_entropy.out
   op_api: False
 - func: binary_cross_entropy_backward
   op_api: True
 - func: binary_cross_entropy_backward.grad_input
   op_api: True
 - func: binary_cross_entropy_with_logits
   op_api: False
 - func: bincount
   op_api: True
 - func: bitwise_and.Scalar
   op_api: True
 - func: bitwise_and.Scalar_out
   op_api: True
 - func: bitwise_and.Tensor
   op_api: True
 - func: bitwise_and.Tensor_out
   op_api: True
 - func: bitwise_and_.Scalar
   op_api: True
 - func: bitwise_and_.Tensor
   op_api: True
 - func: bitwise_not
   op_api: True
 - func: bitwise_not.out
   op_api: True
 - func: bitwise_not_
   op_api: True
 - func: bitwise_or.Scalar
   op_api: True
 - func: bitwise_or.Scalar_out
   op_api: True
 - func: bitwise_or.Tensor
   op_api: True
 - func: bitwise_or.Tensor_out
   op_api: True
 - func: bitwise_xor.Scalar
   op_api: False
 - func: bitwise_xor.Scalar_out
   op_api: False
 - func: bitwise_xor.Tensor
   op_api: False
 - func: bitwise_xor.Tensor_out
   op_api: False
 - func: bitwise_xor_.Scalar
   op_api: False
 - func: bitwise_xor_.Tensor
   op_api: False
 - blackman_window
 - blackman_window.periodic
 - func: bmm
   op_api: True
 - func: bmm.out
   op_api: True
 - func: cat
   op_api: True
 - func: cat.names
   op_api: True
 - func: cat.names_out
   op_api: True
 - func: cat.out
   op_api: True
 - cdist
 - func: ceil
   op_api: True
 - func: ceil.out
   op_api: True
 - ceil_
 - channel_shuffle
 - func: clamp
   op_api: True
 - func: clamp.out
   op_api: True
 - func: clamp_
   op_api: True
 - func: clamp_max
   op_api: False
 - clamp_max.out
 - func: clamp_max_
   op_api: False
 - func: clamp_min
   op_api: True
 - func: clamp_min.out
   op_api: True
 - func: clamp_min_
   op_api: True
 - func: clamp.Tensor
   op_api: True
 - func: clamp.Tensor_out
   op_api: True
 - func: clamp_.Tensor
   op_api: True
 - func: clamp_max.Tensor
   op_api: False
 - func: clamp_max.Tensor_out
   op_api: False
 - clamp_max_.Tensor
 - func: clamp_min.Tensor
   op_api: False
 - func: clamp_min.Tensor_out
   op_api: False
 - func: clamp_min_.Tensor
   op_api: False
 - func: clone
   op_api: True
 - func: col2im
   op_api: False
 - func: col2im.out
   op_api: False
 - col2im_backward
 - col2im_backward.grad_input
 - func: constant_pad_nd
   op_api: False
 - contiguous
 - func: conv_tbc
   op_api: False
 - func: conv_tbc_backward
   op_api: False
 - conv_transpose2d.input
 - conv_transpose3d.input
 - func: convolution_backward
   op_api: True
 - convolution_backward_overrideable
 - convolution_overrideable
 - func: count_nonzero
   op_api: True
 - func: count_nonzero.dim_IntList
   op_api: True
 - func: copy_
   op_api: True
 - copy_memory_
 - func: cos
   op_api: True
 - func: cos.out
   op_api: True
 - func: cos_
   op_api: True
 - func: cosh
   op_api: False
 - func: cosh.out
   op_api: False
 - func: cosh_
   op_api: False
 - crop_and_resize
 - func: ctc_loss.IntList
   op_api: True
 - func: ctc_loss.Tensor
   op_api: True
 - cumprod.dimname_out
 - cumprod.out
 - cumprod_
 - cumprod_.dimname
 - func: cumsum.dimname_out
   op_api: True
 - func: cumsum.out
   op_api: True
 - func: cumsum
   op_api: True
 - decode_jpeg
 - diag
 - diag.out
 - func: div.Scalar
   op_api: True
 - func: div.Scalar_mode
   op_api: True
 - func: div.Tensor
   op_api: True
 - func: div.Tensor_mode
   op_api: True
 - func: div.out
   op_api: True
 - func: div.out_mode
   op_api: True
 - func: div_.Scalar
   op_api: True
 - func: div_.Scalar_mode
   op_api: True
 - func: div_.Tensor
   op_api: True
 - func: div_.Tensor_mode
   op_api: True
 - func: dot
   op_api: True
 - func: dot.out
   op_api: True
 - func: dropout
   op_api: False
 - dropout_with_byte_mask
 - func: embedding
   op_api: True
 - func: embedding_backward
   op_api: True
 - func: embedding_dense_backward
   op_api: True
 - func: embedding_renorm_
   op_api: True
 - empty.memory_format
 - empty_like
 - empty_strided
 - empty_with_format
 - empty_with_format.names
 - func: eq.Scalar
   op_api: True
 - func: eq.Scalar_out
   op_api: True
 - func: eq.Tensor
   op_api: True
 - func: eq.Tensor_out
   op_api: True
 - func: eq_.Scalar
   op_api: True
 - func: eq_.Tensor
   op_api: True
 - func: equal
   op_api: True
 - func: erf
   op_api: True
 - func: erf.out
   op_api: True
 - func: erf_
   op_api: True
 - func: erfc
   op_api: False
 - func: erfc.out
   op_api: False
 - func: erfc_
   op_api: False
 - func: erfinv
   op_api: False
 - func: erfinv.out
   op_api: False
 - func: erfinv_
   op_api: False
 - func: exp
   op_api: True
 - func: exp.out
   op_api: True
 - func: exp2
   op_api: False
 - func: exp2.out
   op_api: False
 - func: exp2_
   op_api: False
 - func: exp_
   op_api: True
 - func: expm1
   op_api: False
 - func: expm1.out
   op_api: False
 - expm1_
 - func: eye
   op_api: True
 - func: eye.m
   op_api: True
 - func: eye.m_out
   op_api: True
 - func: eye.out
   op_api: True
 - func: fill_.Tensor
   op_api: True
 - func: fill_.Scalar
   op_api: True
 - func: fill_diagonal_
   op_api: False
 - func: flip
   op_api: True
 - func: floor
   op_api: True
 - func: floor.out
   op_api: True
 - func: floor_
   op_api: False
 - func: floor_divide
   op_api: False
 - func: floor_divide.Scalar
   op_api: False
 - func: floor_divide.out
   op_api: False
 - func: floor_divide_.Scalar
   op_api: False
 - func: floor_divide_.Tensor
   op_api: False
 - func: fmod.Scalar
   op_api: False
 - func: fmod.Scalar_out
   op_api: False
 - func: fmod.Tensor
   op_api: False
 - func: fmod.Tensor_out
   op_api: False
 - func: fmod_.Scalar
   op_api: False
 - func: fmod_.Tensor
   op_api: False
 - func: frac
   op_api: False
 - func: frac.out
   op_api: False
 - func: frac_
   op_api: False
 - full
 - full.names
 - full.out
 - func: gather
   op_api: True
 - func: gather.dimname
   op_api: True
 - func: gather.dimname_out
   op_api: True
 - func: gather.out
   op_api: True
 - func: ge.Scalar
   op_api: True
 - func: ge.Scalar_out
   op_api: True
 - func: ge.Tensor
   op_api: True
 - func: ge.Tensor_out
   op_api: True
 - func: ge_.Scalar
   op_api: False
 - func: ge_.Tensor
   op_api: False
 - func: gelu
   op_api: True
 - func: gelu.out
   op_api: True
 - func: gelu_backward
   op_api: True
 - func: ger
   op_api: False
 - func: ger.out
   op_api: False
 - func: glu
   op_api: False
 - func: glu.out
   op_api: False
 - func: glu_backward
   op_api: False
 - func: glu_backward.grad_input
   op_api: False
 - func: grid_sampler_2d
   op_api: True
 - func: grid_sampler_2d_backward
   op_api: True
 - grid_sampler_3d
 - grid_sampler_3d_backward
 - gru.input
 - func: gt.Scalar
   op_api: True
 - func: gt.Scalar_out
   op_api: True
 - func: gt.Tensor
   op_api: True
 - func: gt.Tensor_out
   op_api: True
 - func: gt_.Scalar
   op_api: False
 - func: gt_.Tensor
   op_api: False
 - hamming_window
 - hamming_window.periodic
 - hamming_window.periodic_alpha
 - hamming_window.periodic_alpha_beta
 - hann_window
 - hann_window.periodic
 - hardshrink
 - hardshrink_backward
 - func: hardsigmoid
   op_api: True
 - func: hardsigmoid.out
   op_api: True
 - func: hardsigmoid_
   op_api: True
 - func: hardsigmoid_backward
   op_api: False
 - func: hardswish
   op_api: True
 - func: hardswish.out
   op_api: True
 - func: hardswish_
   op_api: True
 - func: hardswish_backward
   op_api: True
 - func: hardtanh
   op_api: True
 - hardtanh.out
 - func: hardtanh_
   op_api: True
 - func: hardtanh_backward
   op_api: True
 - hardtanh_backward.grad_input
 - func: histc
   op_api: True
 - func: histc.out
   op_api: True
 - im2col
 - im2col.out
 - func: im2col_backward
   op_api: True
 - func: im2col_backward.grad_input
   op_api: True
 - image_normalize
 - image_normalize_
 - img_to_tensor
 - func: index.Tensor
   op_api: True
 - func: index_add
   op_api: True
 - index_add.dimname
 - func: index_add.out
   op_api: True
 - func: index_fill.int_Scalar
   op_api: False
 - func: index_fill.int_Tensor
   op_api: False
 - func: index_fill_.int_Scalar
   op_api: False
 - func: index_fill_.int_Tensor
   op_api: True
 - func: index_select
   op_api: True
 - func: index_put
   op_api: True
 - func: index_put_
   op_api: True
 - func: index_select.dimname
   op_api: False
 - func: index_select.dimname_out
   op_api: False
 - func: index_select.out
   op_api: True
 - func: inverse
   op_api: False
 - func: inverse.out
   op_api: False
 - is_pinned
 - is_set_to
 - func: isclose
   op_api: False
 - isfinite
 - isnan
 - func: kl_div
   op_api: False
 - func: kl_div_backward
   op_api: True
 - func: kthvalue
   op_api: True
 - func: kthvalue.dimname
   op_api: True
 - func: kthvalue.dimname_out
   op_api: True
 - func: kthvalue.values
   op_api: False
 - func: l1_loss
   op_api: False
 - func: l1_loss.out
   op_api: False
 - func: l1_loss_backward
   op_api: False
 - func: l1_loss_backward.grad_input
   op_api: False
 - func: le.Scalar
   op_api: True
 - func: le.Scalar_out
   op_api: True
 - func: le.Tensor
   op_api: True
 - func: le.Tensor_out
   op_api: True
 - func: le_.Scalar
   op_api: False
 - func: le_.Tensor
   op_api: False
 - func: leaky_relu
   op_api: True
 - func: leaky_relu.out
   op_api: True
 - func: leaky_relu_
   op_api: False
 - func: leaky_relu_backward
   op_api: True
 - func: leaky_relu_backward.grad_input
   op_api: True
 - func: lerp.Scalar
   op_api: False
 - func: lerp.Scalar_out
   op_api: False
 - func: lerp.Tensor
   op_api: False
 - func: lerp.Tensor_out
   op_api: False
 - func: lerp_.Scalar
   op_api: False
 - func: lerp_.Tensor
   op_api: False
 - func: linalg_cross
   op_api: True
 - func: linalg_cross.out
   op_api: True
 - linspace
 - linspace.out
 - func: log
   op_api: True
 - func: log.out
   op_api: True
 - func: log10
   op_api: True
 - func: log10.out
   op_api: True
 - func: log10_
   op_api: True
 - func: log1p
   op_api: False
 - func: log1p.out
   op_api: False
 - func: log1p_
   op_api: False
 - func: log2
   op_api: True
 - func: log2.out
   op_api: True
 - func: log2_
   op_api: False
 - func: log_
   op_api: False
 - func: log_sigmoid
   op_api: False
 - func: log_sigmoid.out
   op_api: False
 - log_sigmoid_backward
 - log_sigmoid_backward.grad_input
 - func: log_sigmoid_forward
   op_api: False
 - func: log_sigmoid_forward.output
   op_api: False
 - log_softmax.Dimname
 - log_softmax.int
 - func: logaddexp
   op_api: False
 - func: logaddexp.out
   op_api: False
 - func: logaddexp2
   op_api: False
 - func: logaddexp2.out
   op_api: False
 - func: logical_and
   op_api: True
 - func: logical_and.out
   op_api: True
 - func: logical_and_
   op_api: False
 - func: logical_not
   op_api: False
 - func: logical_not.out
   op_api: False
 - func: logical_not_
   op_api: False
 - func: logical_or
   op_api: True
 - func: logical_or.out
   op_api: True
 - func: logical_or_
   op_api: False
 - func: logical_xor
   op_api: False
 - func: logical_xor.out
   op_api: False
 - logspace
 - logspace.out
 - func: logsumexp
   op_api: False
 - func: logsumexp.names
   op_api: False
 - func: logsumexp.names_out
   op_api: False
 - func: logsumexp.out
   op_api: False
 - lstm.data
 - lstm.input
 - lstm_cell
 - func: lt.Scalar
   op_api: True
 - func: lt.Scalar_out
   op_api: True
 - func: lt.Tensor
   op_api: True
 - func: lt.Tensor_out
   op_api: True
 - func: lt_.Scalar
   op_api: True
 - func: lt_.Tensor
   op_api: True
 - func: masked_fill_.Scalar
   op_api: True
 - func: masked_fill_.Tensor
   op_api: True
 - func: masked_scatter_
   op_api: True
 - func: masked_select
   op_api: True
 - func: masked_select.out
   op_api: True
 - func: max
   op_api: True
 - func: max.dim
   op_api: True
 - func: max.dim_max
   op_api: True
 - max.names_dim
 - max.names_dim_max
 - func: max.out
   op_api: False
 - max_pool2d
 - max_pool2d_with_indices
 - max_pool2d_with_indices.out
 - max_pool2d_with_indices_backward
 - max_pool2d_with_indices_backward.grad_input
 - max_pool3d_with_indices
 - max_pool3d_with_indices.out
 - max_pool3d_with_indices_backward
 - max_pool3d_with_indices_backward.grad_input
 - func: max_unpool2d
   op_api: True
 - func: max_unpool2d.out
   op_api: True
 - func: max_unpool2d_backward
   op_api: True
 - func: max_unpool2d_backward.grad_input
   op_api: True
 - max_unpool3d
 - max_unpool3d.out
 - max_unpool3d_backward
 - max_unpool3d_backward.grad_input
 - func: maximum
   op_api: True
 - func: maximum.out
   op_api: True
 - func: mean
   op_api: True
 - mean.dim
 - mean.names_dim
 - mean.names_out
 - func: mean.out
   op_api: True
 - func: median
   op_api: True
 - median.dim
 - func: median.dim_values
   op_api: True
 - median.names_dim
 - median.names_dim_values
 - func: min
   op_api: True
 - func: min.dim
   op_api: True
 - func: min.dim_min
   op_api: True
 - min.names_dim
 - min.names_dim_min
 - func: min.out
   op_api: False
 - func: minimum
   op_api: True
 - func: minimum.out
   op_api: True
 - mish
 - mish.out
 - mish_
 - func: mish_backward
   op_api: False
 - func: mm
   op_api: True
 - func: mm.out
   op_api: True
 - func: mse_loss
   op_api: True
 - func: mse_loss.out
   op_api: True
 - func: mse_loss_backward
   op_api: True
 - func: mse_loss_backward.grad_input
   op_api: True
 - func: mul.Scalar
   op_api: True
 - func: mul.Tensor
   op_api: True
 - func: mul.out
   op_api: True
 - func: mul_.Scalar
   op_api: True
 - func: mul_.Tensor
   op_api: True
 - multilabel_margin_loss
 - multilabel_margin_loss.out
 - multilabel_margin_loss_forward
 - multilabel_margin_loss_forward.output
 - func: multinomial
   op_api: True
 - func: multinomial.out
   op_api: True
 - mv
 - mv.out
 - func: nanmedian
   op_api: False
 - func: native_batch_norm
   op_api: True
 - func: native_batch_norm.out
   op_api: True
 - func: native_batch_norm_backward
   op_api: True
 - func: native_dropout
   op_api: False
 - func: native_dropout_backward
   op_api: False
 - func: native_layer_norm
   op_api: True
 - func: native_layer_norm_backward
   op_api: True
 - func: ne.Scalar
   op_api: True
 - func: ne.Scalar_out
   op_api: True
 - func: ne.Tensor
   op_api: True
 - func: ne.Tensor_out
   op_api: True
 - ne_.Scalar
 - ne_.Tensor
 - func: neg
   op_api: True
 - func: neg.out
   op_api: True
 - func: neg_
   op_api: False
 - nll_loss
 - nll_loss.out
 - nll_loss2d
 - nll_loss2d.out
 - func: nll_loss2d_backward
   op_api: True
 - func: nll_loss2d_backward.grad_input
   op_api: True
 - func: nll_loss2d_forward
   op_api: True
 - func: nll_loss2d_forward.output
   op_api: True
 - func: nll_loss_backward
   op_api: True
 - func: nll_loss_backward.grad_input
   op_api: True
 - func: nll_loss_forward
   op_api: True
 - func: nll_loss_forward.output
   op_api: True
 - func: nonzero
   op_api: True
 - func: nonzero.out
   op_api: True
 - norm.Scalar
 - norm.ScalarOpt_dim
 - norm.ScalarOpt_dim_dtype
 - norm.ScalarOpt_dtype
 - norm.dtype_out
 - func: norm.out
   op_api: True
 - func: normal.Tensor_Tensor
   op_api: False
 - func: normal.Tensor_Tensor_out
   op_api: False
 - func: normal.Tensor_float
   op_api: False
 - func: normal.Tensor_float_out
   op_api: False
 - func: normal.float_Tensor
   op_api: False
 - func: normal.float_Tensor_out
   op_api: False
 - func: normal.float_float
   op_api: False
 - func: normal.float_float_out
   op_api: False
 - func: normal_
   op_api: True
 - func: one_
   op_api: True
 - func: one_hot
   op_api: False
 - func: ones
   op_api: True
 - func: ones.names
   op_api: True
 - func: ones.out
   op_api: True
 - func: ones_like
   op_api: True
 - pdist
 - func: pow.Scalar
   op_api: True
 - func: pow.Scalar_out
   op_api: True
 - func: pow.Tensor_Scalar
   op_api: True
 - func: pow.Tensor_Scalar_out
   op_api: True
 - func: pow.Tensor_Tensor
   op_api: True
 - func: pow.Tensor_Tensor_out
   op_api: True
 - func: pow_.Scalar
   op_api: False
 - func: pow_.Tensor
   op_api: False
 - func: prelu
   op_api: True
 - prelu_backward
 - func: prod
   op_api: True
 - prod.Dimname_out
 - prod.dim_Dimname
 - func: prod.dim_int
   op_api: True
 - func: prod.int_out
   op_api: True
 - func: put_
   op_api: True
 - func: qr
   op_api: False
 - func: qr.Q
   op_api: False
 - quantize_per_channel
 - quantize_per_tensor
 - func: random_
   op_api: True
 - func: random_.from
   op_api: True
 - func: random_.to
   op_api: True
 - func: randperm
   op_api: True
 - func: randperm.generator
   op_api: True
 - func: randperm.generator_out
   op_api: True
 - func: randperm.out
   op_api: True
 - range
 - func: range.out
   op_api: True
 - range.step
 - func: reciprocal
   op_api: True
 - func: reciprocal.out
   op_api: True
 - func: reciprocal_
   op_api: True
 - func: reflection_pad1d
   op_api: True
 - func: reflection_pad1d.out
   op_api: True
 - func: reflection_pad1d_backward
   op_api: False
 - func: reflection_pad1d_backward.grad_input
   op_api: False
 - func: reflection_pad2d
   op_api: True
 - func: reflection_pad2d.out
   op_api: True
 - func: reflection_pad2d_backward
   op_api: True
 - func: reflection_pad2d_backward.grad_input
   op_api: True
 - func: relu
   op_api: True
 - func: relu_
   op_api: True
 - func: remainder.Scalar
   op_api: False
 - func: remainder.Scalar_out
   op_api: False
 - func: remainder.Tensor
   op_api: False
 - func: remainder.Tensor_out
   op_api: False
 - func: remainder_.Scalar
   op_api: False
 - func: remainder_.Tensor
   op_api: False
 - func: remainder.Scalar_Tensor
   op_api: False
 - func: renorm
   op_api: False
 - func: renorm.out
   op_api: False
 - func: renorm_
   op_api: False
 - func: repeat
   op_api: False
 - func: repeat_interleave.self_Tensor
   op_api: True
 - func: repeat_interleave.self_int
   op_api: True
 - func: replication_pad1d
   op_api: False
 - func: replication_pad1d.out
   op_api: False
 - func: replication_pad1d_backward
   op_api: False
 - func: replication_pad1d_backward.grad_input
   op_api: False
 - func: replication_pad2d
   op_api: False
 - func: replication_pad2d.out
   op_api: False
 - func: replication_pad2d_backward
   op_api: True
 - func: replication_pad2d_backward.grad_input
   op_api: True
 - resize_
 - resize_as_
 - reverse
 - func: roll
   op_api: True
 - func: round
   op_api: True
 - func: round.out
   op_api: True
 - func: round_
   op_api: True
 - func: rrelu_with_noise
   op_api: True
 - func: rrelu_with_noise.out
   op_api: True
 - func: rrelu_with_noise_
   op_api: True
 - rrelu_with_noise_backward
 - func: square.out
   op_api: True
 - func: rsqrt
   op_api: False
 - func: rsqrt.out
   op_api: False
 - func: rsqrt_
   op_api: False
 - func: rsub.Scalar
   op_api: False
 - func: rsub.Tensor
   op_api: False
 - func: scalar_tensor
   op_api: False
 - func: scatter.src_out
   op_api: True
 - func: scatter.value_out
   op_api: True
 - func: scatter.reduce_out
   op_api: True
 - func: scatter.value_reduce_out
   op_api: True
 - func: scatter_add
   op_api: True
 - func: scatter_add.dimname
   op_api: True
 - func: scatter_add_
   op_api: True
 - func: searchsorted.Scalar
   op_api: True
 - func: searchsorted.Tensor
   op_api: True
 - func: searchsorted.Tensor_out
   op_api: True
 - set_
 - set_.source_Storage
 - set_.source_Storage_storage_offset
 - set_.source_Tensor
 - func: sgn
   op_api: True
 - func: sgn.out
   op_api: True
 - func: sigmoid
   op_api: True
 - func: sigmoid.out
   op_api: True
 - func: sigmoid_
   op_api: True
 - func: sigmoid_backward
   op_api: True
 - func: sigmoid_backward.grad_input
   op_api: True
 - func: sign
   op_api: True
 - func: sign.out
   op_api: True
 - func: sign_
   op_api: True
 - func: sin
   op_api: True
 - func: sin.out
   op_api: True
 - func: sin_
   op_api: True
 - func: sinh
   op_api: False
 - func: sinh.out
   op_api: False
 - func: sinh_
   op_api: False
 - func: linalg_slogdet
   op_api: False
 - func: linalg_slogdet.out
   op_api: False
 - slow_conv3d
 - slow_conv3d.out
 - slow_conv3d_forward
 - slow_conv3d_forward.output
 - slow_conv_dilated2d
 - slow_conv_dilated2d_backward
 - slow_conv_transpose2d
 - slow_conv_transpose2d.out
 - slow_conv_transpose2d_backward
 - func: smooth_l1_loss
   op_api: True
 - func: smooth_l1_loss.out
   op_api: True
 - func: smooth_l1_loss_backward
   op_api: True
 - func: smooth_l1_loss_backward.grad_input
   op_api: True
 - soft_margin_loss
 - soft_margin_loss.out
 - soft_margin_loss_backward
 - soft_margin_loss_backward.grad_input
 - softmax.Dimname
 - softmax.int
 - func: softplus
   op_api: True
 - func: softplus.out
   op_api: True
 - func: softplus_backward.grad_input
   op_api: False
 - softshrink
 - softshrink.out
 - softshrink_backward
 - softshrink_backward.grad_input
 - func: sort
   op_api: True
 - func: sort.stable
   op_api: True
 - func: sort.dimname
   op_api: True
 - func: sort.dimname_values
   op_api: True
 - func: sort.values
   op_api: True
 - func: sort.values_stable
   op_api: True
 - func: sqrt
   op_api: True
 - func: sqrt.out
   op_api: True
 - func: sqrt_
   op_api: True
 - squeeze
 - squeeze.dim
 - func: stack
   op_api: True
 - func: stack.out
   op_api: True
 - func: std.correction
   op_api: True
 - func: std.correction_out
   op_api: True
 - func: std.correction_names
   op_api: True
 - func: std.correction_names_out
   op_api: True
 - func: std.names_out
   op_api: True
 - func: std.out
   op_api: True
 - std_mean.correction
 - std_mean.correction_names
 - func: sub.Scalar
   op_api: True
 - func: sub.Tensor
   op_api: True
 - func: sub.out
   op_api: True
 - func: sub_.Scalar
   op_api: False
 - func: sub_.Tensor
   op_api: False
 - func: sum
   op_api: True
 - func: sum.DimnameList_out
   op_api: True
 - func: sum.IntList_out
   op_api: True
 - func: sum.dim_DimnameList
   op_api: True
 - func: sum.dim_IntList
   op_api: True
 - func: take
   op_api: True
 - func: take.out
   op_api: True
 - func: tanh
   op_api: True
 - func: tanh.out
   op_api: True
 - func: tanh_
   op_api: True
 - func: tan
   op_api: True
 - func: tan.out
   op_api: True
 - func: tan_
   op_api: True
 - func: tanh_backward
   op_api: True
 - func: tanh_backward.grad_input
   op_api: True
 - func: threshold
   op_api: True
 - func: threshold.out
   op_api: True
 - func: threshold_
   op_api: True
 - func: threshold_backward
   op_api: True
 - to.device
 - func: to.dtype
   op_api: True
 - to.dtype_layout
 - to.other
 - func: topk
   op_api: True
 - func: topk.values
   op_api: True
 - triangular_solve.X
 - func: tril
   op_api: True
 - func: tril.out
   op_api: True
 - func: tril_
   op_api: True
 - tril_indices
 - func: triu
   op_api: True
 - func: triu.out
   op_api: True
 - func: triu_
   op_api: True
 - triu_indices
 - func: true_divide.Scalar
   op_api: False
 - func: true_divide.Tensor
   op_api: False
 - func: true_divide.out
   op_api: False
 - true_divide_.Scalar
 - true_divide_.Tensor
 - func: trace
   op_api: True
 - func: trunc
   op_api: False
 - func: trunc.out
   op_api: False
 - func: trunc_
   op_api: False
 - unfold
 - func: uniform_
   op_api: True
 - func: unique_consecutive
   op_api: False
 - unsqueeze
 - upsample_bicubic2d
 - upsample_bicubic2d.out
 - upsample_bicubic2d.vec
 - upsample_bicubic2d_backward
 - upsample_bicubic2d_backward.grad_input
 - upsample_bicubic2d_backward.vec
 - func: upsample_bilinear2d
   op_api: True
 - func: upsample_bilinear2d.out
   op_api: True
 - func: upsample_bilinear2d.vec
   op_api: True
 - func: upsample_bilinear2d_backward
   op_api: False
 - func: upsample_bilinear2d_backward.grad_input
   op_api: False
 - func: upsample_bilinear2d_backward.vec
   op_api: False
 - upsample_linear1d
 - upsample_linear1d.out
 - upsample_linear1d.vec
 - upsample_linear1d_backward
 - upsample_linear1d_backward.vec
 - func: upsample_nearest1d
   op_api: True
 - func: upsample_nearest1d.out
   op_api: True
 - func: upsample_nearest1d.vec
   op_api: True
 - func: upsample_nearest1d_backward
   op_api: False
 - func: upsample_nearest1d_backward.grad_input
   op_api: False
 - func: upsample_nearest1d_backward.vec
   op_api: False
 - func: upsample_nearest2d
   op_api: True
 - func: upsample_nearest2d.out
   op_api: True
 - func: upsample_nearest2d.vec
   op_api: True
 - func: upsample_nearest2d_backward
   op_api: False
 - func: upsample_nearest2d_backward.grad_input
   op_api: False
 - func: upsample_nearest2d_backward.vec
   op_api: False
 - upsample_nearest3d
 - upsample_nearest3d.out
 - upsample_nearest3d.vec
 - upsample_nearest3d_backward
 - upsample_nearest3d_backward.grad_input
 - upsample_nearest3d_backward.vec
 - upsample_trilinear3d
 - upsample_trilinear3d.out
 - upsample_trilinear3d.vec
 - upsample_trilinear3d_backward
 - upsample_trilinear3d_backward.grad_input
 - upsample_trilinear3d_backward.vec
 - func: var.correction
   op_api: False
 - func: var.correction_names
   op_api: False
 - func: var.out
   op_api: False
 - func: var.names_out
   op_api: False
 - func: var.correction_out
   op_api: False
 - func: var.correction_names_out
   op_api: False
 - var_mean.correction
 - var_mean.correction_names
 - view
 - view_as_complex
 - view_as_real
 - where
 - func: where.self
   op_api: True
 - func: xlogy.OutScalar_Other
   op_api: False
 - func: xlogy.OutScalar_Self
   op_api: False
 - func: xlogy.OutTensor
   op_api: False
 - func: xlogy.Scalar_Other
   op_api: False
 - func: xlogy.Scalar_Self
   op_api: False
 - func: xlogy.Tensor
   op_api: False
 - func: xlogy_.Scalar_Other
   op_api: False
 - func: xlogy_.Tensor
   op_api: False
 - func: zero_
   op_api: True
 - func: zeros
   op_api: True
 - func: zeros.names
   op_api: True
 - func: zeros.out
   op_api: True
 - func: zeros_like
   op_api: False
 - func: silu
   op_api: False
 - func: silu_
   op_api: False
 - func: silu.out
   op_api: False
 - func: silu_backward.grad_input
   op_api: False
 - func: silu_backward
   op_api: False

autograd:
  - func: matmul
    op_api: False
  - func: matmul.out
    op_api: False
  - celu
  - celu_
  - func: elu.out
    op_api: False
  - func: elu
    op_api: False
  - func: elu_
    op_api: False
  - func: binary_cross_entropy_with_logits_backward
    op_api: False
  - func: selu
    op_api: False
  - func: selu_
    op_api: False

tocpu:
  - _cholesky_helper
  - _cholesky_solve_helper
  - _compute_linear_combination
  - _compute_linear_combination.out
  - _dirichlet_grad
  - _embedding_bag_per_sample_weights_backward
  - _efficientzerotensor
  - _fft_r2c
  - _fft_r2c.out
  - _fft_c2r
  - _fft_c2r.out
  - _fft_c2c
  - _fft_c2c.out
  - _foreach_add.Scalar
  - _foreach_add_.Scalar
  - _foreach_sub.Scalar
  - _foreach_sub_.Scalar
  - _foreach_mul.Scalar
  - _foreach_mul_.Scalar
  - _foreach_div.Scalar
  - _foreach_div_.Scalar
  - _foreach_add.List
  - _foreach_add_.List
  - _foreach_sub.List
  - _foreach_sub_.List
  - _foreach_mul.List
  - _foreach_mul_.List
  - _foreach_div.List
  - _foreach_div_.List
  - _foreach_add.ScalarList
  - _foreach_add_.ScalarList
  - _foreach_sub.ScalarList
  - _foreach_sub_.ScalarList
  - _foreach_div.ScalarList
  - _foreach_div_.ScalarList
  - _foreach_mul.ScalarList
  - _foreach_mul_.ScalarList
  - _foreach_exp
  - _foreach_zero_
  - _foreach_exp_
  - _foreach_sqrt
  - _foreach_sqrt_
  - _foreach_abs
  - _foreach_abs_
  - _foreach_acos
  - _foreach_acos_
  - _foreach_asin
  - _foreach_asin_
  - _foreach_atan
  - _foreach_atan_
  - _foreach_ceil
  - _foreach_ceil_
  - _foreach_cos
  - _foreach_cos_
  - _foreach_cosh
  - _foreach_cosh_
  - _foreach_erf
  - _foreach_erf_
  - _foreach_erfc
  - _foreach_erfc_
  - _foreach_expm1
  - _foreach_expm1_
  - _foreach_floor
  - _foreach_floor_
  - _foreach_log
  - _foreach_log_
  - _foreach_log10
  - _foreach_log10_
  - _foreach_log1p
  - _foreach_log1p_
  - _foreach_log2
  - _foreach_log2_
  - _foreach_neg
  - _foreach_neg_
  - _foreach_tan
  - _foreach_tan_
  - _foreach_tanh
  - _foreach_tanh_
  - _foreach_sin
  - _foreach_sin_
  - _foreach_sinh
  - _foreach_sinh_
  - _foreach_round
  - _foreach_round_
  - _foreach_lgamma
  - _foreach_lgamma_
  - _foreach_frac
  - _foreach_frac_
  - _foreach_reciprocal
  - _foreach_reciprocal_
  - _foreach_sigmoid
  - _foreach_sigmoid_
  - _foreach_trunc
  - _foreach_trunc_
  - _foreach_addcdiv_.Scalar
  - _foreach_addcmul_.Scalar
  - _foreach_addcdiv_.ScalarList
  - _foreach_addcmul_.ScalarList
  - _foreach_addcdiv.Scalar
  - _foreach_addcmul.Scalar
  - _foreach_addcdiv.ScalarList
  - _foreach_addcmul.ScalarList
  - _foreach_maximum.List
  - _foreach_minimum.List
  - _linalg_inv_out_helper_
  - _linalg_qr_helper
  - _logcumsumexp
  - _logcumsumexp.out
  - _pdist_backward
  - _sample_dirichlet
  - _standard_gamma_grad
  - _standard_gamma
  - _test_optional_intlist
  - _test_optional_filled_intlist
  - _test_optional_floatlist
  - _solve_helper
  - _unique
  - angle
  - angle.out
  - binomial
  - bucketize.Tensor
  - bucketize.Tensor_out
  - bucketize.Scalar
  - cauchy_
  - cholesky_inverse
  - cholesky_inverse.out
  - complex.out
  - exponential_
  - geometric_
  - linalg_vector_norm
  - linalg_vector_norm.out
  - log_normal_
  - logit
  - logit_
  - logit.out
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_backward
  - multi_margin_loss.out
  - multi_margin_loss
  - multi_margin_loss_backward.grad_input
  - multi_margin_loss_backward
  - mode
  - nanmedian.dim_values
  - nansum
  - nansum.dim_IntList
  - nansum.IntList_out
  - native_group_norm_backward
  - narrow_copy.out
  - poisson
  - polar.out
  - replication_pad3d_backward.grad_input
  - replication_pad3d_backward
  - repeat_interleave.Tensor
  - slow_conv_dilated3d
  - slow_conv_transpose3d.out
  - slow_conv_transpose3d
  - sspaddmm.out
  - tensordot.out
  - to_sparse.sparse_dim
  - to_sparse
  - to_mkldnn
  - unfold_backward
  - unique_dim
  - unique_dim_consecutive
  - vdot
  - sinc.out
  - gcd.out
  - lcm.out
  - isposinf.out
  - isneginf.out
  - fmin
  - fmin.out
  - fmax
  - fmax.out
  - copysign.Tensor
  - copysign_.Tensor
  - copysign.out
  - copysign.Scalar
  - copysign_.Scalar
  - matrix_exp
  - _empty_affine_quantized
  - _empty_per_channel_affine_quantized
  - lgamma.out
  - lgamma_
  - lgamma
  - digamma.out
  - digamma
  - polygamma.out
  - igamma.out
  - igamma
  - igamma_
  - igammac.out
  - igammac
  - igammac_
  - i0.out
  - signbit.out
  - hypot.out
  - hypot
  - nextafter.out
  - nextafter
  - batch_norm_update_stats
  - heaviside.out
  - scatter_.reduce
  - scatter_.value_reduce
  - digamma_
  - adaptive_max_pool3d.out
  - adaptive_max_pool3d
  - adaptive_max_pool3d_backward.grad_input
  - adaptive_max_pool3d_backward
  - fractional_max_pool3d.output
  - fractional_max_pool3d
  - fractional_max_pool3d_backward.grad_input
  - fractional_max_pool3d_backward
  - fractional_max_pool2d.output
  - fractional_max_pool2d
  - fractional_max_pool2d_backward.grad_input
  - fractional_max_pool2d_backward
  - replication_pad3d.out
  - replication_pad3d
  - logit_backward.grad_input
  - logit_backward
  - orgqr.out
  - orgqr
  - _lu_with_info

unsupported:
  - _conj
  - conj
  - bitwise_left_shift.Tensor
  - bitwise_left_shift_.Tensor
  - bitwise_left_shift.Tensor_out
  - bitwise_left_shift.Tensor_Scalar
  - bitwise_left_shift_.Tensor_Scalar
  - bitwise_left_shift.Tensor_Scalar_out
  - bitwise_left_shift.Scalar_Tensor
  - bitwise_right_shift.Tensor
  - bitwise_right_shift_.Tensor
  - bitwise_right_shift.Tensor_out
  - bitwise_right_shift.Tensor_Scalar
  - bitwise_right_shift_.Tensor_Scalar
  - bitwise_right_shift.Tensor_Scalar_out
  - bitwise_right_shift.Scalar_Tensor
  - _conj_physical
  - conj_physical
  - conj_physical.out
  - conj_physical_
  - frexp.Tensor
  - frexp.Tensor_out
  - isin.Tensor_Tensor_out
  - isin.Tensor_Tensor
  - isin.Tensor_Scalar_out
  - isin.Tensor_Scalar
  - isin.Scalar_Tensor_out
  - isin.Scalar_Tensor
  - cholesky.out
  - cholesky
  - geqrf.a
  - geqrf
  - logdet
  - linalg_lu_factor_ex
  - linalg_lu_factor_ex.out
  - lu_solve.out
  - lu_solve
  - lu_unpack
  - lu_unpack.out
  - _det_lu_based_helper
  - linalg_det
  - linalg_det.out
  - linalg_cholesky_ex
  - linalg_cholesky_ex.L
  - linalg_eig
  - linalg_eig.out
  - linalg_eigh
  - linalg_eigh.eigvals
  - linalg_eigvalsh
  - linalg_eigvalsh.out
  - linalg_solve_triangular.out
  - linalg_solve_triangular
  - linalg_lstsq
  - linalg_lstsq.out
  - special_entr
  - special_entr.out
  - special_erfcx
  - special_erfcx.out
  - special_zeta
  - special_zeta.self_scalar
  - special_zeta.other_scalar
  - special_zeta.out
  - special_zeta.self_scalar_out
  - special_zeta.other_scalar_out

custom:
  - func: _npu_storage_resize(Tensor self, int size) -> Tensor
  - func: npu_change_data_ptr(Tensor dst, Tensor src, int index) -> int
  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: npu_alloc_float_status(Tensor self) -> Tensor
  - func: npu_get_float_status(Tensor self) -> Tensor
  - func: npu_clear_float_status(Tensor self) -> Tensor
  - func: one_(Tensor(a!) self) -> Tensor(a!)
  - func: fast_gelu(Tensor self) -> Tensor
  - func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
  - func: _amp_foreach_non_finite_check_(Tensor[] scaled_grads) -> bool
  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: unsafe_empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> Tensor[]
  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3, bool try_recover_truncated=False) -> Tensor
  - func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
  - func: reverse(Tensor self, int[] axis) -> Tensor
  - func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor
  - func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)
  - func: img_to_tensor(Tensor self) -> Tensor
  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)
  - func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
  - func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_flash_attention_grad(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, bool gen_mask_parallel=True, bool sync=False) -> Tensor[]
  - func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]
  - func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]
  - func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)
  - func: format_contiguous(Tensor self) -> Tensor
  - func: check_match(Tensor self) -> bool
  - func: check_memory_overlaps(Tensor[] inputs, Tensor[] outputs) -> ()
  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
  - func: get_storage_size(Tensor self) -> int
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_enque_tensor(Tensor[] tensors, str format_string, int capacity=3) -> ()
  - func: npu_hcom_allgather(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allgather.out(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: bscpp_add(Tensor self, Tensor other) -> Tensor
    bscpp_op: True
  - func: npu_rotary_mul(Tensor self, Tensor r1, Tensor r2) -> Tensor
  - func: npu_rotary_mul_backward(Tensor grad, Tensor self, Tensor r1, Tensor r2) -> (Tensor, Tensor, Tensor)

custom_autograd:
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
  - func: npu_silu(Tensor self) -> Tensor
  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> Tensor[]
  - func: npu_mish(Tensor self) -> Tensor
  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seqMask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flagSeq, bool direction) -> Tensor[]
  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor[]
  - func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor
  - func: npu_flash_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, bool gen_mask_parallel=True, bool sync=False) -> Tensor[]
