backend: XLA
cpp_namespace: at_npu::native
supported:
  - _cast_Byte
  - _cast_Char
  - _cast_Double
  - _cast_Float
  - _cast_Int
  - _cast_Long
  - _cast_Short
  - _cast_Half
  - _backward
  - set_data
  - data
  - is_leaf
  - output_nr
  - _version
  - requires_grad_
  - retain_grad
  - _fw_primal
  - _make_dual
  - _unpack_dual
  - rename_
  - rename
  - align_to
  - align_to.ellipsis_idx
  - align_as
  - align_tensors
  - refine_names
  - _use_cudnn_ctc_loss
  - _cudnn_ctc_loss
  - _use_cudnn_rnn_flatten_weight
  - _cudnn_rnn_flatten_weight
  - _cudnn_rnn
  - _cudnn_rnn_backward
  - _cudnn_init_dropout_state
  - _debug_has_internal_overlap
  - _fused_dropout
  - _masked_scale
  - _sobol_engine_draw
  - _sobol_engine_ff_
  - _sobol_engine_scramble_
  - _sobol_engine_initialize_state_
  - _reshape_from_tensor
  - _shape_as_tensor
  - dropout
  - dropout_
  - feature_dropout
  - feature_dropout_
  - alpha_dropout
  - alpha_dropout_
  - feature_alpha_dropout
  - feature_alpha_dropout_
  - abs
  - abs_
  - abs.out
  - absolute
  - absolute_
  - absolute.out
  - angle
  - angle.out
  - view_as_real
  - view_as_complex
  - sgn
  - sgn_
  - sgn.out
  - real
  - imag
  - conj
  # - conj.out
  - _conj
  - acos
  - acos_
  - acos.out
  - arccos
  - arccos_
  - arccos.out
  - avg_pool1d
  - adaptive_avg_pool1d
  - adaptive_max_pool1d
  - add.Tensor
  - add_.Tensor
  - add.out
  - _add_relu.Tensor
  - _add_relu_.Tensor
  - _add_relu.out
  - add.Scalar
  - add_.Scalar
  - addmv
  - addmv_
  - addmv.out
  # - _addmv_impl_
  - addr
  - addr_
  - addr.out
  - affine_grid_generator
  - affine_grid_generator_backward
  - all.dim
  - all.out
  - allclose
  - any.dim
  - any.out
  - arange
  - arange.start
  - arange.start_step
  - arange.out
  - arange.start_out
  - _dim_arange
  - argmax
  - argmax.out
  - argmin
  - argmin.out
  - acosh
  - acosh_
  - acosh.out
  - arccosh
  - arccosh_
  - arccosh.out
  - asinh
  - asinh_
  - asinh.out
  - arcsinh
  - arcsinh_
  - arcsinh.out
  - atanh
  - atanh_
  - atanh.out
  - arctanh
  - arctanh_
  - arctanh.out
  - as_strided
  - as_strided_
  - asin
  - asin_
  - asin.out
  - arcsin
  - arcsin_
  - arcsin.out
  - atan
  - atan_
  - atan.out
  - arctan
  - arctan_
  - arctan.out
  - atleast_1d
  - atleast_1d.Sequence
  - atleast_2d
  - atleast_2d.Sequence
  - atleast_3d
  - atleast_3d.Sequence
  - baddbmm
  - baddbmm_
  # - _baddbmm_mkl_
  - baddbmm.out
  - bartlett_window
  - bartlett_window.periodic
  - batch_norm
  - quantized_batch_norm
  - _batch_norm_impl_index
  - _batch_norm_impl_index_backward
  - bernoulli
  - bernoulli.out
  - bernoulli_.Tensor
  - bernoulli_.float
  - bernoulli.p
  - bilinear
  - binary_cross_entropy
  - binary_cross_entropy.out
  - binary_cross_entropy_backward
  - binary_cross_entropy_backward.grad_input
  - binary_cross_entropy_with_logits
  - binary_cross_entropy_with_logits_backward
  - bincount
  - bitwise_not
  - bitwise_not_
  - bitwise_not.out
  - copysign.Tensor
  - copysign_.Tensor
  - copysign.out
  - copysign.Scalar
  - copysign_.Scalar
  - logical_not
  - logical_not_
  - logical_not.out
  - logical_xor
  - logical_xor_
  - logical_xor.out
  - logical_and
  - logical_and_
  - logical_and.out
  - logical_or
  - logical_or_
  - logical_or.out
  - blackman_window
  - blackman_window.periodic
  - bmm
  - bmm.out
  - broadcast_tensors
  - broadcast_to
  - cat
  - cat.out
  - cat.names
  - cat.names_out
  - block_diag
  - ceil
  - ceil_
  - ceil.out
  - chain_matmul
  - unsafe_chunk
  - chunk
  - tensor_split.sections
  - tensor_split.indices
  - tensor_split.tensor_indices_or_sections
  - clamp
  - clamp_
  - clamp.out
  - clamp_max
  - clamp_max_
  - clamp_max.out
  - clamp_min
  - clamp_min_
  - clamp_min.out
  - clip
  - clip_
  - clip.out
  - cudnn_is_acceptable
  - complex
  - complex.out
  - polar
  - polar.out
  - constant_pad_nd
  - contiguous
  - convolution
  - convolution_backward
  - convolution_overrideable
  - convolution_backward_overrideable
  - _convolution
  - _convolution_double_backward
  - _conv_depthwise2d
  - _conv_depthwise2d.out
  - conv1d
  - conv2d
  - conv3d
  - conv_tbc
  - conv_tbc_backward
  - conv_transpose1d
  - conv_transpose2d.input
  - conv_transpose3d.input
  - copy_
  - _copy_from
  - cos
  - cos_
  - cos.out
  - cosh
  - cosh_
  - cosh.out
  - cosine_embedding_loss
  - count_nonzero.dim_IntList
  - count_nonzero
  - cudnn_affine_grid_generator
  - cudnn_affine_grid_generator_backward
  - cudnn_batch_norm
  - cudnn_batch_norm_backward
  - cudnn_convolution
  - cudnn_grid_sampler
  - cudnn_grid_sampler_backward
  - cummax
  - cummax.out
  - cummax.dimname
  - cummax.dimname_out
  - _cummax_helper
  - cummin
  - cummin.out
  - cummin.dimname
  - cummin.dimname_out
  - _cummin_helper
  - cummaxmin_backward
  - cumprod
  - cumprod_
  - cumprod.out
  - cumprod.dimname
  - cumprod_.dimname
  - cumprod.dimname_out
  - cumprod_backward
  - cumsum
  - cumsum_
  - cumsum.out
  - cumsum.dimname
  - cumsum_.dimname
  - cumsum.dimname_out
  - ctc_loss.IntList
  - ctc_loss.Tensor
  - _ctc_loss
  - _ctc_loss_backward
  - diag_embed
  - diagflat
  - diagonal
  - diagonal.Dimname
  - diagonal_backward
  - fill_diagonal_
  - diff
  - diff.out
  - div.Tensor
  - div_.Tensor
  - div.out
  - div.out_mode
  - div.Scalar
  - div.Scalar_mode
  - div_.Scalar
  - div.Tensor_mode
  - div_.Tensor_mode
  - div_.Scalar_mode
  - divide.Tensor
  - divide_.Tensor
  - divide.out
  - divide.Scalar
  - divide_.Scalar
  - divide.Tensor_mode
  - divide_.Tensor_mode
  - divide.out_mode
  - divide.Scalar_mode
  - divide_.Scalar_mode
  - true_divide.Tensor
  - true_divide_.Tensor
  - true_divide.out
  - true_divide.Scalar
  - true_divide_.Scalar
  - dot
  - dot.out
  - vdot
  - vdot.out
  - einsum
  - embedding
  - embedding_backward
  - embedding_dense_backward
  - embedding_renorm_
  - embedding_sparse_backward
  - _embedding_bag_forward_only
  - _rowwise_prune
  - row_stack
  - row_stack.out
  - embedding_bag
  - _embedding_bag
  - _embedding_bag_backward
  - _embedding_bag_sparse_backward
  - _embedding_bag_dense_backward
  - _embedding_bag_per_sample_weights_backward
  - empty.memory_format
  - new_empty
  - new_empty_strided
  - new_full
  - new_zeros
  - _empty_affine_quantized
  - _empty_per_channel_affine_quantized
  - resize_
  - empty_quantized
  - empty.out
  - empty_like
  - empty_strided
  - erf
  - erf_
  - erf.out
  - erfc
  - erfc_
  - erfc.out
  - exp
  - exp_
  - exp.out
  - exp2
  - exp2_
  - exp2.out
  - expm1
  - expm1_
  - expm1.out
  - expand
  - expand_as
  - eye
  - eye.m
  - eye.out
  - eye.m_out
  - flatten.using_ints
  - flatten.named_out_dim
  - flatten.using_names
  - flatten.DimnameList
  - unflatten.int
  - unflatten.Dimname
  - fill_.Scalar
  - fill_.Tensor
  - floor
  - floor_
  - floor.out
  - floor_divide
  - floor_divide_.Tensor
  - floor_divide.out
  - floor_divide.Scalar
  - floor_divide_.Scalar
  - frac
  - frac_
  - frac.out
  - full.names
  - full
  - full.out
  - full_like
  - from_file
  - gcd.out
  - gcd
  - gcd_
  - lcm.out
  - lcm
  - lcm_
  - grid_sampler
  - grid_sampler_2d
  - grid_sampler_2d_backward
  - _grid_sampler_2d_cpu_fallback
  - _grid_sampler_2d_cpu_fallback_backward
  - grid_sampler_3d
  - grid_sampler_3d_backward
  - hann_window
  - hann_window.periodic
  - hamming_window
  - hamming_window.periodic
  - hamming_window.periodic_alpha
  - hamming_window.periodic_alpha_beta
  - kaiser_window
  - kaiser_window.periodic
  - kaiser_window.beta
  - hinge_embedding_loss
  - group_norm
  - native_group_norm
  - native_group_norm_backward
  - _fft_r2c
  - _fft_r2c.out
  - _fft_c2r
  - _fft_c2r.out
  - _fft_c2c
  - _fft_c2c.out
  - _cufft_get_plan_cache_size
  - _cufft_get_plan_cache_max_size
  - _cufft_set_plan_cache_max_size
  - _cufft_clear_plan_cache
  - index.Tensor
  - index_copy_
  - index_copy
  - index_copy_.dimname
  - index_copy.dimname
  - index_put_
  - index_put
  - _index_put_impl_
  - instance_norm
  - inverse
  - inverse.out
  - isclose
  - isnan
  - is_distributed
  - is_floating_point
  - is_complex
  - isreal
  - is_nonzero
  - is_same_size
  - is_signed
  - kl_div
  - kl_div_backward
  - kron
  - kron.out
  - kthvalue
  - kthvalue.values
  - kthvalue.dimname
  - kthvalue.dimname_out
  - layer_norm
  - native_layer_norm
  - native_layer_norm_backward
  - nan_to_num
  - nan_to_num_
  - nan_to_num.out
  - linear
  - mkldnn_linear
  - mkldnn_linear_backward_input
  - mkldnn_linear_backward_weights
  - mkldnn_linear_backward
  - fbgemm_linear_int8_weight_fp32_activation
  - fbgemm_linear_int8_weight
  - fbgemm_linear_quantize_weight
  - fbgemm_pack_gemm_matrix_fp16
  - fbgemm_linear_fp16_weight_fp32_activation
  - fbgemm_linear_fp16_weight
  - fbgemm_pack_quantized_matrix
  - fbgemm_pack_quantized_matrix.KN
  - ldexp.Tensor
  - ldexp_
  - ldexp.out
  - linspace
  - linspace.out
  - log
  - log_
  - log.out
  - log10
  - log10_
  - log10.out
  - log1p
  - log1p_
  - log1p.out
  - log2
  - log2_
  - log2.out
  - logaddexp.out
  - logaddexp
  - logaddexp2.out
  - logaddexp2
  - xlogy.Tensor
  - xlogy.Scalar_Self
  - xlogy.Scalar_Other
  - xlogy_.Tensor
  - xlogy_.Scalar_Other
  - xlogy.OutTensor
  - xlogy.OutScalar_Self
  - xlogy.OutScalar_Other
  - logdet
  - logspace
  - logspace.out
  - log_softmax.int
  - log_softmax.Dimname
  - _log_softmax
  - _log_softmax_backward_data
  - _log_softmax_backward_data.out
  - _logcumsumexp
  - _logcumsumexp.out
  - logcumsumexp
  - logcumsumexp.out
  - logcumsumexp.dimname
  - logcumsumexp.dimname_out
  - logsumexp
  - logsumexp.out
  - logsumexp.names
  - logsumexp.names_out
  - margin_ranking_loss
  - matmul
  - matmul.out
  - matrix_rank.tol
  - matrix_rank
  - matrix_power
  - matrix_exp
  - matrix_exp_backward
  - _aminmax
  - _aminmax.dim
  - _compute_linear_combination
  - _compute_linear_combination.out
  - max.dim
  - max.dim_max
  - maximum
  - maximum.out
  - max.names_dim
  - max.names_dim_max
  - value_selecting_reduction_backward
  - amax
  - amax.out
  - max_pool1d_with_indices
  - max_pool1d
  - max_pool2d
  - mkldnn_max_pool2d
  - mkldnn_max_pool3d
  - quantized_max_pool1d
  - quantized_max_pool2d
  - max_pool3d
  - mean
  - mean.dim
  - mean.out
  - mean.names_dim
  - mean.names_out
  - median
  - median.dim
  - median.dim_values
  - median.names_dim
  - median.names_dim_values
  - nanmedian
  - nanmedian.dim
  - nanmedian.dim_values
  - nanmedian.names_dim
  - nanmedian.names_dim_values
  - min.dim
  - min.dim_min
  - min.names_dim
  - min.names_dim_min
  - amin
  - amin.out
  - mkldnn_convolution
  - miopen_batch_norm
  - miopen_batch_norm_backward
  - miopen_convolution
  - miopen_convolution_transpose
  - miopen_depthwise_convolution
  - miopen_rnn
  - miopen_rnn_backward
  - mm
  - mm.out
  - _sparse_mm
  - _sparse_sparse_matmul
  - mode
  - mode.values
  - mode.dimname
  - mode.dimname_out
  - mul.Tensor
  - mul_.Tensor
  - mul.out
  - mul.Scalar
  - mul_.Scalar
  - multiply.Tensor
  - multiply_.Tensor
  - multiply.out
  - multiply.Scalar
  - multiply_.Scalar
  - mv
  - mv.out
  - mvlgamma
  - mvlgamma_
  - narrow_copy
  - narrow_copy.out
  - narrow
  - narrow.Tensor
  - native_batch_norm
  - native_batch_norm.out
  - batch_norm_stats
  - batch_norm_elemt
  - batch_norm_elemt.out
  - batch_norm_gather_stats
  - batch_norm_gather_stats_with_counts
  - native_batch_norm_backward
  - batch_norm_backward_reduce
  - batch_norm_backward_elemt
  - batch_norm_update_stats
  - is_vulkan_available
  - _nnpack_available
  - _nnpack_spatial_convolution
  - ones.names
  - ones
  - ones.out
  - ones_like
  - pairwise_distance
  - cdist
  - _euclidean_dist
  - _cdist_forward
  - _cdist_backward
  - pdist
  - _pdist_forward
  - _pdist_backward
  - cosine_similarity
  - permute
  - movedim.intlist
  - movedim.int
  - moveaxis.intlist
  - moveaxis.int
  - numpy_T
  - pixel_shuffle
  - pixel_unshuffle
  - channel_shuffle
  - is_pinned
  - _pin_memory
  - pinverse
  - poisson_nll_loss
  - rad2deg
  - rad2deg_
  - rad2deg.out
  - deg2rad
  - deg2rad_
  - deg2rad.out
  - scalar_tensor
  - rand.names
  - rand.generator_with_names
  - rand
  - rand.generator
  - rand.out
  - rand.generator_out
  - rand_like
  - randint
  - randint.generator
  - randint.low
  - randint.low_generator
  - randint.out
  - randint.generator_out
  - randint.low_out
  - randint.low_generator_out
  - randint_like
  - randint_like.low_dtype
  - randn
  - randn.generator
  - randn.names
  - randn.generator_with_names
  - randn.out
  - randn.generator_out
  - randn_like
  - randperm
  - randperm.generator
  - randperm.out
  - randperm.generator_out
  - range.step
  - range
  - range.out
  - ravel
  - reciprocal
  - reciprocal_
  - reciprocal.out
  - neg
  - neg_
  - neg.out
  - negative
  - negative_
  - negative.out
  - repeat
  - repeat_interleave.self_int
  - reshape
  - _mkldnn_reshape
  - reshape_as
  - round
  - round_
  - round.out
  - rrelu
  - rrelu_
  - relu
  - relu_
  - prelu
  - prelu_backward
  - gelu
  - gelu_backward
  - infinitely_differentiable_gelu_backward
  - hardshrink
  - hardshrink_backward
  - rsqrt
  - rsqrt_
  - rsqrt.out
  - select.Dimname
  - select.int
  - select_backward
  - selu
  - selu_
  - celu
  - celu_
  - silu_backward
  - sigmoid
  - sigmoid_
  - sigmoid.out
  - logit
  - logit_
  - logit.out
  - sin
  - sin_
  - sin.out
  - sinc
  - sinc_
  - sinc.out
  - sinh
  - sinh_
  - sinh.out
  - detach
  - detach_
  - size.int
  - size.Dimname
  - slice.Tensor
  - slice_backward
  - slogdet
  - smm
  - softmax.int
  - softmax.Dimname
  - _softmax
  - _softmax_backward_data
  - _softmax_backward_data.out
  - unsafe_split.Tensor
  - split.Tensor
  - unsafe_split_with_sizes
  - split_with_sizes
  - squeeze
  - squeeze.dim
  # - squeeze.dimname
  - squeeze_
  - squeeze_.dim
  - squeeze_.dimname
  - sspaddmm
  - sspaddmm.out
  - stack
  - stack.out
  - _stack
  - _stack.out
  - hstack
  - hstack.out
  - vstack
  - vstack.out
  - dstack
  - dstack.out
  - stft
  - istft
  - stride.int
  - stride.Dimname
  - sum
  - sum.dim_IntList
  - sum.dim_DimnameList
  - sum.IntList_out
  - sum.DimnameList_out
  - nansum
  - nansum.dim_IntList
  - nansum.IntList_out
  - sum_to_size
  - sqrt
  - sqrt_
  - sqrt.out
  - square
  - square_
  - std
  - std.dim
  - std_mean
  - std_mean.dim
  - std_mean.names_dim
  - std.out
  - std.names_dim
  - std.names_out
  - prod
  - prod.dim_int
  - prod.int_out
  - prod.dim_Dimname
  - prod.Dimname_out
  - t
  - t_
  - tan
  - tan_
  - tan.out
  - tanh
  - tanh_
  - tanh.out
  - tensordot
  - tensordot.out
  - threshold
  - threshold_
  - threshold.out
  - threshold_backward
  - tile
  - transpose.int
  - transpose.Dimname
  - _mkldnn_transpose
  - transpose_
  - _mkldnn_transpose_
  - one_hot
  - flip
  - fliplr
  - flipud
  - roll
  - rot90
  - trapz.x
  - trapz.dx
  - _trilinear
  - triplet_margin_loss
  - trunc
  - trunc_
  - trunc.out
  - fix
  - fix_
  - fix.out
  - type_as
  - _has_compatible_shallow_copy_type
  - _unique
  - unique_dim
  - unique_consecutive
  - unique_dim_consecutive
  - _unique2
  - _unsafe_view
  - unsqueeze
  - unsqueeze_
  - vander
  - var
  - var.dim
  - var.out
  - var.names_dim
  - var.names_out
  - var_mean
  - var_mean.dim
  - var_mean.names_dim
  - view_as
  - where.self
  - where
  - _s_where
  - norm_except_dim
  - _weight_norm
  - _weight_norm_cuda_interface
  - _weight_norm_cuda_interface_backward
  - _weight_norm_differentiable_backward
  - zeros.names
  - zeros
  - zeros.out
  - zeros_like
  - _standard_gamma_grad
  - _standard_gamma
  - _dirichlet_grad
  - _sample_dirichlet
  - poisson
  - binomial
  - native_norm
  - native_norm.ScalarOpt_dim_dtype
  - _sparse_sum
  - _sparse_sum.dtype
  - _sparse_sum.dim
  - _sparse_sum.dim_dtype
  - _sparse_sum_backward
  - _sparse_softmax.int
  - _sparse_softmax.Dimname
  - _sparse_softmax
  - _sparse_softmax_backward_data
  - _sparse_log_softmax.int
  - _sparse_log_softmax.Dimname
  - _sparse_log_softmax
  - _sparse_log_softmax_backward_data
  - norm.ScalarOpt_dtype
  - norm.Scalar
  - norm.ScalarOpt_dim_dtype
  - norm.ScalarOpt_dim
  - norm.dtype_out
  - norm.out
  - frobenius_norm
  - frobenius_norm.dim
  - frobenius_norm.out
  - nuclear_norm
  - nuclear_norm.out
  - nuclear_norm.dim
  - nuclear_norm.dim_out
  - clone
  - resize_as_
  - zero_
  - sub.out
  - sub.Tensor
  - sub_.Tensor
  - sub.Scalar
  - sub_.Scalar
  - subtract.out
  - subtract.Tensor
  - subtract_.Tensor
  - subtract.Scalar
  - subtract_.Scalar
  - rsub.Tensor
  - heaviside.out
  - heaviside
  - heaviside_
  - rsub.Scalar
  - _sparse_addmm
  - addmm.out
  - addmm
  - addmm_
  - sparse_coo_tensor.size
  - sparse_coo_tensor.indices
  - sparse_coo_tensor.indices_size
  - _sparse_coo_tensor_unsafe
  - _validate_sparse_coo_tensor_args
  - _sparse_coo_tensor_with_dims
  - _sparse_coo_tensor_with_dims_and_tensors
  - sparse_resize_
  - sparse_resize_and_clear_
  - sparse_mask
  - to_dense
  - to_dense_backward
  - sparse_dim
  - _dimI
  - dense_dim
  - _dimV
  - _nnz
  - coalesce
  - is_coalesced
  - _indices
  - _values
  - _coalesced_
  - indices
  - values
  - hspmm.out
  - hspmm
  - copy_sparse_to_sparse_
  - unbind.int
  - unbind.Dimname
  - to_sparse.sparse_dim
  - to_sparse
  - to_mkldnn
  - mkldnn_reorder_conv2d_weight
  - mkldnn_reorder_conv3d_weight
  - to_mkldnn_backward
  - quantize_per_tensor
  - quantize_per_channel
  - dequantize.self
  - dequantize.tensors
  - q_scale
  - q_zero_point
  - q_per_channel_scales
  - q_per_channel_zero_points
  - q_per_channel_axis
  - int_repr
  - _make_per_tensor_quantized_tensor
  - _make_per_channel_quantized_tensor
  - qscheme
  - fake_quantize_per_tensor_affine
  - fake_quantize_per_tensor_affine_cachemask
  - fake_quantize_per_tensor_affine_cachemask_backward
  - _fake_quantize_learnable_per_tensor_affine
  - _fake_quantize_learnable_per_tensor_affine_backward
  - fake_quantize_per_channel_affine
  - fake_quantize_per_channel_affine_cachemask
  - fake_quantize_per_channel_affine_cachemask_backward
  - _fake_quantize_learnable_per_channel_affine
  - _fake_quantize_learnable_per_channel_affine_backward
  - _choose_qparams_per_tensor
  - _saturate_weight_to_fp16
  - choose_qparams_optimized
  - to.dtype_layout
  - to.device
  - to.dtype
  - to.other
  - meshgrid
  - cartesian_prod
  - combinations
  - item
  - result_type.Tensor
  - result_type.Scalar
  - result_type.Scalar_Tensor
  - result_type.Scalar_Scalar
  - can_cast
  - promote_types
  - _local_scalar_dense
  - _thnn_fused_lstm_cell
  - _thnn_fused_lstm_cell_backward
  - _thnn_differentiable_lstm_cell_backward
  - _thnn_fused_gru_cell
  - _thnn_fused_gru_cell_backward
  - _thnn_differentiable_gru_cell_backward
  - lstm.input
  - lstm.data
  - gru.input
  - rnn_tanh.input
  - rnn_tanh.data
  - rnn_relu.input
  - rnn_relu.data
  - lstm_cell
  - gru_cell
  - rnn_tanh_cell
  - rnn_relu_cell
  - quantized_gru_cell
  - quantized_rnn_relu_cell
  - quantized_rnn_tanh_cell
  - _pack_padded_sequence
  - _pack_padded_sequence_backward
  - _pad_packed_sequence
  - set_.source_Storage
  - set_.source_Storage_storage_offset
  - set_.source_Tensor
  - set_
  - is_set_to
  - masked_fill_.Scalar
  - masked_fill.Scalar
  - masked_fill_.Tensor
  - masked_fill.Tensor
  - masked_scatter_
  - masked_scatter
  - view
  - put_
  - index_add_
  - index_add
  - index_add.dimname
  - index_add.out
  - index_fill_.int_Scalar
  - index_fill.int_Scalar
  - index_fill_.int_Tensor
  - index_fill.int_Tensor
  - scatter_.src
  - scatter.src
  - scatter.src_out
  - scatter.value
  - scatter.value_out
  - scatter.dimname_src
  - scatter.dimname_value
  - scatter_add_
  - scatter_add
  - scatter_add.dimname
  - eq_.Scalar
  - eq_.Tensor
  - bitwise_and.Tensor_out
  - bitwise_and.Scalar_out
  - bitwise_and.Scalar
  - bitwise_and.Tensor
  - bitwise_and_.Scalar
  - bitwise_and_.Tensor
  - __and__.Scalar
  - __and__.Tensor
  - __iand__.Scalar
  - __iand__.Tensor
  - bitwise_or.Tensor_out
  - bitwise_or.Scalar_out
  - bitwise_or.Scalar
  - bitwise_or.Tensor
  - bitwise_or_.Scalar
  - bitwise_or_.Tensor
  - __or__.Scalar
  - __or__.Tensor
  - __ior__.Scalar
  - __ior__.Tensor
  - bitwise_xor.Tensor_out
  - bitwise_xor.Scalar_out
  - bitwise_xor.Scalar
  - bitwise_xor.Tensor
  - bitwise_xor_.Scalar
  - bitwise_xor_.Tensor
  - __xor__.Scalar
  - __xor__.Tensor
  - __ixor__.Scalar
  - __ixor__.Tensor
  - __lshift__.Scalar
  - __lshift__.Tensor
  - __ilshift__.Scalar
  - __ilshift__.Tensor
  - __rshift__.Scalar
  - __rshift__.Tensor
  - __irshift__.Scalar
  - __irshift__.Tensor
  - atan2_
  - tril_
  - triu_
  - digamma_
  - polygamma_
  - renorm_
  - lerp_.Scalar
  - lerp_.Tensor
  - fmod_.Scalar
  - fmod_.Tensor
  - remainder_.Scalar
  - remainder_.Tensor
  - addbmm_
  - addbmm.out
  - addbmm
  - addcdiv_
  - random_.from
  - random_.to
  - random_
  - uniform_
  - cauchy_
  - log_normal_
  - exponential_
  - geometric_
  - diag.out
  - diag
  - diag_backward
  - cross.out
  - cross
  - triu.out
  - triu
  - tril.out
  - tril
  - tril_indices
  - triu_indices
  - trace
  - trace_backward
  - ne.Scalar_out
  - ne.Scalar
  - ne.Tensor_out
  - ne.Tensor
  - ne_.Scalar
  - ne_.Tensor
  - not_equal.Scalar_out
  - not_equal.Scalar
  - not_equal.Tensor_out
  - not_equal.Tensor
  - not_equal_.Scalar
  - not_equal_.Tensor
  - eq.Scalar_out
  - eq.Scalar
  - eq.Tensor_out
  - eq.Tensor
  - ge.Scalar_out
  - ge.Scalar
  - ge.Tensor_out
  - ge.Tensor
  - ge_.Scalar
  - ge_.Tensor
  - greater_equal.Scalar_out
  - greater_equal.Scalar
  - greater_equal.Tensor_out
  - greater_equal.Tensor
  - greater_equal_.Scalar
  - greater_equal_.Tensor
  - le.Scalar_out
  - le.Scalar
  - le.Tensor_out
  - le.Tensor
  - le_.Scalar
  - le_.Tensor
  - less_equal.Scalar_out
  - less_equal.Scalar
  - less_equal.Tensor_out
  - less_equal.Tensor
  - less_equal_.Scalar
  - less_equal_.Tensor
  - gt.Scalar_out
  - gt.Scalar
  - gt.Tensor_out
  - gt.Tensor
  - gt_.Scalar
  - gt_.Tensor
  - greater.Scalar_out
  - greater.Scalar
  - greater.Tensor_out
  - greater.Tensor
  - greater_.Scalar
  - greater_.Tensor
  - lt.Scalar_out
  - lt.Scalar
  - lt.Tensor_out
  - lt.Tensor
  - lt_.Scalar
  - lt_.Tensor
  - less.Scalar_out
  - less.Scalar
  - less.Tensor_out
  - less.Tensor
  - less_.Scalar
  - less_.Tensor
  - take.out
  - take
  - index_select.out
  - index_select
  - index_select.dimname_out
  - index_select.dimname
  - index_select_backward
  - masked_select.out
  - masked_select
  - masked_select_backward
  - nonzero.out
  - nonzero
  - nonzero_numpy
  - gather.out
  - gather
  - gather_backward
  - gather.dimname_out
  - gather.dimname
  - _gather_sparse_backward
  - addcmul.out
  - addcmul
  - addcmul_
  - addcdiv.out
  - addcdiv
  - lstsq.X
  - lstsq
  - triangular_solve.X
  - triangular_solve
  - symeig.e
  - symeig
  - _symeig_helper
  - eig.e
  - eig
  - svd.U
  - svd
  - _linalg_svd.U
  - swapaxes
  - swapaxes_
  - swapdims
  - swapdims_
  - cholesky.out
  - cholesky
  - cholesky_solve.out
  - cholesky_solve
  - _cholesky_solve_helper
  - solve
  - solve.solution
  - _solve_helper
  - cholesky_inverse
  - cholesky_inverse.out
  - qr.Q
  - qr
  - geqrf.a
  - geqrf
  - orgqr.out
  - orgqr
  - ormqr.out
  - ormqr
  - _lu_with_info
  - lu_solve.out
  - lu_solve
  - multinomial.out
  - multinomial
  - lgamma.out
  - lgamma_
  - lgamma
  - digamma.out
  - digamma
  - polygamma.out
  - polygamma
  - erfinv
  - erfinv_
  - erfinv.out
  - i0
  - i0_
  - i0.out
  - sign
  - sign_
  - sign.out
  - sgn.out
  - signbit
  - signbit.out
  - dist
  - atan2.out
  - atan2
  - lerp.Scalar_out
  - lerp.Tensor_out
  - lerp.Scalar
  - lerp.Tensor
  - histc.out
  - histc
  - fmod.Scalar_out
  - fmod.Scalar
  - fmod.Tensor_out
  - fmod.Tensor
  - hypot.out
  - hypot
  - hypot_
  - igamma.out
  - igamma
  - igamma_
  - igammac.out
  - igammac
  - igammac_
  - nextafter.out
  - nextafter
  - nextafter_
  - remainder.Scalar_out
  - remainder.Scalar
  - remainder.Tensor_out
  - remainder.Tensor
  - min
  - fmin
  - fmin.out
  - max
  - fmax
  - fmax.out
  - maximum
  - maximum.out
  - max.out
  - minimum
  - minimum.out
  - min.out
  - quantile.scalar_out
  - quantile.scalar
  - quantile.out
  - quantile
  - nanquantile.scalar_out
  - nanquantile.scalar
  - nanquantile.out
  - nanquantile
  - sort.values
  - sort
  - sort.dimname_values
  - sort.dimname
  - msort.out
  - msort
  - argsort
  - argsort.dimname
  - topk.values
  - topk
  - all
  - any
  - renorm.out
  - renorm
  - unfold
  - unfold_backward
  - equal
  - pow.Tensor_Tensor_out
  - pow.Tensor_Tensor
  - pow.Scalar_out
  - pow.Scalar
  - pow.Tensor_Scalar_out
  - pow.Tensor_Scalar
  - pow_.Scalar
  - pow_.Tensor
  - float_power.Tensor_Tensor_out
  - float_power.Tensor_Tensor
  - float_power.Scalar_out
  - float_power.Scalar
  - float_power.Tensor_Scalar_out
  - float_power.Tensor_Scalar
  - float_power_.Scalar
  - float_power_.Tensor
  - normal_
  - normal.Tensor_float_out
  - normal.Tensor_float
  - normal.float_Tensor_out
  - normal.float_Tensor
  - normal.Tensor_Tensor_out
  - normal.Tensor_Tensor
  - normal.float_float
  - normal.float_float_out
  - alias
  - _index_copy_
  - _amp_foreach_non_finite_check_and_unscale_
  - _cat
  - _cat.out
  - _foreach_add.Scalar
  - _foreach_add_.Scalar
  - _foreach_sub.Scalar
  - _foreach_sub_.Scalar
  - _foreach_mul.Scalar
  - _foreach_mul_.Scalar
  - _foreach_div.Scalar
  - _foreach_div_.Scalar
  - _foreach_add.List
  - _foreach_add_.List
  - _foreach_sub.List
  - _foreach_sub_.List
  - _foreach_mul.List
  - _foreach_mul_.List
  - _foreach_div.List
  - _foreach_div_.List
  - _foreach_add.ScalarList
  - _foreach_add_.ScalarList
  - _foreach_sub.ScalarList
  - _foreach_sub_.ScalarList
  - _foreach_div.ScalarList
  - _foreach_div_.ScalarList
  - _foreach_mul.ScalarList
  - _foreach_mul_.ScalarList
  - _foreach_exp
  - _foreach_zero_
  - _foreach_exp_
  - _foreach_sqrt
  - _foreach_sqrt_
  - _foreach_abs
  - _foreach_abs_
  - _foreach_acos
  - _foreach_acos_
  - _foreach_asin
  - _foreach_asin_
  - _foreach_atan
  - _foreach_atan_
  - _foreach_ceil
  - _foreach_ceil_
  - _foreach_cos
  - _foreach_cos_
  - _foreach_cosh
  - _foreach_cosh_
  - _foreach_erf
  - _foreach_erf_
  - _foreach_erfc
  - _foreach_erfc_
  - _foreach_expm1
  - _foreach_expm1_
  - _foreach_floor
  - _foreach_floor_
  - _foreach_log
  - _foreach_log_
  - _foreach_log10
  - _foreach_log10_
  - _foreach_log1p
  - _foreach_log1p_
  - _foreach_log2
  - _foreach_log2_
  - _foreach_neg
  - _foreach_neg_
  - _foreach_tan
  - _foreach_tan_
  - _foreach_tanh
  - _foreach_tanh_
  - _foreach_sin
  - _foreach_sin_
  - _foreach_sinh
  - _foreach_sinh_
  - _foreach_round
  - _foreach_round_
  - _foreach_lgamma
  - _foreach_lgamma_
  - _foreach_frac
  - _foreach_frac_
  - _foreach_reciprocal
  - _foreach_reciprocal_
  - _foreach_sigmoid
  - _foreach_sigmoid_
  - _foreach_trunc
  - _foreach_trunc_
  - _foreach_addcdiv_.Scalar
  - _foreach_addcmul_.Scalar
  - _foreach_addcdiv_.ScalarList
  - _foreach_addcmul_.ScalarList
  - _foreach_addcdiv.Scalar
  - _foreach_addcmul.Scalar
  - _foreach_addcdiv.ScalarList
  - _foreach_addcmul.ScalarList
  - _foreach_maximum.List
  - _foreach_minimum.List
  - bucketize.Tensor
  - bucketize.Tensor_out
  - bucketize.Scalar
  - searchsorted.Tensor
  - searchsorted.Tensor_out
  - searchsorted.Scalar
  - mse_loss.out
  - mse_loss
  - mse_loss_backward.grad_input
  - mse_loss_backward
  - l1_loss.out
  - l1_loss
  - l1_loss_backward.grad_input
  - l1_loss_backward
  - multi_margin_loss.out
  - multi_margin_loss
  - multi_margin_loss_backward.grad_input
  - multi_margin_loss_backward
  - multilabel_margin_loss.out
  - multilabel_margin_loss
  - multilabel_margin_loss_forward.output
  - multilabel_margin_loss_forward
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_backward
  - nll_loss.out
  - nll_loss
  - nll_loss_forward.output
  - nll_loss_forward
  - nll_loss_backward.grad_input
  - nll_loss_backward
  - nll_loss2d.out
  - nll_loss2d
  - nll_loss2d_forward.output
  - nll_loss2d_forward
  - nll_loss2d_backward.grad_input
  - nll_loss2d_backward
  - smooth_l1_loss.out
  - smooth_l1_loss
  - smooth_l1_loss_backward.grad_input
  - smooth_l1_loss_backward
  - soft_margin_loss.out
  - soft_margin_loss
  - soft_margin_loss_backward.grad_input
  - soft_margin_loss_backward
  - glu.out
  - glu
  - glu_backward.grad_input
  - glu_backward
  - hardsigmoid.out
  - hardsigmoid
  - hardsigmoid_
  - hardsigmoid_backward
  - hardtanh.out
  - hardtanh
  - hardtanh_backward.grad_input
  - hardtanh_backward
  - hardtanh_
  - hardswish.out
  - hardswish
  - hardswish_
  - hardswish_backward
  - leaky_relu.out
  - leaky_relu
  - leaky_relu_backward
  - leaky_relu_
  - log_sigmoid.out
  - log_sigmoid
  - log_sigmoid_forward.output
  - log_sigmoid_forward
  - log_sigmoid_backward.grad_input
  - log_sigmoid_backward
  - rrelu_with_noise.out
  - rrelu_with_noise
  - rrelu_with_noise_backward
  - rrelu_with_noise_
  - softplus.out
  - softplus
  - softplus_backward.grad_input
  - softplus_backward
  - softshrink.out
  - softshrink
  - softshrink_backward.grad_input
  - softshrink_backward
  - adaptive_avg_pool2d.out
  - adaptive_avg_pool2d
  - mkldnn_adaptive_avg_pool2d
  - _adaptive_avg_pool2d
  - _adaptive_avg_pool2d_backward
  - _adaptive_avg_pool3d
  - adaptive_avg_pool3d.out
  - adaptive_avg_pool3d
  - _adaptive_avg_pool3d_backward
  - adaptive_avg_pool3d_backward.grad_input
  - adaptive_max_pool2d.out
  - adaptive_max_pool2d
  - adaptive_max_pool2d_backward.grad_input
  - adaptive_max_pool2d_backward
  - adaptive_max_pool3d.out
  - adaptive_max_pool3d
  - adaptive_max_pool3d_backward.grad_input
  - avg_pool2d.out
  - avg_pool2d
  - avg_pool2d_backward.grad_input
  - avg_pool2d_backward
  - avg_pool3d.out
  - avg_pool3d
  - avg_pool3d_backward.grad_input
  - avg_pool3d_backward
  - fractional_max_pool2d.output
  - fractional_max_pool2d
  - fractional_max_pool2d_backward.grad_input
  - fractional_max_pool2d_backward
  - fractional_max_pool3d.output
  - fractional_max_pool3d
  - fractional_max_pool3d_backward.grad_input
  - fractional_max_pool3d_backward
  - max_pool2d_with_indices.out
  - max_pool2d_with_indices
  - max_pool2d_with_indices_backward.grad_input
  - max_pool2d_with_indices_backward
  - max_pool3d_with_indices.out
  - max_pool3d_with_indices
  - max_pool3d_with_indices_backward.grad_input
  - max_pool3d_with_indices_backward
  - max_unpool2d.out
  - max_unpool2d
  - max_unpool2d_backward.grad_input
  - max_unpool2d_backward
  - max_unpool3d.out
  - max_unpool3d
  - max_unpool3d_backward.grad_input
  - max_unpool3d_backward
  - reflection_pad1d.out
  - reflection_pad1d
  - reflection_pad1d_backward.grad_input
  - reflection_pad1d_backward
  - reflection_pad2d.out
  - reflection_pad2d
  - reflection_pad2d_backward.grad_input
  - reflection_pad2d_backward
  - replication_pad1d.out
  - replication_pad1d
  - replication_pad1d_backward.grad_input
  - replication_pad1d_backward
  - replication_pad2d.out
  - replication_pad2d
  - replication_pad2d_backward.grad_input
  - replication_pad2d_backward
  - replication_pad3d.out
  - replication_pad3d
  - replication_pad3d_backward.grad_input
  - replication_pad3d_backward
  - upsample_trilinear3d.vec
  - upsample_trilinear3d_backward.vec
  - upsample_nearest1d.vec
  - upsample_nearest1d_backward.vec
  - upsample_nearest2d.vec
  - upsample_nearest2d_backward.vec
  - upsample_nearest3d.vec
  - upsample_nearest3d_backward.vec
  - upsample_linear1d.out
  - upsample_linear1d
  - upsample_linear1d.vec
  - upsample_linear1d_backward.grad_input
  - upsample_linear1d_backward
  - upsample_linear1d_backward.vec
  - upsample_bilinear2d.out
  - upsample_bilinear2d
  - upsample_bilinear2d.vec
  - upsample_bilinear2d_backward.grad_input
  - upsample_bilinear2d_backward
  - upsample_bilinear2d_backward.vec
  - upsample_bicubic2d.out
  - upsample_bicubic2d
  - upsample_bicubic2d.vec
  - upsample_bicubic2d_backward.grad_input
  - upsample_bicubic2d_backward
  - upsample_bicubic2d_backward.vec
  - upsample_trilinear3d.out
  - upsample_trilinear3d
  - upsample_trilinear3d_backward.grad_input
  - upsample_trilinear3d_backward
  - upsample_nearest1d.out
  - upsample_nearest1d
  - upsample_nearest1d_backward.grad_input
  - upsample_nearest1d_backward
  - upsample_nearest2d.out
  - upsample_nearest2d
  - upsample_nearest2d_backward.grad_input
  - upsample_nearest2d_backward
  - upsample_nearest3d.out
  - upsample_nearest3d
  - upsample_nearest3d_backward.grad_input
  - upsample_nearest3d_backward
  - sigmoid_backward.grad_input
  - sigmoid_backward
  - logit_backward.grad_input
  - logit_backward
  - tanh_backward.grad_input
  - tanh_backward
  - slow_conv_transpose2d.out
  - slow_conv_transpose2d
  - slow_conv_transpose3d.out
  - slow_conv_transpose3d
  - thnn_conv2d.out
  - thnn_conv2d
  - _slow_conv2d_forward
  - _slow_conv2d_forward.output
  - _slow_conv2d_backward.output_mask
  - slow_conv3d.out
  - slow_conv3d
  - slow_conv3d_forward.output
  - slow_conv3d_forward
  - slow_conv_dilated2d
  - slow_conv_dilated3d
  - col2im.out
  - col2im
  - col2im_backward.grad_input
  - col2im_backward
  - column_stack
  - column_stack.out
  - im2col.out
  - im2col
  - im2col_backward.grad_input
  - im2col_backward
  - isfinite
  - isinf
  - record_stream
  - isposinf
  - isposinf.out
  - isneginf
  - isneginf.out
  - _add_batch_dim
  - _remove_batch_dim
  - fft_fft
  - fft_fft.out
  - fft_ifft
  - fft_ifft.out
  - fft_rfft
  - fft_rfft.out
  - fft_irfft
  - fft_irfft.out
  - fft_hfft
  - fft_hfft.out
  - fft_ihfft
  - fft_ihfft.out
  - fft_fft2
  - fft_fft2.out
  - fft_ifft2
  - fft_ifft2.out
  - fft_rfft2
  - fft_rfft2.out
  - fft_irfft2
  - fft_irfft2.out
  - fft_fftn
  - fft_fftn.out
  - fft_ifftn
  - fft_ifftn.out
  - fft_rfftn
  - fft_rfftn.out
  - fft_irfftn
  - fft_irfftn.out
  - fft_fftfreq
  - fft_fftfreq.out
  - fft_rfftfreq
  - fft_rfftfreq.out
  - fft_fftshift
  - fft_ifftshift
  - linalg_cholesky
  - linalg_cholesky.out
  - linalg_det
  - det
  - linalg_slogdet
  - linalg_slogdet.out
  - linalg_eigh
  - linalg_eigh.eigvals
  - linalg_eigvalsh
  - linalg_eigvalsh.out
  - _linalg_inv_out_helper_
  - linalg_inv
  - linalg_inv.out
  - inner
  - inner.out
  - outer
  - outer.out
  - ger
  - ger.out
  - linalg_norm
  - linalg_norm.ord_str
  - linalg_norm.out
  - linalg_norm.ord_str_out
  - linalg_svd.U
  - linalg_svd
  - linalg_cond
  - linalg_cond.out
  - linalg_cond.p_str
  - linalg_cond.p_str_out
  - linalg_pinv
  - linalg_pinv.rcond_tensor
  - linalg_pinv.out
  - linalg_pinv.out_rcond_tensor
  - linalg_solve
  - linalg_solve.out
  - linalg_tensorinv
  - linalg_tensorinv.out
  - linalg_tensorsolve
  - linalg_tensorsolve.out
  - linalg_qr
  - linalg_qr.out
  - _linalg_qr_helper
  - linalg_matrix_rank
  - linalg_matrix_rank.out
  - _test_serialization_subcmul
  - _test_optional_intlist
  - _test_optional_filled_intlist
  - _test_optional_floatlist
  - _test_string_default
  - _test_ambiguous_defaults.a
  - _test_ambiguous_defaults.b

autograd:
  - elu.out
  - elu
  - elu_
  - silu
  - silu_
  - silu.out

custom:
  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
    variants: function, method
  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
    variants: function, method
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    variants: method
  - func: npu_alloc_float_status(Tensor self) -> Tensor
    variants: function, method
  - func: npu_get_float_status(Tensor self) -> Tensor
    variants: function, method
  - func: npu_clear_float_status(Tensor self) -> Tensor
    variants: function, method
  - func: one_(Tensor(a!) self) -> Tensor(a!)
    variants: method, function
  - func: fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
    variants: function, method
  - func: _amp_foreach_non_finite_check_(Tensor[] scaled_grads) -> bool
  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
    variants: function, method
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
    variants: function, method
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
    variants: function, method
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
    variants: function, method
  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
    variants: function, method
  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> Tensor[]
  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
    variants: function, method
  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3) -> Tensor
  - func: crop_and_resize(Tensor self, Tensor boxes, Tensor box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
  - func: reverse(Tensor self, int[] axis) -> Tensor
  - func: image_normalize(Tensor self, Tensor mean, Tensor variance, int dtype=0) -> Tensor
  - func: image_normalize_(Tensor(a!) self, Tensor mean, Tensor variance, int dtype=0) -> Tensor(a!)
  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    variants: function
custom_autograd:
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: fast_gelu(Tensor self) -> Tensor
  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
    variants: function, method
  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
    variants: function, method
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
    variants: function, method
  - func: npu_silu(Tensor self) -> Tensor
  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> Tensor[]
  - func: npu_mish(Tensor self) -> Tensor
    variants: function, method
  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seqMask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flagSeq, bool direction) -> Tensor[]
    variants: function
  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
    variants: function, method
  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
  - func: get_storage_size(Tensor self) -> int
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor[]
    variants: function