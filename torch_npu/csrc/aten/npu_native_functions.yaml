backend: NPU
cpp_namespace: at_npu::native
supported:
 - __and__.Scalar
 - __and__.Tensor
 - __ilshift__.Scalar
 - __ilshift__.Tensor
 - __ior__.Scalar
 - __ior__.Tensor
 - __irshift__.Scalar
 - __irshift__.Tensor
 - __lshift__.Scalar
 - __lshift__.Tensor
 - __or__.Scalar
 - __or__.Tensor
 - __rshift__.Scalar
 - __rshift__.Tensor
 - __xor__.Scalar
 - __xor__.Tensor
 - _adaptive_avg_pool2d
 - _adaptive_avg_pool2d_backward
 - _adaptive_avg_pool3d
 - _adaptive_avg_pool3d_backward
 - _add_relu.Tensor
 - _add_relu.out
 - _add_relu_.Tensor
 - _aminmax
 - _aminmax.dim
 - aminmax.out
 - _amp_foreach_non_finite_check_and_unscale_
 - _batch_norm_impl_index
 - _batch_norm_impl_index_backward
 - _cdist_backward
 - _cdist_forward
 - _conv_depthwise2d
 - _conv_depthwise2d.out
 - _conv_depthwise2d_backward
 - _convolution
 - _ctc_loss
 - _ctc_loss_backward
 - _cummax_helper
 - _cummin_helper
 - _dim_arange
 - _embedding_bag
 - _embedding_bag_backward
 - _embedding_bag_dense_backward
 - _embedding_bag_forward_only
 - _index_put_impl_
 - _linalg_svd.U
 - _local_scalar_dense
 - _log_softmax
 - _log_softmax_backward_data
 - _log_softmax_backward_data.out
 - _nnpack_spatial_convolution
 - _pack_padded_sequence
 - _pad_packed_sequence
 - _pdist_forward
 - _pin_memory
 - _slow_conv2d_backward.output_mask
 - _slow_conv2d_forward
 - _slow_conv2d_forward.output
 - _softmax
 - _softmax_backward_data
 - _softmax_backward_data.out
 - _unique2
 - _reshape_alias
 - abs
 - abs.out
 - abs_
 - acos
 - acos.out
 - acos_
 - acosh
 - acosh.out
 - acosh_
 - adaptive_avg_pool1d
 - adaptive_avg_pool2d
 - adaptive_avg_pool2d.out
 - adaptive_avg_pool3d
 - adaptive_avg_pool3d.out
 - adaptive_avg_pool3d_backward.grad_input
 - adaptive_max_pool2d
 - adaptive_max_pool2d.out
 - adaptive_max_pool2d_backward
 - adaptive_max_pool2d_backward.grad_input
 - add.Scalar
 - add.Tensor
 - add.out
 - add_.Scalar
 - add_.Tensor
 - addbmm
 - addbmm.out
 - addbmm_
 - addcdiv
 - addcdiv.out
 - addcdiv_
 - addcmul
 - addcmul.out
 - addcmul_
 - addmm
 - addmm.out
 - addmm_
 - addmv
 - addmv.out
 - addmv_
 - addr
 - addr.out
 - addr_
 - affine_grid_generator
 - affine_grid_generator_backward
 - all
 - all.dim
 - all.out
 - amax
 - amax.out
 - amin
 - amin.out
 - any
 - any.dim
 - any.out
 - arange
 - arange.out
 - arange.start
 - arange.start_out
 - arange.start_step
 - argmax
 - argmin
 - argsort
 - argsort.dimname
 - as_strided
 - as_strided_
 - asin
 - asin.out
 - asin_
 - asinh
 - asinh.out
 - asinh_
 - atan
 - atan.out
 - atan2
 - atan2.out
 - atan2_
 - atan_
 - atanh
 - atanh.out
 - atanh_
 - avg_pool2d
 - avg_pool2d.out
 - avg_pool2d_backward
 - avg_pool2d_backward.grad_input
 - avg_pool3d
 - avg_pool3d.out
 - avg_pool3d_backward
 - avg_pool3d_backward.grad_input
 - baddbmm
 - baddbmm.out
 - baddbmm_
 - bartlett_window
 - bartlett_window.periodic
 - batch_norm
 - batch_norm_backward_elemt
 - batch_norm_backward_reduce
 - batch_norm_elemt
 - batch_norm_elemt.out
 - batch_norm_gather_stats_update
 - batch_norm_gather_stats_with_counts
 - batch_norm_reduce
 - batch_norm_stats
 - bernoulli
 - bernoulli.out
 - bernoulli.p
 - bernoulli_.Tensor
 - bernoulli_.float
 - binary_cross_entropy
 - binary_cross_entropy.out
 - binary_cross_entropy_backward
 - binary_cross_entropy_backward.grad_input
 - bincount
 - bitwise_and.Scalar
 - bitwise_and.Scalar_out
 - bitwise_and.Tensor
 - bitwise_and.Tensor_out
 - bitwise_and_.Scalar
 - bitwise_and_.Tensor
 - bitwise_not
 - bitwise_not.out
 - bitwise_not_
 - bitwise_or.Scalar
 - bitwise_or.Scalar_out
 - bitwise_or.Tensor
 - bitwise_or.Tensor_out
 - bitwise_xor.Scalar
 - bitwise_xor.Scalar_out
 - bitwise_xor.Tensor
 - bitwise_xor.Tensor_out
 - bitwise_xor_.Scalar
 - bitwise_xor_.Tensor
 - blackman_window
 - blackman_window.periodic
 - bmm
 - bmm.out
 - cat
 - cat.names
 - cat.names_out
 - cat.out
 - cdist
 - ceil
 - ceil.out
 - ceil_
 - channel_shuffle
 - clamp
 - clamp.out
 - clamp_
 - clamp_max
 - clamp_max.out
 - clamp_max_
 - clamp_min
 - clamp_min.out
 - clamp_min_
 - clamp.Tensor
 - clamp.Tensor_out
 - clamp_.Tensor
 - clamp_max.Tensor
 - clamp_max.Tensor_out
 - clamp_max_.Tensor
 - clamp_min.Tensor
 - clamp_min.Tensor_out
 - clamp_min_.Tensor
 - clone
 - col2im
 - col2im.out
 - constant_pad_nd
 - contiguous
 - conv_tbc
 - conv_tbc_backward
 - conv_transpose2d.input
 - conv_transpose3d.input
 - convolution
 - convolution_backward
 - convolution_backward_overrideable
 - convolution_overrideable
 - count_nonzero
 - count_nonzero.dim_IntList
 - copy_
 - copy_memory_
 - cos
 - cos.out
 - cos_
 - cosh
 - cosh.out
 - cosh_
 - crop_and_resize
 - ctc_loss.IntList
 - ctc_loss.Tensor
 - cumprod.dimname_out
 - cumprod.out
 - cumprod_
 - cumprod_.dimname
 - cumsum.dimname_out
 - cumsum.out
 - decode_jpeg
 - diag
 - diag.out
 - div.Scalar
 - div.Scalar_mode
 - div.Tensor
 - div.Tensor_mode
 - div.out
 - div.out_mode
 - div_.Scalar
 - div_.Scalar_mode
 - div_.Tensor
 - div_.Tensor_mode
 - dot
 - dot.out
 - dropout
 - dropout_with_byte_mask
 - embedding
 - embedding_backward
 - embedding_dense_backward
 - embedding_renorm_
 - empty.memory_format
 - empty_like
 - empty_strided
 - empty_with_format
 - empty_with_format.names
 - eq.Scalar
 - eq.Scalar_out
 - eq.Tensor
 - eq.Tensor_out
 - eq_.Scalar
 - eq_.Tensor
 - equal
 - erf
 - erf.out
 - erf_
 - erfc
 - erfc.out
 - erfc_
 - erfinv
 - erfinv.out
 - erfinv_
 - exp
 - exp.out
 - exp2
 - exp2.out
 - exp2_
 - exp_
 - expm1
 - expm1.out
 - expm1_
 - eye
 - eye.m
 - eye.m_out
 - eye.out
 - fill_.Scalar
 - fill_.Tensor
 - fill_diagonal_
 - flip
 - floor
 - floor.out
 - floor_
 - floor_divide
 - floor_divide.Scalar
 - floor_divide.out
 - floor_divide_.Scalar
 - floor_divide_.Tensor
 - fmod.Scalar
 - fmod.Scalar_out
 - fmod.Tensor
 - fmod.Tensor_out
 - fmod_.Scalar
 - fmod_.Tensor
 - frac
 - frac.out
 - frac_
 - full
 - full.names
 - full.out
 - gather
 - gather.dimname
 - gather.dimname_out
 - gather.out
 - ge.Scalar
 - ge.Scalar_out
 - ge.Tensor
 - ge.Tensor_out
 - ge_.Scalar
 - ge_.Tensor
 - gelu
 - gelu_backward
 - ger
 - ger.out
 - glu
 - glu.out
 - glu_backward
 - glu_backward.grad_input
 - grid_sampler_2d
 - grid_sampler_2d_backward
 - grid_sampler_3d
 - grid_sampler_3d_backward
 - gru.input
 - gt.Scalar
 - gt.Scalar_out
 - gt.Tensor
 - gt.Tensor_out
 - gt_.Scalar
 - gt_.Tensor
 - hamming_window
 - hamming_window.periodic
 - hamming_window.periodic_alpha
 - hamming_window.periodic_alpha_beta
 - hann_window
 - hann_window.periodic
 - hardshrink
 - hardshrink_backward
 - hardsigmoid
 - hardsigmoid.out
 - hardsigmoid_
 - hardsigmoid_backward
 - hardswish
 - hardswish.out
 - hardswish_
 - hardswish_backward
 - hardtanh
 - hardtanh.out
 - hardtanh_
 - hardtanh_backward
 - hardtanh_backward.grad_input
 - im2col
 - im2col.out
 - image_normalize
 - image_normalize_
 - img_to_tensor
 - index.Tensor
 - index_add
 - index_add.dimname
 - index_add.out
 - index_fill.int_Scalar
 - index_fill.int_Tensor
 - index_fill_.int_Scalar
 - index_fill_.int_Tensor
 - index_put
 - index_put_
 - index_select
 - index_select.dimname
 - index_select.dimname_out
 - index_select.out
 - inverse
 - inverse.out
 - is_pinned
 - isclose
 - isfinite
 - isnan
 - kthvalue
 - kthvalue.dimname
 - kthvalue.dimname_out
 - kthvalue.values
 - le.Scalar
 - le.Scalar_out
 - le.Tensor
 - le.Tensor_out
 - le_.Scalar
 - le_.Tensor
 - leaky_relu
 - leaky_relu.out
 - leaky_relu_
 - leaky_relu_backward
 - lerp.Scalar
 - lerp.Scalar_out
 - lerp.Tensor
 - lerp.Tensor_out
 - lerp_.Scalar
 - lerp_.Tensor
 - linalg_cross
 - linalg_cross.out
 - linspace
 - linspace.out
 - log
 - log.out
 - log10
 - log10.out
 - log10_
 - log1p
 - log1p.out
 - log1p_
 - log2
 - log2.out
 - log2_
 - log_
 - log_sigmoid
 - log_sigmoid.out
 - log_sigmoid_backward
 - log_sigmoid_backward.grad_input
 - log_sigmoid_forward
 - log_sigmoid_forward.output
 - log_softmax.Dimname
 - log_softmax.int
 - logaddexp
 - logaddexp.out
 - logaddexp2
 - logaddexp2.out
 - logical_and
 - logical_and.out
 - logical_and_
 - logical_not
 - logical_not.out
 - logical_not_
 - logical_or
 - logical_or.out
 - logical_or_
 - logspace
 - logspace.out
 - logsumexp
 - logsumexp.names
 - logsumexp.names_out
 - logsumexp.out
 - lstm.data
 - lstm.input
 - lstm_cell
 - lt.Scalar
 - lt.Scalar_out
 - lt.Tensor
 - lt.Tensor_out
 - lt_.Scalar
 - lt_.Tensor
 - masked_fill_.Scalar
 - masked_fill_.Tensor
 - masked_scatter_
 - masked_select
 - masked_select.out
 - matmul
 - matmul_backward
 - matmul.out
 - max
 - max.dim
 - max.dim_max
 - max.names_dim
 - max.names_dim_max
 - max.out
 - max_pool2d_with_indices
 - max_pool2d_with_indices.out
 - max_pool2d_with_indices_backward
 - max_pool2d_with_indices_backward.grad_input
 - max_pool3d_with_indices
 - max_pool3d_with_indices.out
 - max_pool3d_with_indices_backward
 - max_pool3d_with_indices_backward.grad_input
 - max_unpool2d
 - max_unpool2d.out
 - max_unpool3d
 - max_unpool3d.out
 - maximum
 - maximum.out
 - mean
 - mean.dim
 - mean.names_dim
 - mean.names_out
 - mean.out
 - median
 - median.dim
 - median.dim_values
 - median.names_dim
 - median.names_dim_values
 - min
 - min.dim
 - min.dim_min
 - min.names_dim
 - min.names_dim_min
 - min.out
 - minimum
 - minimum.out
 - mish
 - mish.out
 - mish_
 - mish_backward
 - mm
 - mm.out
 - mse_loss
 - mse_loss.out
 - mse_loss_backward
 - mse_loss_backward.grad_input
 - mul.Scalar
 - mul.Tensor
 - mul.out
 - mul_.Scalar
 - mul_.Tensor
 - multilabel_margin_loss
 - multilabel_margin_loss.out
 - multilabel_margin_loss_forward
 - multilabel_margin_loss_forward.output
 - multinomial
 - multinomial.out
 - mv
 - mv.out
 - native_batch_norm
 - native_batch_norm_backward
 - native_layer_norm
 - native_layer_norm_backward
 - ne.Scalar
 - ne.Scalar_out
 - ne.Tensor
 - ne.Tensor_out
 - ne_.Scalar
 - ne_.Tensor
 - neg
 - neg.out
 - neg_
 - new_empty_strided
 - nll_loss
 - nll_loss.out
 - nll_loss2d
 - nll_loss2d.out
 - nll_loss2d_backward
 - nll_loss2d_backward.grad_input
 - nll_loss2d_forward
 - nll_loss2d_forward.output
 - nll_loss_backward
 - nll_loss_backward.grad_input
 - nll_loss_forward
 - nll_loss_forward.output
 - nonzero
 - nonzero.out
 - norm.Scalar
 - norm.ScalarOpt_dim
 - norm.ScalarOpt_dim_dtype
 - norm.ScalarOpt_dtype
 - norm.dtype_out
 - norm.out
 - normal.Tensor_Tensor
 - normal.Tensor_Tensor_out
 - normal.Tensor_float
 - normal.Tensor_float_out
 - normal.float_Tensor
 - normal.float_Tensor_out
 - normal.float_float
 - normal.float_float_out
 - normal_
 - one_
 - one_hot
 - ones
 - ones.names
 - ones.out
 - ones_like
 - pdist
 - pow.Scalar
 - pow.Scalar_out
 - pow.Tensor_Scalar
 - pow.Tensor_Scalar_out
 - pow.Tensor_Tensor
 - pow.Tensor_Tensor_out
 - pow_.Scalar
 - pow_.Tensor
 - _prelu_kernel
 - _prelu_kernel_backward
 - prod
 - prod.Dimname_out
 - prod.dim_Dimname
 - prod.dim_int
 - prod.int_out
 - put_
 - qr
 - qr.Q
 - quantize_per_channel
 - quantize_per_tensor
 - random_
 - random_.from
 - random_.to
 - randperm
 - randperm.generator
 - randperm.generator_out
 - randperm.out
 - range
 - range.out
 - range.step
 - reciprocal
 - reciprocal.out
 - reciprocal_
 - reflection_pad1d
 - reflection_pad1d.out
 - reflection_pad2d
 - reflection_pad2d.out
 - reflection_pad2d_backward
 - reflection_pad2d_backward.grad_input
 - relu
 - relu_
 - remainder.Scalar
 - remainder.Scalar_out
 - remainder.Tensor
 - remainder.Tensor_out
 - remainder_.Scalar
 - remainder_.Tensor
 - renorm
 - renorm.out
 - renorm_
 - repeat
 - repeat_interleave.self_Tensor
 - repeat_interleave.self_int
 - replication_pad1d
 - replication_pad1d.out
 - replication_pad1d_backward
 - replication_pad1d_backward.grad_input
 - replication_pad2d
 - replication_pad2d.out
 - replication_pad2d_backward
 - replication_pad2d_backward.grad_input
 - resize_
 - resize_as_
 - reverse
 - roll
 - round
 - round.out
 - round_
 - rrelu_with_noise
 - rrelu_with_noise.out
 - rrelu_with_noise_
 - rrelu_with_noise_backward
 - rsqrt
 - rsqrt.out
 - rsqrt_
 - rsub.Scalar
 - rsub.Tensor
 - scalar_tensor
 - scatter.src_out
 - scatter.value_out
 - scatter_add
 - scatter_add.dimname
 - scatter_add_
 - searchsorted.Scalar
 - searchsorted.Tensor
 - searchsorted.Tensor_out
 - set_
 - set_.source_Storage
 - set_.source_Storage_storage_offset
 - set_.source_Tensor
 - sgn.out
 - sigmoid
 - sigmoid.out
 - sigmoid_
 - sigmoid_backward
 - sigmoid_backward.grad_input
 - sign
 - sign.out
 - sign_
 - sin
 - sin.out
 - sin_
 - sinh
 - sinh.out
 - sinh_
 - slogdet
 - slow_conv3d
 - slow_conv3d.out
 - slow_conv3d_forward
 - slow_conv3d_forward.output
 - slow_conv_dilated2d
 - slow_conv_dilated2d_backward
 - slow_conv_transpose2d
 - slow_conv_transpose2d.out
 - slow_conv_transpose2d_backward
 - smooth_l1_loss
 - smooth_l1_loss.out
 - smooth_l1_loss_backward
 - smooth_l1_loss_backward.grad_input
 - soft_margin_loss
 - soft_margin_loss.out
 - soft_margin_loss_backward
 - soft_margin_loss_backward.grad_input
 - softmax.Dimname
 - softmax.int
 - softplus
 - softplus.out
 - softplus_backward.grad_input
 - softshrink
 - softshrink.out
 - softshrink_backward
 - softshrink_backward.grad_input
 - sort
 - sort.dimname
 - sort.dimname_values
 - sort.values
 - sqrt
 - sqrt.out
 - sqrt_
 - squeeze
 - squeeze.dim
 - stack
 - stack.out
 - std
 - std.dim
 - std.names_dim
 - std.correction
 - std.names_out
 - std.out
 - std_mean
 - std_mean.dim
 - std_mean.names_dim
 - sub.Scalar
 - sub.Tensor
 - sub.out
 - sub_.Scalar
 - sub_.Tensor
 - sum
 - sum.DimnameList_out
 - sum.IntList_out
 - sum.dim_DimnameList
 - sum.dim_IntList
 - take
 - take.out
 - tan
 - tan.out
 - tan_
 - tanh
 - tanh.out
 - tanh_
 - tanh_backward
 - tanh_backward.grad_input
 - threshold
 - threshold.out
 - threshold_
 - threshold_backward
 - to.device
 - to.dtype
 - to.dtype_layout
 - to.other
 - topk
 - topk.values
 - triangular_solve.X
 - tril
 - tril.out
 - tril_
 - tril_indices
 - triu
 - triu.out
 - triu_
 - triu_indices
 - true_divide.Scalar
 - true_divide.Tensor
 - true_divide.out
 - true_divide_.Scalar
 - true_divide_.Tensor
 - trunc
 - trunc.out
 - trunc_
 - unfold
 - uniform_
 - unique_consecutive
 - unsqueeze
 - upsample_bicubic2d
 - upsample_bicubic2d.out
 - upsample_bicubic2d_backward
 - upsample_bicubic2d_backward.grad_input
 - upsample_bilinear2d
 - upsample_bilinear2d.out
 - upsample_bilinear2d_backward
 - upsample_bilinear2d_backward.grad_input
 - upsample_linear1d
 - upsample_linear1d.out
 - upsample_linear1d_backward
 - upsample_nearest1d
 - upsample_nearest1d.out
 - upsample_nearest1d_backward
 - upsample_nearest1d_backward.grad_input
 - upsample_nearest2d
 - upsample_nearest2d.out
 - upsample_nearest2d_backward
 - upsample_nearest2d_backward.grad_input
 - upsample_nearest3d
 - upsample_nearest3d.out
 - upsample_nearest3d_backward
 - upsample_nearest3d_backward.grad_input
 - upsample_trilinear3d
 - upsample_trilinear3d.out
 - upsample_trilinear3d_backward
 - upsample_trilinear3d_backward.grad_input
 - var
 - var.dim
 - var.names_dim
 - var.names_out
 - var.out
 - var_mean
 - var_mean.dim
 - var_mean.names_dim
 - view
 - where
 - where.self
 - where.self_out
 - xlogy.OutScalar_Other
 - xlogy.OutScalar_Self
 - xlogy.OutTensor
 - xlogy.Scalar_Other
 - xlogy.Scalar_Self
 - xlogy.Tensor
 - xlogy_.Scalar_Other
 - xlogy_.Tensor
 - zero_
 - zeros
 - zeros.names
 - zeros.out
 - zeros_like

autograd:
  - celu
  - celu_
  - elu.out
  - elu
  - elu_
  - silu
  - silu_
  - silu.out
  - binary_cross_entropy_with_logits
  - selu
  - selu_
  - flatten_dense_tensors
  - unflatten_dense_tensors

tocpu:
  - angle
  - mode
  - nanmedian.dim_values
  - nansum
  - native_dropout
  - native_dropout_backward
  - poisson
  - vdot
  - view_as_complex
  - view_as_real

symint:
 - as_strided_
 - embedding
 - embedding_backward
 - _embedding_bag_backward
 - repeat_interleave.self_int
 - zeros
 - new_empty_strided

custom:
  - func: npu_change_data_ptr(Tensor dst, Tensor src, int index) -> int
  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: npu_alloc_float_status(Tensor self) -> Tensor
  - func: npu_get_float_status(Tensor self) -> Tensor
  - func: npu_clear_float_status(Tensor self) -> Tensor
  - func: one_(Tensor(a!) self) -> Tensor(a!)
  - func: npu_fast_gelu(Tensor self) -> Tensor
  - func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
  - func: _amp_foreach_non_finite_check(Tensor[] scaled_grads) -> bool
  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    dispatch:
     CompositeExplicitAutograd: empty_with_format
  - func: unsafe_empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor
    dispatch:
     CompositeExplicitAutograd: empty_with_format
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    dispatch:
     CompositeExplicitAutograd: empty_with_format
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> Tensor[]
  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)
  - func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
    tags: nondeterministic_seeded
  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
    tags: nondeterministic_seeded
  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    tags: nondeterministic_seeded
    dispatch:
     CompositeExplicitAutograd: npu_dropout_gen_mask
  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3) -> Tensor
  - func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
  - func: reverse(Tensor self, int[] axis) -> Tensor
  - func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor
  - func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)
  - func: img_to_tensor(Tensor self) -> Tensor
  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
  - func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)
  - func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
  - func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
  - func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]
  - func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]
  - func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)
  - func: format_contiguous(Tensor self) -> Tensor
  - func: check_match(Tensor self) -> bool
  - func: check_memory_overlaps(Tensor[] inputs, Tensor[] outputs) -> ()
  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
  - func: get_storage_size(Tensor self) -> int
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_enque_tensor(Tensor[] tensors, str format_string, int capacity=3) -> ()
  - func: npu_hcom_allgather(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allgather.out(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)

custom_autograd:
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  - func: fast_gelu(Tensor self) -> Tensor
  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
    tags: nondeterministic_seeded
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
  - func: npu_silu(Tensor self) -> Tensor
  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> Tensor[]
  - func: npu_mish(Tensor self) -> Tensor
  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seqMask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flagSeq, bool direction) -> Tensor[]
  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
    tags: nondeterministic_seeded
  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
    tags: nondeterministic_seeded
  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
    tags: nondeterministic_seeded
  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor[]
  - func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor
