backend: NPU
cpp_namespace: at_npu::native
supported:
 - func: __and__.Scalar
   wrap_impl: __and__
 - func: __and__.Tensor
   wrap_impl: __and__
 - func: __ilshift__.Scalar
   wrap_impl: __ilshift__
 - func: __ilshift__.Tensor
   wrap_impl: __ilshift__
 - func: __ior__.Scalar
   wrap_impl: __ior__
 - func: __ior__.Tensor
   wrap_impl: __ior__
 - func: __irshift__.Scalar
   wrap_impl: __irshift__
 - func: __irshift__.Tensor
   wrap_impl: __irshift__
 - func: __lshift__.Scalar
   wrap_impl: __lshift__
 - func: __lshift__.Tensor
   wrap_impl: __lshift__
 - func: __or__.Scalar
   wrap_impl: __or__
 - func: __or__.Tensor
   wrap_impl: __or__
 - func: __rshift__.Scalar
   wrap_impl: __rshift__
 - func: __rshift__.Tensor
   wrap_impl: __rshift__
 - func: __xor__.Scalar
   wrap_impl: __xor__
 - func: __xor__.Tensor
   wrap_impl: __xor__
 - func: _adaptive_avg_pool2d
   wrap_impl: _adaptive_avg_pool2d
 - func: _adaptive_avg_pool2d_backward
   wrap_impl: _adaptive_avg_pool2d_backward
 - func: _adaptive_avg_pool3d
   wrap_impl: _adaptive_avg_pool3d
 - func: _adaptive_avg_pool3d_backward
   wrap_impl: _adaptive_avg_pool3d_backward
 - func: _add_relu.Tensor
   wrap_impl: _add_relu
 - func: _add_relu.out
   wrap_impl: _add_relu_out
 - func: _add_relu_.Tensor
   wrap_impl: _add_relu_
 - func: _aminmax
   wrap_impl: _aminmax
 - func: _aminmax.dim
   wrap_impl: _aminmax
 - func: aminmax.out
   wrap_impl: aminmax_out
 - func: _amp_foreach_non_finite_check_and_unscale_
   wrap_impl: _amp_foreach_non_finite_check_and_unscale_
 - func: _batch_norm_impl_index
   wrap_impl: _batch_norm_impl_index
 - func: _batch_norm_impl_index_backward
   wrap_impl: _batch_norm_impl_index_backward
 - func: _cdist_backward
   wrap_impl: _cdist_backward
 - func: _cdist_forward
   wrap_impl: _cdist_forward
 - func: _conv_depthwise2d
   wrap_impl: _conv_depthwise2d
 - func: _conv_depthwise2d.out
   wrap_impl: _conv_depthwise2d_out
 - func: _conv_depthwise2d_backward
   wrap_impl: _conv_depthwise2d_backward
 - func: _convolution
   wrap_impl: _convolution
 - func: _ctc_loss
   wrap_impl: _ctc_loss
 - func: _ctc_loss_backward
   wrap_impl: _ctc_loss_backward
 - func: _cummax_helper
   wrap_impl: _cummax_helper
 - func: _cummin_helper
   wrap_impl: _cummin_helper
 - func: _dim_arange
   wrap_impl: _dim_arange
 - func: _embedding_bag
   wrap_impl: _embedding_bag
 - func: _embedding_bag_backward
   wrap_impl: _embedding_bag_backward
 - func: _embedding_bag_dense_backward
   wrap_impl: _embedding_bag_dense_backward
 - func: _embedding_bag_forward_only
   wrap_impl: _embedding_bag_forward_only
 - func: _index_put_impl_
   wrap_impl: _index_put_impl_
 - func: _linalg_svd.U
   wrap_impl: _linalg_svd_out
 - _local_scalar_dense
 - func: _log_softmax
   wrap_impl: _log_softmax
 - func: _log_softmax_backward_data
   wrap_impl: _log_softmax_backward_data
 - func: _log_softmax_backward_data.out
   wrap_impl: _log_softmax_backward_data_out
 - func: _nnpack_spatial_convolution
   wrap_impl: _nnpack_spatial_convolution
 - func: _pack_padded_sequence
   wrap_impl: _pack_padded_sequence
 - func: _pad_packed_sequence
   wrap_impl: _pad_packed_sequence
 - func: _pdist_forward
   wrap_impl: _pdist_forward
 - _pin_memory
 - func: _slow_conv2d_backward.output_mask
   wrap_impl: _slow_conv2d_backward
 - func: _slow_conv2d_forward
   wrap_impl: _slow_conv2d_forward
 - func: _slow_conv2d_forward.output
   wrap_impl: _slow_conv2d_forward_out
 - func: _softmax
   wrap_impl: _softmax
 - func: _softmax_backward_data
   wrap_impl: _softmax_backward_data
 - func: _softmax_backward_data.out
   wrap_impl: _softmax_backward_data_out
 - func: _unique2
   wrap_impl: _unique2
 - _reshape_alias
 - func: abs
   wrap_impl: abs
 - func: abs.out
   wrap_impl: abs_out
 - func: abs_
   wrap_impl: abs_
 - func: acos
   wrap_impl: acos
 - func: acos.out
   wrap_impl: acos_out
 - func: acos_
   wrap_impl: acos_
 - func: acosh
   wrap_impl: acosh
 - func: acosh.out
   wrap_impl: acosh_out
 - func: acosh_
   wrap_impl: acosh_
 - func: adaptive_avg_pool1d
   wrap_impl: adaptive_avg_pool1d
 - func: adaptive_avg_pool2d
   wrap_impl: adaptive_avg_pool2d
 - func: adaptive_avg_pool2d.out
   wrap_impl: adaptive_avg_pool2d_out
 - func: adaptive_avg_pool3d
   wrap_impl: adaptive_avg_pool3d
 - func: adaptive_avg_pool3d.out
   wrap_impl: adaptive_avg_pool3d_out
 - func: adaptive_avg_pool3d_backward.grad_input
   wrap_impl: adaptive_avg_pool3d_backward_out
 - func: adaptive_max_pool2d
   wrap_impl: adaptive_max_pool2d
 - func: adaptive_max_pool2d.out
   wrap_impl: adaptive_max_pool2d_out
 - func: adaptive_max_pool2d_backward
   wrap_impl: adaptive_max_pool2d_backward
 - func: adaptive_max_pool2d_backward.grad_input
   wrap_impl: adaptive_max_pool2d_backward_out
 - func: add.Scalar
   wrap_impl: add
 - func: add.Tensor
   wrap_impl: add
 - func: add.out
   wrap_impl: add_out
 - func: add_.Scalar
   wrap_impl: add_
 - func: add_.Tensor
   wrap_impl: add_
 - func: addbmm
   wrap_impl: addbmm
 - func: addbmm.out
   wrap_impl: addbmm_out
 - func: addbmm_
   wrap_impl: addbmm_
 - func: addcdiv
   wrap_impl: addcdiv
 - func: addcdiv.out
   wrap_impl: addcdiv_out
 - func: addcdiv_
   wrap_impl: addcdiv_
 - func: addcmul
   wrap_impl: addcmul
 - func: addcmul.out
   wrap_impl: addcmul_out
 - func: addcmul_
   wrap_impl: addcmul_
 - func: addmm
   wrap_impl: addmm
 - func: addmm.out
   wrap_impl: addmm_out
 - func: addmm_
   wrap_impl: addmm_
 - func: addmv
   wrap_impl: addmv
 - func: addmv.out
   wrap_impl: addmv_out
 - func: addmv_
   wrap_impl: addmv_
 - func: addr
   wrap_impl: addr
 - func: addr.out
   wrap_impl: addr_out
 - func: addr_
   wrap_impl: addr_
 - func: affine_grid_generator
   wrap_impl: affine_grid_generator
 - func: affine_grid_generator_backward
   wrap_impl: affine_grid_generator_backward
 - func: all
   wrap_impl: all
 - func: all.dim
   wrap_impl: all
 - func: all.out
   wrap_impl: all_out
 - func: amax
   wrap_impl: amax
 - func: amax.out
   wrap_impl: amax_out
 - func: amin
   wrap_impl: amin
 - func: amin.out
   wrap_impl: amin_out
 - func: any
   wrap_impl: any
 - func: any.dim
   wrap_impl: any
 - func: any.out
   wrap_impl: any_out
 - func: arange
   wrap_impl: arange
 - func: arange.out
   wrap_impl: arange_out
 - func: arange.start
   wrap_impl: arange
 - func: arange.start_out
   wrap_impl: arange_out
 - func: arange.start_step
   wrap_impl: arange
 - func: argmax
   wrap_impl: argmax
 - func: argmin
   wrap_impl: argmin
 - func: argsort
   wrap_impl: argsort
 - func: argsort.dimname
   wrap_impl: argsort
 - as_strided
 - as_strided_
 - func: asin
   wrap_impl: asin
 - func: asin.out
   wrap_impl: asin_out
 - func: asin_
   wrap_impl: asin_
 - func: asinh
   wrap_impl: asinh
 - func: asinh.out
   wrap_impl: asinh_out
 - func: asinh_
   wrap_impl: asinh_
 - func: atan
   wrap_impl: atan
 - func: atan.out
   wrap_impl: atan_out
 - func: atan2
   wrap_impl: atan2
 - func: atan2.out
   wrap_impl: atan2_out
 - func: atan2_
   wrap_impl: atan2_
 - func: atan_
   wrap_impl: atan_
 - func: atanh
   wrap_impl: atanh
 - func: atanh.out
   wrap_impl: atanh_out
 - func: atanh_
   wrap_impl: atanh_
 - func: avg_pool2d
   wrap_impl: avg_pool2d
 - func: avg_pool2d.out
   wrap_impl: avg_pool2d_out
 - func: avg_pool2d_backward
   wrap_impl: avg_pool2d_backward
 - func: avg_pool2d_backward.grad_input
   wrap_impl: avg_pool2d_backward_out
 - func: avg_pool3d
   wrap_impl: avg_pool3d
 - func: avg_pool3d.out
   wrap_impl: avg_pool3d_out
 - func: avg_pool3d_backward
   wrap_impl: avg_pool3d_backward
 - func: avg_pool3d_backward.grad_input
   wrap_impl: avg_pool3d_backward_out
 - func: baddbmm
   wrap_impl: baddbmm
 - func: baddbmm.out
   wrap_impl: baddbmm_out
 - func: baddbmm_
   wrap_impl: baddbmm_
 - bartlett_window
 - bartlett_window.periodic
 - func: batch_norm
   wrap_impl: batch_norm
 - func: batch_norm_backward_elemt
   wrap_impl: batch_norm_backward_elemt
 - func: batch_norm_backward_reduce
   wrap_impl: batch_norm_backward_reduce
 - func: batch_norm_elemt
   wrap_impl: batch_norm_elemt
 - func: batch_norm_elemt.out
   wrap_impl: batch_norm_elemt_out
 - func: batch_norm_gather_stats_update
   wrap_impl: batch_norm_gather_stats_update
 - func: batch_norm_gather_stats_with_counts
   wrap_impl: batch_norm_gather_stats_with_counts
 - func: batch_norm_reduce
   wrap_impl: batch_norm_reduce
 - func: batch_norm_stats
   wrap_impl: batch_norm_stats
 - func: bernoulli
   wrap_impl: bernoulli
 - func: bernoulli.out
   wrap_impl: bernoulli_out
 - func: bernoulli.p
   wrap_impl: bernoulli
 - func: bernoulli_.Tensor
   wrap_impl: bernoulli_
 - func: bernoulli_.float
   wrap_impl: bernoulli_
 - func: binary_cross_entropy
   wrap_impl: binary_cross_entropy
 - func: binary_cross_entropy.out
   wrap_impl: binary_cross_entropy_out
 - func: binary_cross_entropy_backward
   wrap_impl: binary_cross_entropy_backward
 - func: binary_cross_entropy_backward.grad_input
   wrap_impl: binary_cross_entropy_backward_out
 - func: bincount
   wrap_impl: bincount
 - func: bitwise_and.Scalar
   wrap_impl: bitwise_and
 - func: bitwise_and.Scalar_out
   wrap_impl: bitwise_and_out
 - func: bitwise_and.Tensor
   wrap_impl: bitwise_and
 - func: bitwise_and.Tensor_out
   wrap_impl: bitwise_and_out
 - func: bitwise_and_.Scalar
   wrap_impl: bitwise_and_
 - func: bitwise_and_.Tensor
   wrap_impl: bitwise_and_
 - func: bitwise_not
   wrap_impl: bitwise_not
 - func: bitwise_not.out
   wrap_impl: bitwise_not_out
 - func: bitwise_not_
   wrap_impl: bitwise_not_
 - func: bitwise_or.Scalar
   wrap_impl: bitwise_or
 - func: bitwise_or.Scalar_out
   wrap_impl: bitwise_or_out
 - func: bitwise_or.Tensor
   wrap_impl: bitwise_or
 - func: bitwise_or.Tensor_out
   wrap_impl: bitwise_or_out
 - func: bitwise_xor.Scalar
   wrap_impl: bitwise_xor
 - func: bitwise_xor.Scalar_out
   wrap_impl: bitwise_xor_out
 - func: bitwise_xor.Tensor
   wrap_impl: bitwise_xor
 - func: bitwise_xor.Tensor_out
   wrap_impl: bitwise_xor_out
 - func: bitwise_xor_.Scalar
   wrap_impl: bitwise_xor_
 - func: bitwise_xor_.Tensor
   wrap_impl: bitwise_xor_
 - blackman_window
 - blackman_window.periodic
 - func: bmm
   wrap_impl: bmm
 - func: bmm.out
   wrap_impl: bmm_out
 - func: cat
   wrap_impl: cat
 - func: cat.names
   wrap_impl: cat
 - func: cat.names_out
   wrap_impl: cat_out
 - func: cat.out
   wrap_impl: cat_out
 - func: cdist
   wrap_impl: cdist
 - func: ceil
   wrap_impl: ceil
 - func: ceil.out
   wrap_impl: ceil_out
 - func: ceil_
   wrap_impl: ceil_
 - func: channel_shuffle
   wrap_impl: channel_shuffle
 - func: clamp
   wrap_impl: clamp
 - func: clamp.out
   wrap_impl: clamp_out
 - func: clamp_
   wrap_impl: clamp_
 - func: clamp_max
   wrap_impl: clamp_max
 - func: clamp_max.out
   wrap_impl: clamp_max_out
 - func: clamp_max_
   wrap_impl: clamp_max_
 - func: clamp_min
   wrap_impl: clamp_min
 - func: clamp_min.out
   wrap_impl: clamp_min_out
 - func: clamp_min_
   wrap_impl: clamp_min_
 - func: clamp.Tensor
   wrap_impl: clamp
 - func: clamp.Tensor_out
   wrap_impl: clamp_out
 - func: clamp_.Tensor
   wrap_impl: clamp_
 - func: clamp_max.Tensor
   wrap_impl: clamp_max
 - func: clamp_max.Tensor_out
   wrap_impl: clamp_max_out
 - func: clamp_max_.Tensor
   wrap_impl: clamp_max_
 - func: clamp_min.Tensor
   wrap_impl: clamp_min
 - func: clamp_min.Tensor_out
   wrap_impl: clamp_min_out
 - func: clamp_min_.Tensor
   wrap_impl: clamp_min_
 - clone
 - func: col2im
   wrap_impl: col2im
 - func: col2im.out
   wrap_impl: col2im_out
 - func: constant_pad_nd
   wrap_impl: constant_pad_nd
 - contiguous
 - func: conv_tbc
   wrap_impl: conv_tbc
 - func: conv_tbc_backward
   wrap_impl: conv_tbc_backward
 - func: conv_transpose2d.input
   wrap_impl: conv_transpose2d
 - func: conv_transpose3d.input
   wrap_impl: conv_transpose3d
 - func: convolution
   wrap_impl: convolution
 - func: convolution_backward
   wrap_impl: convolution_backward
 - func: convolution_backward_overrideable
   wrap_impl: convolution_backward_overrideable
 - func: convolution_overrideable
   wrap_impl: convolution_overrideable
 - count_nonzero
 - count_nonzero.dim_IntList
 - copy_
 - copy_memory_
 - func: cos
   wrap_impl: cos
 - func: cos.out
   wrap_impl: cos_out
 - func: cos_
   wrap_impl: cos_
 - func: cosh
   wrap_impl: cosh
 - func: cosh.out
   wrap_impl: cosh_out
 - func: cosh_
   wrap_impl: cosh_
 - func: crop_and_resize
   wrap_impl: crop_and_resize
 - func: ctc_loss.IntList
   wrap_impl: ctc_loss
 - func: ctc_loss.Tensor
   wrap_impl: ctc_loss
 - cumprod.dimname_out
 - func: cumprod.out
   wrap_impl: cumprod_out
 - func: cumprod_
   wrap_impl: cumprod_
 - func: cumprod_.dimname
   wrap_impl: cumprod_
 - cumsum.dimname_out
 - func: cumsum.out
   wrap_impl: cumsum_out
 - func: decode_jpeg
   wrap_impl: decode_jpeg
 - func: diag
   wrap_impl: diag
 - func: diag.out
   wrap_impl: diag_out
 - func: div.Scalar
   wrap_impl: div
 - func: div.Scalar_mode
   wrap_impl: div
 - func: div.Tensor
   wrap_impl: div
 - func: div.Tensor_mode
   wrap_impl: div
 - func: div.out
   wrap_impl: div_out
 - func: div.out_mode
   wrap_impl: div_out
 - func: div_.Scalar
   wrap_impl: div_
 - func: div_.Scalar_mode
   wrap_impl: div_
 - func: div_.Tensor
   wrap_impl: div_
 - func: div_.Tensor_mode
   wrap_impl: div_
 - func: dot
   wrap_impl: dot
 - func: dot.out
   wrap_impl: dot_out
 - func: dropout
   wrap_impl: dropout
 - func: dropout_with_byte_mask
   wrap_impl: dropout_with_byte_mask
 - func: embedding
   wrap_impl: embedding_symint
 - func: embedding_backward
   wrap_impl: embedding_backward_symint
 - func: embedding_dense_backward
   wrap_impl: embedding_dense_backward
 - func: embedding_renorm_
   wrap_impl: embedding_renorm_
 - empty.memory_format
 - empty_like
 - empty_strided
 - empty_with_format
 - empty_with_format.names
 - func: eq.Scalar
   wrap_impl: eq
 - func: eq.Scalar_out
   wrap_impl: eq_out
 - func: eq.Tensor
   wrap_impl: eq
 - func: eq.Tensor_out
   wrap_impl: eq_out
 - func: eq_.Scalar
   wrap_impl: eq_
 - func: eq_.Tensor
   wrap_impl: eq_
 - func: equal
   wrap_impl: equal
 - func: erf
   wrap_impl: erf
 - func: erf.out
   wrap_impl: erf_out
 - func: erf_
   wrap_impl: erf_
 - func: erfc
   wrap_impl: erfc
 - func: erfc.out
   wrap_impl: erfc_out
 - func: erfc_
   wrap_impl: erfc_
 - func: erfinv
   wrap_impl: erfinv
 - func: erfinv.out
   wrap_impl: erfinv_out
 - func: erfinv_
   wrap_impl: erfinv_
 - func: exp
   wrap_impl: exp
 - func: exp.out
   wrap_impl: exp_out
 - func: exp2
   wrap_impl: exp2
 - func: exp2.out
   wrap_impl: exp2_out
 - func: exp2_
   wrap_impl: exp2_
 - func: exp_
   wrap_impl: exp_
 - func: expm1
   wrap_impl: expm1
 - func: expm1.out
   wrap_impl: expm1_out
 - func: expm1_
   wrap_impl: expm1_
 - func: eye
   wrap_impl: eye
 - func: eye.m
   wrap_impl: eye
 - func: eye.m_out
   wrap_impl: eye_out
 - func: eye.out
   wrap_impl: eye_out
 - func: fill_.Scalar
   wrap_impl: fill_
 - func: fill_.Tensor
   wrap_impl: fill_
 - func: fill_diagonal_
   wrap_impl: fill_diagonal_
 - func: flip
   wrap_impl: flip
 - func: floor
   wrap_impl: floor
 - func: floor.out
   wrap_impl: floor_out
 - func: floor_
   wrap_impl: floor_
 - func: floor_divide
   wrap_impl: floor_divide
 - func: floor_divide.Scalar
   wrap_impl: floor_divide
 - func: floor_divide.out
   wrap_impl: floor_divide_out
 - func: floor_divide_.Scalar
   wrap_impl: floor_divide_
 - func: floor_divide_.Tensor
   wrap_impl: floor_divide_
 - func: fmod.Scalar
   wrap_impl: fmod
 - func: fmod.Scalar_out
   wrap_impl: fmod_out
 - func: fmod.Tensor
   wrap_impl: fmod
 - func: fmod.Tensor_out
   wrap_impl: fmod_out
 - func: fmod_.Scalar
   wrap_impl: fmod_
 - func: fmod_.Tensor
   wrap_impl: fmod_
 - func: frac
   wrap_impl: frac
 - func: frac.out
   wrap_impl: frac_out
 - func: frac_
   wrap_impl: frac_
 - full
 - func: full.names
   wrap_impl: full
 - func: full.out
   wrap_impl: full_out
 - func: gather
   wrap_impl: gather
 - func: gather.dimname
   wrap_impl: gather
 - func: gather.dimname_out
   wrap_impl: gather_out
 - func: gather.out
   wrap_impl: gather_out
 - func: ge.Scalar
   wrap_impl: ge
 - func: ge.Scalar_out
   wrap_impl: ge_out
 - func: ge.Tensor
   wrap_impl: ge
 - func: ge.Tensor_out
   wrap_impl: ge_out
 - func: ge_.Scalar
   wrap_impl: ge_
 - func: ge_.Tensor
   wrap_impl: ge_
 - func: gelu
   wrap_impl: gelu
 - func: gelu_backward
   wrap_impl: gelu_backward
 - func: ger
   wrap_impl: ger
 - func: ger.out
   wrap_impl: ger_out
 - func: glu
   wrap_impl: glu
 - func: glu.out
   wrap_impl: glu_out
 - func: glu_backward
   wrap_impl: glu_backward
 - func: glu_backward.grad_input
   wrap_impl: glu_backward_out
 - func: grid_sampler_2d
   wrap_impl: grid_sampler_2d
 - func: grid_sampler_2d_backward
   wrap_impl: grid_sampler_2d_backward
 - func: grid_sampler_3d
   wrap_impl: grid_sampler_3d
 - func: grid_sampler_3d_backward
   wrap_impl: grid_sampler_3d_backward
 - func: gru.input
   wrap_impl: gru
 - func: gt.Scalar
   wrap_impl: gt
 - func: gt.Scalar_out
   wrap_impl: gt_out
 - func: gt.Tensor
   wrap_impl: gt
 - func: gt.Tensor_out
   wrap_impl: gt_out
 - func: gt_.Scalar
   wrap_impl: gt_
 - func: gt_.Tensor
   wrap_impl: gt_
 - hamming_window
 - hamming_window.periodic
 - hamming_window.periodic_alpha
 - hamming_window.periodic_alpha_beta
 - hann_window
 - hann_window.periodic
 - func: hardshrink
   wrap_impl: hardshrink
 - func: hardshrink_backward
   wrap_impl: hardshrink_backward
 - func: hardsigmoid
   wrap_impl: hardsigmoid
 - func: hardsigmoid.out
   wrap_impl: hardsigmoid_out
 - func: hardsigmoid_
   wrap_impl: hardsigmoid_
 - func: hardsigmoid_backward
   wrap_impl: hardsigmoid_backward
 - func: hardswish
   wrap_impl: hardswish
 - func: hardswish.out
   wrap_impl: hardswish_out
 - func: hardswish_
   wrap_impl: hardswish_
 - func: hardswish_backward
   wrap_impl: hardswish_backward
 - func: hardtanh
   wrap_impl: hardtanh
 - func: hardtanh.out
   wrap_impl: hardtanh_out
 - func: hardtanh_
   wrap_impl: hardtanh_
 - func: hardtanh_backward
   wrap_impl: hardtanh_backward
 - func: hardtanh_backward.grad_input
   wrap_impl: hardtanh_backward_out
 - func: im2col
   wrap_impl: im2col
 - func: im2col.out
   wrap_impl: im2col_out
 - func: image_normalize
   wrap_impl: image_normalize
 - func: image_normalize_
   wrap_impl: image_normalize_
 - func: img_to_tensor
   wrap_impl: img_to_tensor
 - func: index.Tensor
   wrap_impl: index
 - func: index_add
   wrap_impl: index_add
 - func: index_add.dimname
   wrap_impl: index_add
 - func: index_add.out
   wrap_impl: index_add_out
 - func: index_fill.int_Scalar
   wrap_impl: index_fill
 - func: index_fill.int_Tensor
   wrap_impl: index_fill
 - func: index_fill_.int_Scalar
   wrap_impl: index_fill_
 - func: index_fill_.int_Tensor
   wrap_impl: index_fill_
 - func: index_put
   wrap_impl: index_put
 - func: index_put_
   wrap_impl: index_put_
 - func: index_select
   wrap_impl: index_select
 - func: index_select.dimname
   wrap_impl: index_select
 - func: index_select.dimname_out
   wrap_impl: index_select_out
 - func: index_select.out
   wrap_impl: index_select_out
 - func: inverse
   wrap_impl: inverse
 - func: inverse.out
   wrap_impl: inverse_out
 - is_pinned
 - is_set_to
 - func: isclose
   wrap_impl: isclose
 - func: isfinite
   wrap_impl: isfinite
 - isnan
 - func: kthvalue
   wrap_impl: kthvalue
 - func: kthvalue.dimname
   wrap_impl: kthvalue
 - func: kthvalue.dimname_out
   wrap_impl: kthvalue_out
 - func: kthvalue.values
   wrap_impl: kthvalue_out
 - func: le.Scalar
   wrap_impl: le
 - func: le.Scalar_out
   wrap_impl: le_out
 - func: le.Tensor
   wrap_impl: le
 - func: le.Tensor_out
   wrap_impl: le_out
 - func: le_.Scalar
   wrap_impl: le_
 - func: le_.Tensor
   wrap_impl: le_
 - func: leaky_relu
   wrap_impl: leaky_relu
 - func: leaky_relu.out
   wrap_impl: leaky_relu_out
 - func: leaky_relu_
   wrap_impl: leaky_relu_
 - func: leaky_relu_backward
   wrap_impl: leaky_relu_backward
 - func: lerp.Scalar
   wrap_impl: lerp
 - func: lerp.Scalar_out
   wrap_impl: lerp_out
 - func: lerp.Tensor
   wrap_impl: lerp
 - func: lerp.Tensor_out
   wrap_impl: lerp_out
 - func: lerp_.Scalar
   wrap_impl: lerp_
 - func: lerp_.Tensor
   wrap_impl: lerp_
 - func: linalg_cross
   wrap_impl: linalg_cross
 - func: linalg_cross.out
   wrap_impl: linalg_cross_out
 - func: linspace
   wrap_impl: linspace
 - func: linspace.out
   wrap_impl: linspace_out
 - func: log
   wrap_impl: log
 - func: log.out
   wrap_impl: log_out
 - func: log10
   wrap_impl: log10
 - func: log10.out
   wrap_impl: log10_out
 - func: log10_
   wrap_impl: log10_
 - func: log1p
   wrap_impl: log1p
 - func: log1p.out
   wrap_impl: log1p_out
 - func: log1p_
   wrap_impl: log1p_
 - func: log2
   wrap_impl: log2
 - func: log2.out
   wrap_impl: log2_out
 - func: log2_
   wrap_impl: log2_
 - func: log_
   wrap_impl: log_
 - func: log_sigmoid
   wrap_impl: log_sigmoid
 - func: log_sigmoid.out
   wrap_impl: log_sigmoid_out
 - func: log_sigmoid_backward
   wrap_impl: log_sigmoid_backward
 - func: log_sigmoid_backward.grad_input
   wrap_impl: log_sigmoid_backward_out
 - func: log_sigmoid_forward
   wrap_impl: log_sigmoid_forward
 - func: log_sigmoid_forward.output
   wrap_impl: log_sigmoid_forward_out
 - func: log_softmax.Dimname
   wrap_impl: log_softmax
 - func: log_softmax.int
   wrap_impl: log_softmax
 - func: logaddexp
   wrap_impl: logaddexp
 - func: logaddexp.out
   wrap_impl: logaddexp_out
 - func: logaddexp2
   wrap_impl: logaddexp2
 - func: logaddexp2.out
   wrap_impl: logaddexp2_out
 - func: logical_and
   wrap_impl: logical_and
 - func: logical_and.out
   wrap_impl: logical_and_out
 - func: logical_and_
   wrap_impl: logical_and_
 - func: logical_not
   wrap_impl: logical_not
 - func: logical_not.out
   wrap_impl: logical_not_out
 - func: logical_not_
   wrap_impl: logical_not_
 - func: logical_or
   wrap_impl: logical_or
 - func: logical_or.out
   wrap_impl: logical_or_out
 - func: logical_or_
   wrap_impl: logical_or_
 - func: logspace
   wrap_impl: logspace
 - func: logspace.out
   wrap_impl: logspace_out
 - func: logsumexp
   wrap_impl: logsumexp
 - func: logsumexp.names
   wrap_impl: logsumexp
 - func: logsumexp.names_out
   wrap_impl: logsumexp_out
 - func: logsumexp.out
   wrap_impl: logsumexp_out
 - func: lstm.data
   wrap_impl: lstm
 - func: lstm.input
   wrap_impl: lstm
 - func: lstm_cell
   wrap_impl: lstm_cell
 - func: lt.Scalar
   wrap_impl: lt
 - func: lt.Scalar_out
   wrap_impl: lt_out
 - func: lt.Tensor
   wrap_impl: lt
 - func: lt.Tensor_out
   wrap_impl: lt_out
 - func: lt_.Scalar
   wrap_impl: lt_
 - func: lt_.Tensor
   wrap_impl: lt_
 - func: masked_fill_.Scalar
   wrap_impl: masked_fill_
 - func: masked_fill_.Tensor
   wrap_impl: masked_fill_
 - func: masked_scatter_
   wrap_impl: masked_scatter_
 - func: masked_select
   wrap_impl: masked_select
 - func: masked_select.out
   wrap_impl: masked_select_out
 - func: matmul
   wrap_impl: matmul
 - func: matmul_backward
   wrap_impl: matmul_backward
 - func: matmul.out
   wrap_impl: matmul_out
 - func: max
   wrap_impl: max
 - func: max.dim
   wrap_impl: max
 - func: max.dim_max
   wrap_impl: max_out
 - func: max.names_dim
   wrap_impl: max
 - func: max.names_dim_max
   wrap_impl: max_out
 - func: max.out
   wrap_impl: max_out
 - func: max_pool2d_with_indices
   wrap_impl: max_pool2d_with_indices
 - func: max_pool2d_with_indices.out
   wrap_impl: max_pool2d_with_indices_out
 - func: max_pool2d_with_indices_backward
   wrap_impl: max_pool2d_with_indices_backward
 - func: max_pool2d_with_indices_backward.grad_input
   wrap_impl: max_pool2d_with_indices_backward_out
 - func: max_pool3d_with_indices
   wrap_impl: max_pool3d_with_indices
 - func: max_pool3d_with_indices.out
   wrap_impl: max_pool3d_with_indices_out
 - func: max_pool3d_with_indices_backward
   wrap_impl: max_pool3d_with_indices_backward
 - func: max_pool3d_with_indices_backward.grad_input
   wrap_impl: max_pool3d_with_indices_backward_out
 - func: max_unpool2d
   wrap_impl: max_unpool2d
 - func: max_unpool2d.out
   wrap_impl: max_unpool2d_out
 - func: max_unpool3d
   wrap_impl: max_unpool3d
 - func: max_unpool3d.out
   wrap_impl: max_unpool3d_out
 - func: maximum
   wrap_impl: maximum
 - func: maximum.out
   wrap_impl: maximum_out
 - func: mean
   wrap_impl: mean
 - func: mean.dim
   wrap_impl: mean
 - func: mean.names_dim
   wrap_impl: mean
 - func: mean.names_out
   wrap_impl: mean_out
 - func: mean.out
   wrap_impl: mean_out
 - func: median
   wrap_impl: median
 - func: median.dim
   wrap_impl: median
 - func: median.dim_values
   wrap_impl: median_out
 - func: median.names_dim
   wrap_impl: median
 - func: median.names_dim_values
   wrap_impl: median_out
 - func: min
   wrap_impl: min
 - func: min.dim
   wrap_impl: min
 - func: min.dim_min
   wrap_impl: min_out
 - func: min.names_dim
   wrap_impl: min
 - func: min.names_dim_min
   wrap_impl: min_out
 - func: min.out
   wrap_impl: min_out
 - func: minimum
   wrap_impl: minimum
 - func: minimum.out
   wrap_impl: minimum_out
 - func: mish
   wrap_impl: mish
 - func: mish.out
   wrap_impl: mish_out
 - func: mish_
   wrap_impl: mish_
 - func: mish_backward
   wrap_impl: mish_backward
 - func: mm
   wrap_impl: mm
 - func: mm.out
   wrap_impl: mm_out
 - func: mse_loss
   wrap_impl: mse_loss
 - func: mse_loss.out
   wrap_impl: mse_loss_out
 - func: mse_loss_backward
   wrap_impl: mse_loss_backward
 - func: mse_loss_backward.grad_input
   wrap_impl: mse_loss_backward_out
 - func: mul.Scalar
   wrap_impl: mul
 - func: mul.Tensor
   wrap_impl: mul
 - func: mul.out
   wrap_impl: mul_out
 - func: mul_.Scalar
   wrap_impl: mul_
 - func: mul_.Tensor
   wrap_impl: mul_
 - func: multilabel_margin_loss
   wrap_impl: multilabel_margin_loss
 - func: multilabel_margin_loss.out
   wrap_impl: multilabel_margin_loss_out
 - func: multilabel_margin_loss_forward
   wrap_impl: multilabel_margin_loss_forward
 - func: multilabel_margin_loss_forward.output
   wrap_impl: multilabel_margin_loss_forward_out
 - func: multinomial
   wrap_impl: multinomial
 - func: multinomial.out
   wrap_impl: multinomial_out
 - func: mv
   wrap_impl: mv
 - func: mv.out
   wrap_impl: mv_out
 - func: native_batch_norm
   wrap_impl: native_batch_norm
 - func: native_batch_norm_backward
   wrap_impl: native_batch_norm_backward
 - native_dropout
 - native_dropout_backward
 - func: native_layer_norm
   wrap_impl: native_layer_norm
 - func: native_layer_norm_backward
   wrap_impl: native_layer_norm_backward
 - func: ne.Scalar
   wrap_impl: ne
 - func: ne.Scalar_out
   wrap_impl: ne_out
 - func: ne.Tensor
   wrap_impl: ne
 - func: ne.Tensor_out
   wrap_impl: ne_out
 - func: ne_.Scalar
   wrap_impl: ne_
 - func: ne_.Tensor
   wrap_impl: ne_
 - func: neg
   wrap_impl: neg
 - func: neg.out
   wrap_impl: neg_out
 - func: neg_
   wrap_impl: neg_
 - new_empty_strided
 - func: nll_loss
   wrap_impl: nll_loss
 - func: nll_loss.out
   wrap_impl: nll_loss_out
 - func: nll_loss2d
   wrap_impl: nll_loss2d
 - func: nll_loss2d.out
   wrap_impl: nll_loss2d_out
 - func: nll_loss2d_backward
   wrap_impl: nll_loss2d_backward
 - func: nll_loss2d_backward.grad_input
   wrap_impl: nll_loss2d_backward_out
 - func: nll_loss2d_forward
   wrap_impl: nll_loss2d_forward
 - func: nll_loss2d_forward.output
   wrap_impl: nll_loss2d_forward_out
 - func: nll_loss_backward
   wrap_impl: nll_loss_backward
 - func: nll_loss_backward.grad_input
   wrap_impl: nll_loss_backward_out
 - func: nll_loss_forward
   wrap_impl: nll_loss_forward
 - func: nll_loss_forward.output
   wrap_impl: nll_loss_forward_out
 - func: nonzero
   wrap_impl: nonzero
 - func: nonzero.out
   wrap_impl: nonzero_out
 - func: norm.Scalar
   wrap_impl: norm
 - func: norm.ScalarOpt_dim
   wrap_impl: norm
 - func: norm.ScalarOpt_dim_dtype
   wrap_impl: norm
 - func: norm.ScalarOpt_dtype
   wrap_impl: norm
 - func: norm.dtype_out
   wrap_impl: norm_out
 - func: norm.out
   wrap_impl: norm_out
 - func: normal.Tensor_Tensor
   wrap_impl: normal
 - func: normal.Tensor_Tensor_out
   wrap_impl: normal_out
 - func: normal.Tensor_float
   wrap_impl: normal
 - func: normal.Tensor_float_out
   wrap_impl: normal_out
 - func: normal.float_Tensor
   wrap_impl: normal
 - func: normal.float_Tensor_out
   wrap_impl: normal_out
 - func: normal.float_float
   wrap_impl: normal
 - func: normal.float_float_out
   wrap_impl: normal_out
 - func: normal_
   wrap_impl: normal_
 - func: one_
   wrap_impl: one_
 - func: one_hot
   wrap_impl: one_hot
 - func: ones
   wrap_impl: ones
 - func: ones.names
   wrap_impl: ones
 - func: ones.out
   wrap_impl: ones_out
 - func: ones_like
   wrap_impl: ones_like
 - func: pdist
   wrap_impl: pdist
 - func: pow.Scalar
   wrap_impl: pow
 - func: pow.Scalar_out
   wrap_impl: pow_out
 - func: pow.Tensor_Scalar
   wrap_impl: pow
 - func: pow.Tensor_Scalar_out
   wrap_impl: pow_out
 - func: pow.Tensor_Tensor
   wrap_impl: pow
 - func: pow.Tensor_Tensor_out
   wrap_impl: pow_out
 - func: pow_.Scalar
   wrap_impl: pow_
 - func: pow_.Tensor
   wrap_impl: pow_
 - func: _prelu_kernel
   wrap_impl: _prelu_kernel
 - func: _prelu_kernel_backward
   wrap_impl: _prelu_kernel_backward
 - func: prod
   wrap_impl: prod
 - func: prod.Dimname_out
   wrap_impl: prod_out
 - func: prod.dim_Dimname
   wrap_impl: prod
 - func: prod.dim_int
   wrap_impl: prod
 - func: prod.int_out
   wrap_impl: prod_out
 - func: put_
   wrap_impl: put_
 - func: qr
   wrap_impl: qr
 - func: qr.Q
   wrap_impl: qr_out
 - func: quantize_per_channel
   wrap_impl: quantize_per_channel
 - func: quantize_per_tensor
   wrap_impl: quantize_per_tensor
 - func: random_
   wrap_impl: random_
 - func: random_.from
   wrap_impl: random_
 - func: random_.to
   wrap_impl: random_
 - func: randperm
   wrap_impl: randperm
 - func: randperm.generator
   wrap_impl: randperm
 - func: randperm.generator_out
   wrap_impl: randperm_out
 - func: randperm.out
   wrap_impl: randperm_out
 - func: range
   wrap_impl: range
 - func: range.out
   wrap_impl: range_out
 - func: range.step
   wrap_impl: range
 - func: reciprocal
   wrap_impl: reciprocal
 - func: reciprocal.out
   wrap_impl: reciprocal_out
 - func: reciprocal_
   wrap_impl: reciprocal_
 - record_stream
 - func: reflection_pad1d
   wrap_impl: reflection_pad1d
 - func: reflection_pad1d.out
   wrap_impl: reflection_pad1d_out
 - reflection_pad1d_backward
 - reflection_pad1d_backward.grad_input
 - func: reflection_pad2d
   wrap_impl: reflection_pad2d
 - func: reflection_pad2d.out
   wrap_impl: reflection_pad2d_out
 - func: reflection_pad2d_backward
   wrap_impl: reflection_pad2d_backward
 - func: reflection_pad2d_backward.grad_input
   wrap_impl: reflection_pad2d_backward_out
 - func: relu
   wrap_impl: relu
 - func: relu_
   wrap_impl: relu_
 - func: remainder.Scalar
   wrap_impl: remainder
 - func: remainder.Scalar_out
   wrap_impl: remainder_out
 - func: remainder.Tensor
   wrap_impl: remainder
 - func: remainder.Tensor_out
   wrap_impl: remainder_out
 - func: remainder_.Scalar
   wrap_impl: remainder_
 - func: remainder_.Tensor
   wrap_impl: remainder_
 - func: renorm
   wrap_impl: renorm
 - func: renorm.out
   wrap_impl: renorm_out
 - func: renorm_
   wrap_impl: renorm_
 - func: repeat
   wrap_impl: repeat
 - func: repeat_interleave.self_Tensor
   wrap_impl: repeat_interleave
 - func: repeat_interleave.self_int
   wrap_impl: repeat_interleave
 - func: replication_pad1d
   wrap_impl: replication_pad1d
 - func: replication_pad1d.out
   wrap_impl: replication_pad1d_out
 - func: replication_pad1d_backward
   wrap_impl: replication_pad1d_backward
 - func: replication_pad1d_backward.grad_input
   wrap_impl: replication_pad1d_backward_out
 - func: replication_pad2d
   wrap_impl: replication_pad2d
 - func: replication_pad2d.out
   wrap_impl: replication_pad2d_out
 - func: replication_pad2d_backward
   wrap_impl: replication_pad2d_backward
 - func: replication_pad2d_backward.grad_input
   wrap_impl: replication_pad2d_backward_out
 - resize_
 - resize_as_
 - func: reverse
   wrap_impl: reverse
 - func: roll
   wrap_impl: roll
 - func: round
   wrap_impl: round
 - func: round.out
   wrap_impl: round_out
 - func: round_
   wrap_impl: round_
 - func: rrelu_with_noise
   wrap_impl: rrelu_with_noise
 - func: rrelu_with_noise.out
   wrap_impl: rrelu_with_noise_out
 - func: rrelu_with_noise_
   wrap_impl: rrelu_with_noise_
 - func: rrelu_with_noise_backward
   wrap_impl: rrelu_with_noise_backward
 - func: rsqrt
   wrap_impl: rsqrt
 - func: rsqrt.out
   wrap_impl: rsqrt_out
 - func: rsqrt_
   wrap_impl: rsqrt_
 - func: rsub.Scalar
   wrap_impl: rsub
 - func: rsub.Tensor
   wrap_impl: rsub
 - scalar_tensor
 - scatter.src_out
 - scatter.value_out
 - func: scatter_add
   wrap_impl: scatter_add
 - func: scatter_add.dimname
   wrap_impl: scatter_add
 - func: scatter_add_
   wrap_impl: scatter_add_
 - func: searchsorted.Scalar
   wrap_impl: searchsorted
 - func: searchsorted.Tensor
   wrap_impl: searchsorted
 - func: searchsorted.Tensor_out
   wrap_impl: searchsorted_out
 - set_
 - set_.source_Storage
 - set_.source_Storage_storage_offset
 - set_.source_Tensor
 - func: sgn.out
   wrap_impl: sgn_out
 - func: sigmoid
   wrap_impl: sigmoid
 - func: sigmoid.out
   wrap_impl: sigmoid_out
 - func: sigmoid_
   wrap_impl: sigmoid_
 - func: sigmoid_backward
   wrap_impl: sigmoid_backward
 - func: sigmoid_backward.grad_input
   wrap_impl: sigmoid_backward_out
 - func: sign
   wrap_impl: sign
 - func: sign.out
   wrap_impl: sign_out
 - func: sign_
   wrap_impl: sign_
 - func: sin
   wrap_impl: sin
 - func: sin.out
   wrap_impl: sin_out
 - func: sin_
   wrap_impl: sin_
 - func: sinh
   wrap_impl: sinh
 - func: sinh.out
   wrap_impl: sinh_out
 - func: sinh_
   wrap_impl: sinh_
 - func: slogdet
   wrap_impl: slogdet
 - func: slow_conv3d
   wrap_impl: slow_conv3d
 - func: slow_conv3d.out
   wrap_impl: slow_conv3d_out
 - func: slow_conv3d_forward
   wrap_impl: slow_conv3d_forward
 - func: slow_conv3d_forward.output
   wrap_impl: slow_conv3d_forward_out
 - func: slow_conv_dilated2d
   wrap_impl: slow_conv_dilated2d
 - func: slow_conv_dilated2d_backward
   wrap_impl: slow_conv_dilated2d_backward
 - func: slow_conv_transpose2d
   wrap_impl: slow_conv_transpose2d
 - func: slow_conv_transpose2d.out
   wrap_impl: slow_conv_transpose2d_out
 - func: slow_conv_transpose2d_backward
   wrap_impl: slow_conv_transpose2d_backward
 - func: smooth_l1_loss
   wrap_impl: smooth_l1_loss
 - func: smooth_l1_loss.out
   wrap_impl: smooth_l1_loss_out
 - func: smooth_l1_loss_backward
   wrap_impl: smooth_l1_loss_backward
 - func: smooth_l1_loss_backward.grad_input
   wrap_impl: smooth_l1_loss_backward_out
 - func: soft_margin_loss
   wrap_impl: soft_margin_loss
 - func: soft_margin_loss.out
   wrap_impl: soft_margin_loss_out
 - func: soft_margin_loss_backward
   wrap_impl: soft_margin_loss_backward
 - func: soft_margin_loss_backward.grad_input
   wrap_impl: soft_margin_loss_backward_out
 - func: softmax.Dimname
   wrap_impl: softmax
 - func: softmax.int
   wrap_impl: softmax
 - func: softplus
   wrap_impl: softplus
 - func: softplus.out
   wrap_impl: softplus_out
 - func: softplus_backward.grad_input
   wrap_impl: softplus_backward_out
 - func: softshrink
   wrap_impl: softshrink
 - func: softshrink.out
   wrap_impl: softshrink_out
 - func: softshrink_backward
   wrap_impl: softshrink_backward
 - func: softshrink_backward.grad_input
   wrap_impl: softshrink_backward_out
 - func: sort
   wrap_impl: sort
 - func: sort.dimname
   wrap_impl: sort
 - func: sort.dimname_values
   wrap_impl: sort_out
 - func: sort.values
   wrap_impl: sort_out
 - func: sqrt
   wrap_impl: sqrt
 - func: sqrt.out
   wrap_impl: sqrt_out
 - func: sqrt_
   wrap_impl: sqrt_
 - squeeze
 - squeeze.dim
 - func: stack
   wrap_impl: stack
 - func: stack.out
   wrap_impl: stack_out
 - func: std
   wrap_impl: std
 - func: std.dim
   wrap_impl: std
 - func: std.names_dim
   wrap_impl: std
 - func: std.correction
   wrap_impl: std
 - func: std.names_out
   wrap_impl: std_out
 - func: std.out
   wrap_impl: std_out
 - func: std_mean
   wrap_impl: std_mean
 - func: std_mean.dim
   wrap_impl: std_mean
 - func: std_mean.names_dim
   wrap_impl: std_mean
 - func: sub.Scalar
   wrap_impl: sub
 - func: sub.Tensor
   wrap_impl: sub
 - func: sub.out
   wrap_impl: sub_out
 - func: sub_.Scalar
   wrap_impl: sub_
 - func: sub_.Tensor
   wrap_impl: sub_
 - func: sum
   wrap_impl: sum
 - func: sum.DimnameList_out
   wrap_impl: sum_out
 - func: sum.IntList_out
   wrap_impl: sum_out
 - func: sum.dim_DimnameList
   wrap_impl: sum
 - func: sum.dim_IntList
   wrap_impl: sum
 - func: take
   wrap_impl: take
 - func: take.out
   wrap_impl: take_out
 - func: tan
   wrap_impl: tan
 - func: tan.out
   wrap_impl: tan_out
 - func: tan_
   wrap_impl: tan_
 - func: tanh
   wrap_impl: tanh
 - func: tanh.out
   wrap_impl: tanh_out
 - func: tanh_
   wrap_impl: tanh_
 - func: tanh_backward
   wrap_impl: tanh_backward
 - func: tanh_backward.grad_input
   wrap_impl: tanh_backward_out
 - func: threshold
   wrap_impl: threshold
 - func: threshold.out
   wrap_impl: threshold_out
 - func: threshold_
   wrap_impl: threshold_
 - func: threshold_backward
   wrap_impl: threshold_backward
 - to.device
 - to.dtype
 - to.dtype_layout
 - to.other
 - func: topk
   wrap_impl: topk
 - func: topk.values
   wrap_impl: topk_out
 - func: triangular_solve.X
   wrap_impl: triangular_solve_out
 - func: tril
   wrap_impl: tril
 - func: tril.out
   wrap_impl: tril_out
 - func: tril_
   wrap_impl: tril_
 - tril_indices
 - func: triu
   wrap_impl: triu
 - func: triu.out
   wrap_impl: triu_out
 - func: triu_
   wrap_impl: triu_
 - triu_indices
 - func: true_divide.Scalar
   wrap_impl: true_divide
 - func: true_divide.Tensor
   wrap_impl: true_divide
 - func: true_divide.out
   wrap_impl: true_divide_out
 - func: true_divide_.Scalar
   wrap_impl: true_divide_
 - func: true_divide_.Tensor
   wrap_impl: true_divide_
 - func: trunc
   wrap_impl: trunc
 - func: trunc.out
   wrap_impl: trunc_out
 - func: trunc_
   wrap_impl: trunc_
 - unfold
 - func: uniform_
   wrap_impl: uniform_
 - func: unique_consecutive
   wrap_impl: unique_consecutive
 - unsqueeze
 - func: upsample_bicubic2d
   wrap_impl: upsample_bicubic2d
 - func: upsample_bicubic2d.out
   wrap_impl: upsample_bicubic2d_out
 - func: upsample_bicubic2d_backward
   wrap_impl: upsample_bicubic2d_backward
 - func: upsample_bicubic2d_backward.grad_input
   wrap_impl: upsample_bicubic2d_backward_out
 - func: upsample_bilinear2d
   wrap_impl: upsample_bilinear2d
 - func: upsample_bilinear2d.out
   wrap_impl: upsample_bilinear2d_out
 - func: upsample_bilinear2d_backward
   wrap_impl: upsample_bilinear2d_backward
 - func: upsample_bilinear2d_backward.grad_input
   wrap_impl: upsample_bilinear2d_backward_out
 - func: upsample_linear1d
   wrap_impl: upsample_linear1d
 - func: upsample_linear1d.out
   wrap_impl: upsample_linear1d_out
 - func: upsample_linear1d_backward
   wrap_impl: upsample_linear1d_backward
 - func: upsample_nearest1d
   wrap_impl: upsample_nearest1d
 - func: upsample_nearest1d.out
   wrap_impl: upsample_nearest1d_out
 - func: upsample_nearest1d_backward
   wrap_impl: upsample_nearest1d_backward
 - func: upsample_nearest1d_backward.grad_input
   wrap_impl: upsample_nearest1d_backward_out
 - func: upsample_nearest2d
   wrap_impl: upsample_nearest2d
 - func: upsample_nearest2d.out
   wrap_impl: upsample_nearest2d_out
 - func: upsample_nearest2d_backward
   wrap_impl: upsample_nearest2d_backward
 - func: upsample_nearest2d_backward.grad_input
   wrap_impl: upsample_nearest2d_backward_out
 - func: upsample_nearest3d
   wrap_impl: upsample_nearest3d
 - func: upsample_nearest3d.out
   wrap_impl: upsample_nearest3d_out
 - func: upsample_nearest3d_backward
   wrap_impl: upsample_nearest3d_backward
 - func: upsample_nearest3d_backward.grad_input
   wrap_impl: upsample_nearest3d_backward_out
 - func: upsample_trilinear3d
   wrap_impl: upsample_trilinear3d
 - func: upsample_trilinear3d.out
   wrap_impl: upsample_trilinear3d_out
 - func: upsample_trilinear3d_backward
   wrap_impl: upsample_trilinear3d_backward
 - func: upsample_trilinear3d_backward.grad_input
   wrap_impl: upsample_trilinear3d_backward_out
 - func: var
   wrap_impl: var
 - func: var.dim
   wrap_impl: var
 - func: var.names_dim
   wrap_impl: var
 - func: var.names_out
   wrap_impl: var_out
 - func: var.out
   wrap_impl: var_out
 - func: var.correction
   wrap_impl: var
 - func: var.correction_names
   wrap_impl: var
 - func: var.correction_out
   wrap_impl: var_out
 - func: var.correction_names_out
   wrap_impl: var_out
 - func: var_mean
   wrap_impl: var_mean
 - func: var_mean.dim
   wrap_impl: var_mean
 - func: var_mean.names_dim
   wrap_impl: var_mean
 - func: var_mean.correction
   wrap_impl: var_mean
 - func: var_mean.correction_names
   wrap_impl: var_mean
 - view
 - func: where
   wrap_impl: where
 - func: where.self
   wrap_impl: where
 - func: where.self_out
   wrap_impl: where_out
 - func: xlogy.OutScalar_Other
   wrap_impl: xlogy_out
 - func: xlogy.OutScalar_Self
   wrap_impl: xlogy_out
 - func: xlogy.OutTensor
   wrap_impl: xlogy_out
 - func: xlogy.Scalar_Other
   wrap_impl: xlogy
 - func: xlogy.Scalar_Self
   wrap_impl: xlogy
 - func: xlogy.Tensor
   wrap_impl: xlogy
 - func: xlogy_.Scalar_Other
   wrap_impl: xlogy_
 - func: xlogy_.Tensor
   wrap_impl: xlogy_
 - func: zero_
   wrap_impl: zero_
 - func: zeros
   wrap_impl: zeros
 - func: zeros.names
   wrap_impl: zeros
 - func: zeros.out
   wrap_impl: zeros_out
 - func: zeros_like
   wrap_impl: zeros_like
 - _copy_from_and_resize

autograd:
  - func: celu
    wrap_impl: celu
  - func: celu_
    wrap_impl: celu_
  - func: elu.out
    wrap_impl: elu_out
  - func: elu
    wrap_impl: elu
  - func: elu_
    wrap_impl: elu_
  - func: silu
    wrap_impl: silu
  - func: silu_
    wrap_impl: silu_
  - func: silu.out
    wrap_impl: silu_out
  - func: binary_cross_entropy_with_logits
    wrap_impl: binary_cross_entropy_with_logits
  - func: selu
    wrap_impl: selu
  - func: selu_
    wrap_impl: selu_
  - flatten_dense_tensors
  - unflatten_dense_tensors

symint:
 - as_strided_
 - func: embedding
   wrap_impl: embedding_symint
 - func: embedding_backward
   wrap_impl: embedding_backward_symint
 - func: _embedding_bag_backward
   wrap_impl: _embedding_bag_backward_symint
 - func: repeat_interleave.self_int
   wrap_impl: repeat_interleave_symint
 - func: zeros
   wrap_impl: zeros_symint
 - new_empty_strided

custom:
  - func: npu_change_data_ptr(Tensor dst, Tensor src, int index) -> int
  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
    wrap_impl: npu_transpose
  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_transpose_out
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
    wrap_impl: npu_broadcast
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_broadcast_out
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    wrap_impl: npu_dtype_cast_
  - func: npu_alloc_float_status(Tensor self) -> Tensor
  - func: npu_get_float_status(Tensor self) -> Tensor
  - func: npu_clear_float_status(Tensor self) -> Tensor
  - func: one_(Tensor(a!) self) -> Tensor(a!)
    wrap_impl: one_
  - func: npu_fast_gelu(Tensor self) -> Tensor
    wrap_impl: npu_fast_gelu
  - func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
    wrap_impl: npu_fast_gelu_backward
  - func: _amp_foreach_non_finite_check(Tensor[] scaled_grads) -> bool
    wrap_impl: _amp_foreach_non_finite_check
  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
    wrap_impl: npu_sign_bits_pack
  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
    wrap_impl: npu_bert_apply_adam
  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    wrap_impl: npu_bert_apply_adam_out
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv_transpose2d_backward
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv_transpose3d_backward
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_convolution_backward
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_conv_transpose2d
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_conv2d
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_conv2d_out
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv2d_backward
  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_conv3d
  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_conv3d_out
  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv3d_backward
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    dispatch:
     CompositeExplicitAutograd: empty_with_format
  - func: unsafe_empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor
    dispatch:
     CompositeExplicitAutograd: empty_with_format
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    dispatch:
     CompositeExplicitAutograd: empty_with_format
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
    wrap_impl: npu_stride_add
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
    wrap_impl: npu_slice
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_slice_out
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
    wrap_impl: npu_indexing
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_indexing_out
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
    wrap_impl: npu_softmax_cross_entropy_with_logits_backward
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
    wrap_impl: npu_roi_align
  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
    wrap_impl: npu_roi_alignbk
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
    wrap_impl: npu_ps_roi_pooling_backward
  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_sort_v2_out
  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
    wrap_impl: npu_sort_v2
  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
    wrap_impl: npu_confusion_transpose_backward
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
    wrap_impl: npu_one_hot
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
    wrap_impl: npu_linear_backward
  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
    wrap_impl: npu_anchor_response_flags
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
    wrap_impl: npu_dropout_backward
  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_nms_rotated
  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
    wrap_impl: npu_masked_fill_range
  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
    wrap_impl: npu_sub_sample
  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
    wrap_impl: npu_yolo_boxes_encode
  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
    wrap_impl: npu_scatter
  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
    wrap_impl: npu_layer_norm_eval
  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
    wrap_impl: npu_max_backward
  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
    wrap_impl: npu_rotated_box_encode
  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
    wrap_impl: npu_rotated_box_decode
  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
    wrap_impl: npu_rotated_overlaps
  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
    wrap_impl: npu_silu_backward
  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
    wrap_impl: npu_rotated_iou
  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_nms_with_mask
  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> Tensor[]
    wrap_impl: npu_gru_backward
  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
    wrap_impl: npu_mish_backward
  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
    wrap_impl: npu_min_backward
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
    wrap_impl: npu_reshape
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_reshape_out
  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_batch_nms
  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
    wrap_impl: npu_bounding_box_encode
  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
    wrap_impl: npu_bounding_box_decode
  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_apply_adam
  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    wrap_impl: npu_apply_adam_out
  - func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_apply_adam_w
  - func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    wrap_impl: npu_apply_adam_w_out
  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_deformable_conv2dbk
  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_giou_backward
  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_diou_backward
  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
    wrap_impl: npu_iou
  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
    wrap_impl: npu_nms_v4
  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
    wrap_impl: npu_pad
  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
    wrap_impl: npu_random_choice_with_mask
    tags: nondeterministic_seeded
  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
    wrap_impl: npu_normalize_batch
  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
    wrap_impl: npu_ptiou
  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_backward
  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
    wrap_impl: _dropout_with_byte_mask_backward
  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
    wrap_impl: dropout_with_byte_mask
    tags: nondeterministic_seeded
  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
    wrap_impl: npu_dropout_with_add_softmax_backward
  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
    wrap_impl: npu_multi_head_attention_backward
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    wrap_impl: npu_dropout_gen_mask
    tags: nondeterministic_seeded
    dispatch:
     CompositeExplicitAutograd: npu_dropout_gen_mask
  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_ciou_backward
  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
    wrap_impl: npu_sign_bits_unpack
  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3, bool try_recover_truncated=False) -> Tensor
    wrap_impl: decode_jpeg
  - func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
    wrap_impl: crop_and_resize
  - func: reverse(Tensor self, int[] axis) -> Tensor
    wrap_impl: reverse
  - func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor
    wrap_impl: image_normalize
  - func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)
    wrap_impl: image_normalize_
  - func: img_to_tensor(Tensor self) -> Tensor
    wrap_impl: img_to_tensor
  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
    wrap_impl: _conv_depthwise2d_backward
  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    wrap_impl: slow_conv_dilated2d_backward
  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    wrap_impl: slow_conv_transpose2d_backward
  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_cell_backward
  - func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)
    wrap_impl: batch_norm_reduce
  - func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
    wrap_impl: batch_norm_gather_stats_update
  - func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_fused_attention_score_backward
  - func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_fused_attention_score_fwd
  - func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_fused_attention_score_grad
  - func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]
    wrap_impl: npu_fused_attention_qkv_grad
  - func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]
    wrap_impl: npu_fused_attention_layernorm_qkv_fwd
  - func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_layernorm_grad
  - func: format_contiguous(Tensor self) -> Tensor
  - func: check_match(Tensor self) -> bool
  - func: check_memory_overlaps(Tensor[] inputs, Tensor[] outputs) -> ()
  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
    wrap_impl: npu_ifmr
  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
    wrap_impl: npu_grid_assign_positive
  - func: get_storage_size(Tensor self) -> int
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_enque_tensor(Tensor[] tensors, str format_string, int capacity=3) -> ()
  - func: npu_hcom_allgather(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allgather.out(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)

custom_autograd:
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_convolution
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_convolution_transpose
  - func: fast_gelu(Tensor self) -> Tensor
    wrap_impl: fast_gelu
  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
    wrap_impl: npu_confusion_transpose
  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
    wrap_impl: npu_ps_roi_pooling
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
    wrap_impl: npu_linear
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
    wrap_impl: _npu_dropout
    tags: nondeterministic_seeded
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
    wrap_impl: npu_softmax_cross_entropy_with_logits
  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_max
  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_max
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
    wrap_impl: npu_bmmV2
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
    wrap_impl: npu_dtype_cast
  - func: npu_silu(Tensor self) -> Tensor
    wrap_impl: npu_silu
  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
    wrap_impl: npu_silu_
  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> Tensor[]
    wrap_impl: npu_gru
    tags: nondeterministic_seeded
  - func: npu_mish(Tensor self) -> Tensor
    wrap_impl: npu_mish
  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_min
  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_min
  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
    wrap_impl: npu_deformable_conv2d
  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
    wrap_impl: npu_giou
  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
    wrap_impl: npu_diou
  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seqMask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flagSeq, bool direction) -> Tensor[]
    wrap_impl: npu_lstm
    tags: nondeterministic_seeded
  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
    wrap_impl: _dropout_with_byte_mask
    tags: nondeterministic_seeded
  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_dropout_with_add_softmax
    tags: nondeterministic_seeded
  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
    wrap_impl: npu_scaled_masked_softmax
  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> Tensor[]
    wrap_impl: npu_multi_head_attention
    tags: nondeterministic_seeded
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
    wrap_impl: npu_dropout_do_mask
    tags: nondeterministic_seeded
  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
    wrap_impl: npu_ciou
  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor[]
    wrap_impl: npu_lstm_cell
  - func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor
    wrap_impl: npu_fused_attention_score
