backend: NPU
cpp_namespace: at_npu::native
supported:
 - _local_scalar_dense
 - _pin_memory
 - _reshape_alias
 - as_strided
 - as_strided_
 - bartlett_window
 - bartlett_window.periodic
 - blackman_window
 - blackman_window.periodic
 - func: clone
   op_api: True
 - contiguous
 - func: copy_
   op_api: True
 - copy_memory_
 - empty.memory_format
 - empty_like
 - empty_strided
 - empty_with_format
 - empty_with_format.names
 - full
 - full.names
 - full.out
 - hamming_window
 - hamming_window.periodic
 - hamming_window.periodic_alpha
 - hamming_window.periodic_alpha_beta
 - hann_window
 - hann_window.periodic
 - is_pinned
 - func: is_set_to
   op_api: False
 - isnan
 - max_pool2d
 - resize_
 - resize_as_
 - func: scalar_tensor
   op_api: True
 - set_
 - set_.source_Storage
 - set_.source_Storage_storage_offset
 - set_.source_Tensor
 - squeeze
 - squeeze.dim
 - to.device
 - func: to.dtype
   op_api: True
 - to.dtype_layout
 - to.other
 - tril_indices
 - triu_indices
 - unfold
 - unsqueeze
 - view
 - view_as_complex
 - view_as_real

autograd:
  - func: matmul
    op_api: False
  - func: matmul.out
    op_api: False

custom:
  - func: _npu_storage_resize(Tensor self, int size) -> Tensor
  - func: npu_change_data_ptr(Tensor dst, Tensor src, int index) -> int
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
    exposed: True
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
    exposed: True
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    exposed: True
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    exposed: True
  - func: unsafe_empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    exposed: True
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
    exposed: True
  - func: npu_multi_head_attention_grad(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, bool gen_mask_parallel=True, bool sync=False) -> Tensor[]
  - func: format_contiguous(Tensor self) -> Tensor
  - func: check_match(Tensor self) -> bool
  - func: check_memory_overlaps(Tensor[] inputs, Tensor[] outputs) -> ()
  - func: get_storage_size(Tensor self) -> int
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_hcom_allgather(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm) -> Tensor
  - func: npu_hcom_allgather.out(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
    exposed: True
  - func: _npu_format_cast(Tensor self, int acl_format) -> Tensor

custom_autograd:
  - func: npu_multi_head_attention_score(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, bool gen_mask_parallel=True, bool sync=False) -> Tensor[]
