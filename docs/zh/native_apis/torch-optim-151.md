# torch.optim

> [!NOTE]  
> 若API“是否支持“为“是“，“限制与说明“为“-“，说明此API和原生API支持度保持一致。

|API名称|是否支持|限制与说明|
|--|--|--|
|torch.optim.Optimizer|是|-|
|Optimizer.add_param_group|是|-|
|Optimizer.load_state_dict|是|-|
|Optimizer.state_dict|是|-|
|Optimizer.step|是|-|
|Optimizer.zero_grad|是|-|
|torch.optim.Adadelta|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.Adadelta.add_param_group|是|-|
|torch.optim.Adadelta.load_state_dict|是|-|
|torch.optim.Adadelta.register_load_state_dict_post_hook|是|-|
|torch.optim.Adadelta.register_load_state_dict_pre_hook|是|-|
|torch.optim.Adadelta.register_state_dict_post_hook|是|-|
|torch.optim.Adadelta.register_state_dict_pre_hook|是|-|
|torch.optim.Adadelta.register_step_post_hook|是|-|
|torch.optim.Adadelta.register_step_pre_hook|是|-|
|torch.optim.Adadelta.state_dict|是|-|
|torch.optim.Adadelta.step|否|-|
|torch.optim.Adadelta.zero_grad|是|-|
|torch.optim.Adagrad|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.Adagrad.add_param_group|是|-|
|torch.optim.Adagrad.load_state_dict|是|-|
|torch.optim.Adagrad.register_load_state_dict_post_hook|是|-|
|torch.optim.Adagrad.register_load_state_dict_pre_hook|是|-|
|torch.optim.Adagrad.register_state_dict_post_hook|是|-|
|torch.optim.Adagrad.register_state_dict_pre_hook|是|-|
|torch.optim.Adagrad.register_step_post_hook|是|-|
|torch.optim.Adagrad.register_step_pre_hook|是|-|
|torch.optim.Adagrad.state_dict|是|-|
|torch.optim.Adagrad.step|是|-|
|torch.optim.Adagrad.zero_grad|是|-|
|torch.optim.Adam|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False<br>可能回退至CPU执行|
|torch.optim.Adam.add_param_group|是|-|
|torch.optim.Adam.load_state_dict|是|-|
|torch.optim.Adam.register_load_state_dict_post_hook|是|-|
|torch.optim.Adam.register_load_state_dict_pre_hook|是|-|
|torch.optim.Adam.register_state_dict_post_hook|是|-|
|torch.optim.Adam.register_state_dict_pre_hook|是|-|
|torch.optim.Adam.register_step_post_hook|是|-|
|torch.optim.Adam.register_step_pre_hook|是|-|
|torch.optim.Adam.state_dict|是|-|
|torch.optim.Adam.step|否|-|
|torch.optim.Adam.zero_grad|是|-|
|torch.optim.AdamW|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False<br>优化器在启动fused的情况下（fused=True），暂不支持grad_scale和found_inf参数。对标_single_tensor_adamw实现，fp32与cpu/cuda一致，fp16和bf16采用升精度实现，与cpu/cuda不一致|
|torch.optim.AdamW.add_param_group|是|-|
|torch.optim.AdamW.load_state_dict|是|-|
|torch.optim.AdamW.register_load_state_dict_post_hook|是|-|
|torch.optim.AdamW.register_load_state_dict_pre_hook|是|-|
|torch.optim.AdamW.register_state_dict_post_hook|是|-|
|torch.optim.AdamW.register_state_dict_pre_hook|是|-|
|torch.optim.AdamW.register_step_post_hook|是|-|
|torch.optim.AdamW.register_step_pre_hook|是|-|
|torch.optim.AdamW.state_dict|是|-|
|torch.optim.AdamW.step|是|支持fp16，fp32|
|torch.optim.AdamW.zero_grad|是|支持fp16，fp32|
|torch.optim.SparseAdam.add_param_group|是|-|
|torch.optim.SparseAdam.load_state_dict|是|-|
|torch.optim.SparseAdam.register_load_state_dict_post_hook|是|-|
|torch.optim.SparseAdam.register_load_state_dict_pre_hook|是|-|
|torch.optim.SparseAdam.register_state_dict_post_hook|是|-|
|torch.optim.SparseAdam.register_state_dict_pre_hook|是|-|
|torch.optim.SparseAdam.register_step_post_hook|是|-|
|torch.optim.SparseAdam.register_step_pre_hook|是|-|
|torch.optim.Adamax|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.Adamax.add_param_group|是|支持fp16，fp32|
|torch.optim.Adamax.load_state_dict|是|支持fp16，fp32|
|torch.optim.Adamax.register_load_state_dict_post_hook|是|-|
|torch.optim.Adamax.register_load_state_dict_pre_hook|是|-|
|torch.optim.Adamax.register_state_dict_post_hook|是|-|
|torch.optim.Adamax.register_state_dict_pre_hook|是|-|
|torch.optim.Adamax.register_step_post_hook|是|-|
|torch.optim.Adamax.register_step_pre_hook|是|-|
|torch.optim.Adamax.state_dict|是|支持fp16，fp32|
|torch.optim.Adamax.step|是|支持fp16，fp32|
|torch.optim.Adamax.zero_grad|是|支持fp16，fp32|
|torch.optim.ASGD|是|支持fp16，fp32|
|torch.optim.ASGD.add_param_group|是|支持fp16，fp32|
|torch.optim.ASGD.load_state_dict|是|支持fp16，fp32|
|torch.optim.ASGD.register_load_state_dict_post_hook|是|-|
|torch.optim.ASGD.register_load_state_dict_pre_hook|是|-|
|torch.optim.ASGD.register_state_dict_post_hook|是|-|
|torch.optim.ASGD.register_state_dict_pre_hook|是|-|
|torch.optim.ASGD.register_step_post_hook|是|-|
|torch.optim.ASGD.register_step_pre_hook|是|-|
|torch.optim.ASGD.state_dict|是|支持fp16，fp32|
|torch.optim.ASGD.step|是|支持fp16，fp32|
|torch.optim.ASGD.zero_grad|是|支持fp16，fp32|
|torch.optim.LBFGS|是|-|
|torch.optim.LBFGS.add_param_group|是|-|
|torch.optim.LBFGS.load_state_dict|是|-|
|torch.optim.LBFGS.register_load_state_dict_post_hook|是|-|
|torch.optim.LBFGS.register_load_state_dict_pre_hook|是|-|
|torch.optim.LBFGS.register_state_dict_post_hook|是|-|
|torch.optim.LBFGS.register_state_dict_pre_hook|是|-|
|torch.optim.LBFGS.register_step_post_hook|是|-|
|torch.optim.LBFGS.register_step_pre_hook|是|-|
|torch.optim.LBFGS.state_dict|是|-|
|torch.optim.LBFGS.step|否|-|
|torch.optim.LBFGS.zero_grad|是|-|
|torch.optim.NAdam|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.NAdam.add_param_group|是|支持fp16，fp32|
|torch.optim.NAdam.load_state_dict|是|支持fp16，fp32|
|torch.optim.NAdam.register_load_state_dict_post_hook|是|-|
|torch.optim.NAdam.register_load_state_dict_pre_hook|是|-|
|torch.optim.NAdam.register_state_dict_post_hook|是|-|
|torch.optim.NAdam.register_state_dict_pre_hook|是|-|
|torch.optim.NAdam.register_step_post_hook|是|-|
|torch.optim.NAdam.register_step_pre_hook|是|-|
|torch.optim.NAdam.state_dict|是|支持fp16，fp32|
|torch.optim.NAdam.step|是|支持fp16，fp32|
|torch.optim.NAdam.zero_grad|是|支持fp16，fp32|
|torch.optim.RAdam|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.RAdam.add_param_group|是|支持fp16，fp32|
|torch.optim.RAdam.load_state_dict|是|支持fp16，fp32|
|torch.optim.RAdam.register_load_state_dict_post_hook|是|-|
|torch.optim.RAdam.register_load_state_dict_pre_hook|是|-|
|torch.optim.RAdam.register_state_dict_post_hook|是|-|
|torch.optim.RAdam.register_state_dict_pre_hook|是|-|
|torch.optim.RAdam.register_step_post_hook|是|-|
|torch.optim.RAdam.register_step_pre_hook|是|-|
|torch.optim.RAdam.state_dict|是|支持fp16，fp32|
|torch.optim.RAdam.step|是|支持fp16，fp32|
|torch.optim.RAdam.zero_grad|是|支持fp16，fp32|
|torch.optim.RMSprop|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.RMSprop.add_param_group|是|-|
|torch.optim.RMSprop.load_state_dict|是|-|
|torch.optim.RMSprop.register_load_state_dict_post_hook|是|-|
|torch.optim.RMSprop.register_load_state_dict_pre_hook|是|-|
|torch.optim.RMSprop.register_state_dict_post_hook|是|-|
|torch.optim.RMSprop.register_state_dict_pre_hook|是|-|
|torch.optim.RMSprop.register_step_post_hook|是|-|
|torch.optim.RMSprop.register_step_pre_hook|是|-|
|torch.optim.RMSprop.state_dict|是|-|
|torch.optim.RMSprop.step|是|-|
|torch.optim.RMSprop.zero_grad|是|-|
|torch.optim.Rprop|是|-|
|torch.optim.Rprop.add_param_group|是|支持fp16，fp32|
|torch.optim.Rprop.load_state_dict|是|支持fp16，fp32|
|torch.optim.Rprop.register_load_state_dict_post_hook|是|-|
|torch.optim.Rprop.register_load_state_dict_pre_hook|是|-|
|torch.optim.Rprop.register_state_dict_post_hook|是|-|
|torch.optim.Rprop.register_state_dict_pre_hook|是|-|
|torch.optim.Rprop.register_step_post_hook|是|-|
|torch.optim.Rprop.register_step_pre_hook|是|-|
|torch.optim.Rprop.state_dict|是|支持fp16，fp32|
|torch.optim.Rprop.step|是|支持fp16，fp32|
|torch.optim.Rprop.zero_grad|是|支持fp16，fp32|
|torch.optim.SGD|是|支持bf16，fp16，fp32<br>优化器在启动foreach的情况下（默认情况foreach=None或foreach=True），当被优化的参数分组过多时由于foreach算子的特性会导致性能下降。这种情况建议设置为foreach=False|
|torch.optim.SGD.add_param_group|是|支持fp16，fp32|
|torch.optim.SGD.load_state_dict|是|支持fp16，fp32|
|torch.optim.SGD.register_load_state_dict_post_hook|是|-|
|torch.optim.SGD.register_load_state_dict_pre_hook|是|-|
|torch.optim.SGD.register_state_dict_post_hook|是|-|
|torch.optim.SGD.register_state_dict_pre_hook|是|-|
|torch.optim.SGD.register_step_post_hook|是|-|
|torch.optim.SGD.register_step_pre_hook|是|-|
|torch.optim.SGD.state_dict|是|支持fp16，fp32|
|torch.optim.SGD.step|是|支持fp16，fp32|
|torch.optim.SGD.zero_grad|是|支持fp16，fp32|
|torch.optim.lr_scheduler.LambdaLR|是|-|
|torch.optim.lr_scheduler.LambdaLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.LambdaLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.LambdaLR.print_lr|是|-|
|torch.optim.lr_scheduler.LambdaLR.state_dict|是|-|
|torch.optim.lr_scheduler.MultiplicativeLR|是|-|
|torch.optim.lr_scheduler.MultiplicativeLR.get_last_lr|是|支持fp32|
|torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict|是|支持fp32|
|torch.optim.lr_scheduler.MultiplicativeLR.print_lr|是|支持fp32|
|torch.optim.lr_scheduler.MultiplicativeLR.state_dict|是|支持fp32|
|torch.optim.lr_scheduler.StepLR|是|-|
|torch.optim.lr_scheduler.StepLR.get_last_lr|是|支持fp16，fp32|
|torch.optim.lr_scheduler.StepLR.load_state_dict|是|支持fp16，fp32|
|torch.optim.lr_scheduler.StepLR.print_lr|是|支持fp16，fp32|
|torch.optim.lr_scheduler.StepLR.state_dict|是|支持fp16，fp32|
|torch.optim.lr_scheduler.MultiStepLR|是|-|
|torch.optim.lr_scheduler.MultiStepLR.get_last_lr|是|支持fp16，fp32|
|torch.optim.lr_scheduler.MultiStepLR.load_state_dict|是|支持fp16，fp32|
|torch.optim.lr_scheduler.MultiStepLR.print_lr|是|支持fp16，fp32|
|torch.optim.lr_scheduler.MultiStepLR.state_dict|是|支持fp16，fp32|
|torch.optim.lr_scheduler.ConstantLR|是|-|
|torch.optim.lr_scheduler.ConstantLR.get_last_lr|是|支持fp32|
|torch.optim.lr_scheduler.ConstantLR.load_state_dict|是|支持fp32|
|torch.optim.lr_scheduler.ConstantLR.print_lr|是|支持fp32|
|torch.optim.lr_scheduler.ConstantLR.state_dict|是|支持fp32|
|torch.optim.lr_scheduler.LinearLR|是|-|
|torch.optim.lr_scheduler.LinearLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.LinearLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.LinearLR.print_lr|是|-|
|torch.optim.lr_scheduler.LinearLR.state_dict|是|-|
|torch.optim.lr_scheduler.ExponentialLR|是|-|
|torch.optim.lr_scheduler.ExponentialLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.ExponentialLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.ExponentialLR.print_lr|是|-|
|torch.optim.lr_scheduler.ExponentialLR.state_dict|是|-|
|torch.optim.lr_scheduler.PolynomialLR|是|-|
|torch.optim.lr_scheduler.PolynomialLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.PolynomialLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.PolynomialLR.print_lr|是|-|
|torch.optim.lr_scheduler.PolynomialLR.state_dict|是|-|
|torch.optim.lr_scheduler.CosineAnnealingLR|是|-|
|torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.CosineAnnealingLR.print_lr|是|-|
|torch.optim.lr_scheduler.CosineAnnealingLR.state_dict|是|-|
|torch.optim.lr_scheduler.ChainedScheduler|是|-|
|torch.optim.lr_scheduler.ChainedScheduler.get_last_lr|是|-|
|torch.optim.lr_scheduler.ChainedScheduler.load_state_dict|是|-|
|torch.optim.lr_scheduler.ChainedScheduler.print_lr|是|-|
|torch.optim.lr_scheduler.ChainedScheduler.state_dict|是|-|
|torch.optim.lr_scheduler.SequentialLR|是|-|
|torch.optim.lr_scheduler.SequentialLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.SequentialLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.SequentialLR.print_lr|是|-|
|torch.optim.lr_scheduler.SequentialLR.state_dict|是|-|
|torch.optim.lr_scheduler.ReduceLROnPlateau|是|-|
|torch.optim.lr_scheduler.CyclicLR|是|-|
|torch.optim.lr_scheduler.CyclicLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.CyclicLR.get_lr|是|-|
|torch.optim.lr_scheduler.CyclicLR.print_lr|是|-|
|torch.optim.lr_scheduler.OneCycleLR|是|-|
|torch.optim.lr_scheduler.OneCycleLR.get_last_lr|是|-|
|torch.optim.lr_scheduler.OneCycleLR.load_state_dict|是|-|
|torch.optim.lr_scheduler.OneCycleLR.print_lr|是|-|
|torch.optim.lr_scheduler.OneCycleLR.state_dict|是|-|
|torch.optim.lr_scheduler.CosineAnnealingWarmRestarts|是|-|
|torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_last_lr|是|-|
|torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.load_state_dict|是|-|
|torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.print_lr|是|-|
|torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.state_dict|是|-|
|torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step|是|-|


