{
  "torch_npu.contrib.BiLSTM": {
    "signature": "(input_size, hidden_size)"
  },
  "torch_npu.contrib.BiLSTM.forward": {
    "signature": "(self, inputs)"
  },
  "torch_npu.contrib.ChannelShuffle": {
    "signature": "(in_channels, groups=2, split_shuffle=True)"
  },
  "torch_npu.contrib.ChannelShuffle.check_self": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.ChannelShuffle.forward": {
    "signature": "(self, x1, x2)"
  },
  "torch_npu.contrib.DCNv2": {
    "signature": "(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, bias=True, pack=True)"
  },
  "torch_npu.contrib.DCNv2.init_param": {
    "signature": "(self)"
  },
  "torch_npu.contrib.DCNv2.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.Focus": {
    "signature": "(c1, c2, k=1, s=1, p=None, g=1, act=True)"
  },
  "torch_npu.contrib.Focus.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.FusedColorJitter": {
    "signature": "(brightness=0, contrast=0, saturation=0, hue=0)"
  },
  "torch_npu.contrib.FusedColorJitter._check_input": {
    "signature": "(self, value, name, center=1, bound=(0, inf), clip_first_on_zero=True)"
  },
  "torch_npu.contrib.FusedColorJitter.forward": {
    "signature": "(self, img)"
  },
  "torch_npu.contrib.LabelSmoothingCrossEntropy": {
    "signature": "(num_classes=1000, smooth_factor=0.0)"
  },
  "torch_npu.contrib.LabelSmoothingCrossEntropy.forward": {
    "signature": "(self, pred, target)"
  },
  "torch_npu.contrib.LinearA8W8Quant": {
    "signature": "(in_features: int, out_features: int, *, bias: bool = True, offset: bool = False, pertoken_scale: bool = False, device=None, dtype=None, output_dtype=None) -> None"
  },
  "torch_npu.contrib.LinearA8W8Quant.forward": {
    "signature": "(self, linear_quant_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.LinearWeightQuant": {
    "signature": "(in_features, out_features, bias: bool = True, device=None, dtype=None, antiquant_offset: bool = False, quant_scale: bool = False, quant_offset: bool = False, antiquant_group_size: int = 0) -> None"
  },
  "torch_npu.contrib.LinearWeightQuant.forward": {
    "signature": "(self, x: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.LinearQuant": {
    "signature": "(in_features: int, out_features: int, *, bias: bool = True, offset: bool = False, pertoken_scale: bool = False, device=None, dtype=None, output_dtype=None) -> None"
  },
  "torch_npu.contrib.LinearQuant.forward": {
    "signature": "(self, linear_quant_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.Mish": {
    "signature": "()"
  },
  "torch_npu.contrib.Mish.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.ModulatedDeformConv": {
    "signature": "(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, bias=True, pack=True)"
  },
  "torch_npu.contrib.ModulatedDeformConv.init_param": {
    "signature": "(self)"
  },
  "torch_npu.contrib.ModulatedDeformConv.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.MultiheadAttention": {
    "signature": "(embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8)"
  },
  "torch_npu.contrib.MultiheadAttention.prepare_for_onnx_export_": {
    "signature": "(self)"
  },
  "torch_npu.contrib.MultiheadAttention.prepare_for_tpu_": {
    "signature": "(self, **kwargs)"
  },
  "torch_npu.contrib.MultiheadAttention.reset_parameters": {
    "signature": "(self)"
  },
  "torch_npu.contrib.MultiheadAttention.transpose_for_scores": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.MultiheadAttention.forward": {
    "signature": "(self, query, key: Union[torch.Tensor, NoneType], value: Union[torch.Tensor, NoneType], bsz, tgt_len, s_len, key_padding_mask: Union[torch.Tensor, NoneType] = None, incremental_state: Union[Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], NoneType] = None, need_weights: bool = True, static_kv: bool = False, attn_mask: Union[torch.Tensor, NoneType] = None, before_softmax: bool = False, need_head_weights: bool = False) -> Tuple[torch.Tensor, Union[torch.Tensor, NoneType]]"
  },
  "torch_npu.contrib.MultiheadAttention.multi_attn": {
    "signature": "(self, query, key, value, key_padding_mask, bsz, tgt_len)"
  },
  "torch_npu.contrib.MultiheadAttention._append_prev_key_padding_mask": {
    "signature": "(key_padding_mask: Union[torch.Tensor, NoneType], prev_key_padding_mask: Union[torch.Tensor, NoneType], batch_size: int, src_len: int, static_kv: bool) -> Union[torch.Tensor, NoneType]"
  },
  "torch_npu.contrib.MultiheadAttention._get_input_buffer": {
    "signature": "(self, incremental_state: Union[Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], NoneType]) -> Dict[str, Union[torch.Tensor, NoneType]]"
  },
  "torch_npu.contrib.MultiheadAttention._set_input_buffer": {
    "signature": "(self, incremental_state: Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], buffer: Dict[str, Union[torch.Tensor, NoneType]])"
  },
  "torch_npu.contrib.MultiheadAttention.apply_sparse_mask": {
    "signature": "(self, attn_weights, tgt_len: int, src_len: int, bsz: int)"
  },
  "torch_npu.contrib.MultiheadAttention.upgrade_state_dict_named": {
    "signature": "(self, state_dict, name)"
  },
  "torch_npu.contrib.NpuCachedDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.NpuCachedDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.NpuCachedDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.NpuDropPath": {
    "signature": "(drop_prob=None)"
  },
  "torch_npu.contrib.NpuDropPath.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.NpuDropPath.enable_droppath_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.NpuFairseqDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.NpuFairseqDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.NpuFairseqDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.PSROIPool": {
    "signature": "(pooled_height=7, pooled_width=7, spatial_scale=0.0625, group_size=7, output_dim=22)"
  },
  "torch_npu.contrib.PSROIPool.forward": {
    "signature": "(self, features, rois)"
  },
  "torch_npu.contrib.Prefetcher": {
    "signature": "(loader, stream=None)"
  },
  "torch_npu.contrib.Prefetcher.preload": {
    "signature": "(self)"
  },
  "torch_npu.contrib.Prefetcher.next": {
    "signature": "(self)"
  },
  "torch_npu.contrib.ROIAlign": {
    "signature": "(output_size, spatial_scale, sampling_ratio, aligned=True)"
  },
  "torch_npu.contrib.ROIAlign.forward": {
    "signature": "(self, input_tensor, rois)"
  },
  "torch_npu.contrib.SiLU": {
    "signature": "()"
  },
  "torch_npu.contrib.SiLU.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.Swish": {
    "signature": "()"
  },
  "torch_npu.contrib.Swish.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.matmul_transpose": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.npu_batched_multiclass_nms": {
    "signature": "(multi_bboxes, multi_scores, score_thr=0.05, nms_thr=0.45, max_num=50, score_factors=None)"
  },
  "torch_npu.contrib.npu_bbox_coder_decode_xywh2xyxy": {
    "signature": "(bboxes, pred_bboxes, means=None, stds=None, max_shape=None, wh_ratio_clip=0.016)"
  },
  "torch_npu.contrib.npu_bbox_coder_encode_xyxy2xywh": {
    "signature": "(bboxes, gt_bboxes, means=None, stds=None, is_normalized=False, normalized_scale=10000.0)"
  },
  "torch_npu.contrib.npu_bbox_coder_encode_yolo": {
    "signature": "(bboxes, gt_bboxes, stride)"
  },
  "torch_npu.contrib.npu_ciou": {
    "signature": "(boxes1, boxes2, trans=True, is_cross=False, mode=0)"
  },
  "torch_npu.contrib.npu_diou": {
    "signature": "(boxes1, boxes2, trans=True, is_cross=False, mode=0)"
  },
  "torch_npu.contrib.npu_fast_condition_index_put": {
    "signature": "(x, condition, value)"
  },
  "torch_npu.contrib.npu_fused_attention": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.npu_fused_attention_with_layernorm": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.npu_giou": {
    "signature": "(boxes1, boxes2, is_permuted=True)"
  },
  "torch_npu.contrib.npu_iou": {
    "signature": "(boxes1, boxes2, mode='ptiou', is_normalized=False, normalized_scale=100.0)"
  },
  "torch_npu.contrib.npu_multiclass_nms": {
    "signature": "(multi_bboxes, multi_scores, score_thr=0.05, nms_thr=0.45, max_num=50, score_factors=None)"
  },
  "torch_npu.contrib.npu_ptiou": {
    "signature": "(boxes1, boxes2, mode='ptiou', is_normalized=False, normalized_scale=100.0)"
  },
  "torch_npu.contrib.npu_single_level_responsible_flags": {
    "signature": "(featmap_size, gt_bboxes, stride, num_base_anchors)"
  },
  "torch_npu.contrib.roll": {
    "signature": "(x, shifts, dims)"
  },
  "torch_npu.contrib.function.dropout_with_byte_mask": {
    "signature": "(input1, p=0.5, training=True, inplace=False)"
  },
  "torch_npu.contrib.function.fuse_add_softmax_dropout": {
    "signature": "(training, dropout, attn_mask, attn_scores, attn_head_size, p=0.5, dim=-1)"
  },
  "torch_npu.contrib.function.matmul_transpose": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.npu_batched_multiclass_nms": {
    "signature": "(multi_bboxes, multi_scores, score_thr=0.05, nms_thr=0.45, max_num=50, score_factors=None)"
  },
  "torch_npu.contrib.function.npu_bbox_coder_decode_xywh2xyxy": {
    "signature": "(bboxes, pred_bboxes, means=None, stds=None, max_shape=None, wh_ratio_clip=0.016)"
  },
  "torch_npu.contrib.function.npu_bbox_coder_encode_xyxy2xywh": {
    "signature": "(bboxes, gt_bboxes, means=None, stds=None, is_normalized=False, normalized_scale=10000.0)"
  },
  "torch_npu.contrib.function.npu_bbox_coder_encode_yolo": {
    "signature": "(bboxes, gt_bboxes, stride)"
  },
  "torch_npu.contrib.function.npu_ciou": {
    "signature": "(boxes1, boxes2, trans=True, is_cross=False, mode=0)"
  },
  "torch_npu.contrib.function.npu_diou": {
    "signature": "(boxes1, boxes2, trans=True, is_cross=False, mode=0)"
  },
  "torch_npu.contrib.function.npu_fast_condition_index_put": {
    "signature": "(x, condition, value)"
  },
  "torch_npu.contrib.function.npu_fused_attention": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.npu_fused_attention_with_layernorm": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.npu_giou": {
    "signature": "(boxes1, boxes2, is_permuted=True)"
  },
  "torch_npu.contrib.function.npu_iou": {
    "signature": "(boxes1, boxes2, mode='ptiou', is_normalized=False, normalized_scale=100.0)"
  },
  "torch_npu.contrib.function.npu_multiclass_nms": {
    "signature": "(multi_bboxes, multi_scores, score_thr=0.05, nms_thr=0.45, max_num=50, score_factors=None)"
  },
  "torch_npu.contrib.function.npu_ptiou": {
    "signature": "(boxes1, boxes2, mode='ptiou', is_normalized=False, normalized_scale=100.0)"
  },
  "torch_npu.contrib.function.npu_single_level_responsible_flags": {
    "signature": "(featmap_size, gt_bboxes, stride, num_base_anchors)"
  },
  "torch_npu.contrib.function.roll": {
    "signature": "(x, shifts, dims)"
  },
  "torch_npu.contrib.function.anchor_generator.box_dtype_check": {
    "signature": "(box)"
  },
  "torch_npu.contrib.function.anchor_generator.npu_single_level_responsible_flags": {
    "signature": "(featmap_size, gt_bboxes, stride, num_base_anchors)"
  },
  "torch_npu.contrib.function.bbox_coder.npu_bbox_coder_decode_xywh2xyxy": {
    "signature": "(bboxes, pred_bboxes, means=None, stds=None, max_shape=None, wh_ratio_clip=0.016)"
  },
  "torch_npu.contrib.function.bbox_coder.npu_bbox_coder_encode_xyxy2xywh": {
    "signature": "(bboxes, gt_bboxes, means=None, stds=None, is_normalized=False, normalized_scale=10000.0)"
  },
  "torch_npu.contrib.function.bbox_coder.npu_bbox_coder_encode_yolo": {
    "signature": "(bboxes, gt_bboxes, stride)"
  },
  "torch_npu.contrib.function.fuse_add_softmax_dropout.fuse_add_softmax_dropout": {
    "signature": "(training, dropout, attn_mask, attn_scores, attn_head_size, p=0.5, dim=-1)"
  },
  "torch_npu.contrib.function.fused_attention.npu_fused_attention": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.fused_attention.npu_fused_attention_with_layernorm": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.index_op.npu_fast_condition_index_put": {
    "signature": "(x, condition, value)"
  },
  "torch_npu.contrib.function.iou.npu_ciou": {
    "signature": "(boxes1, boxes2, trans=True, is_cross=False, mode=0)"
  },
  "torch_npu.contrib.function.iou.npu_diou": {
    "signature": "(boxes1, boxes2, trans=True, is_cross=False, mode=0)"
  },
  "torch_npu.contrib.function.iou.npu_giou": {
    "signature": "(boxes1, boxes2, is_permuted=True)"
  },
  "torch_npu.contrib.function.iou.npu_iou": {
    "signature": "(boxes1, boxes2, mode='ptiou', is_normalized=False, normalized_scale=100.0)"
  },
  "torch_npu.contrib.function.iou.npu_ptiou": {
    "signature": "(boxes1, boxes2, mode='ptiou', is_normalized=False, normalized_scale=100.0)"
  },
  "torch_npu.contrib.function.matmul_transpose.MatmulApply": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.matmul_transpose.MatmulApply.forward": {
    "signature": "(ctx, self, mat2)"
  },
  "torch_npu.contrib.function.matmul_transpose.MatmulApply.backward": {
    "signature": "(ctx, grad)"
  },
  "torch_npu.contrib.function.matmul_transpose.matmul_transpose": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.function.nms.npu_batched_multiclass_nms": {
    "signature": "(multi_bboxes, multi_scores, score_thr=0.05, nms_thr=0.45, max_num=50, score_factors=None)"
  },
  "torch_npu.contrib.function.nms.npu_multiclass_nms": {
    "signature": "(multi_bboxes, multi_scores, score_thr=0.05, nms_thr=0.45, max_num=50, score_factors=None)"
  },
  "torch_npu.contrib.function.npu_functional.dropout_with_byte_mask": {
    "signature": "(input1, p=0.5, training=True, inplace=False)"
  },
  "torch_npu.contrib.function.roll.roll": {
    "signature": "(x, shifts, dims)"
  },
  "torch_npu.contrib.module.BiLSTM": {
    "signature": "(input_size, hidden_size)"
  },
  "torch_npu.contrib.module.BiLSTM.forward": {
    "signature": "(self, inputs)"
  },
  "torch_npu.contrib.module.ChannelShuffle": {
    "signature": "(in_channels, groups=2, split_shuffle=True)"
  },
  "torch_npu.contrib.module.ChannelShuffle.check_self": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.ChannelShuffle.forward": {
    "signature": "(self, x1, x2)"
  },
  "torch_npu.contrib.module.DCNv2": {
    "signature": "(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, bias=True, pack=True)"
  },
  "torch_npu.contrib.module.DCNv2.init_param": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.DCNv2.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.DropoutWithByteMask": {
    "signature": "(p=0.5, inplace=False, max_seed=1023)"
  },
  "torch_npu.contrib.module.DropoutWithByteMask.forward": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.Focus": {
    "signature": "(c1, c2, k=1, s=1, p=None, g=1, act=True)"
  },
  "torch_npu.contrib.module.Focus.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.FusedColorJitter": {
    "signature": "(brightness=0, contrast=0, saturation=0, hue=0)"
  },
  "torch_npu.contrib.module.FusedColorJitter._check_input": {
    "signature": "(self, value, name, center=1, bound=(0, inf), clip_first_on_zero=True)"
  },
  "torch_npu.contrib.module.FusedColorJitter.forward": {
    "signature": "(self, img)"
  },
  "torch_npu.contrib.module.LabelSmoothingCrossEntropy": {
    "signature": "(num_classes=1000, smooth_factor=0.0)"
  },
  "torch_npu.contrib.module.LabelSmoothingCrossEntropy.forward": {
    "signature": "(self, pred, target)"
  },
  "torch_npu.contrib.module.LinearA8W8Quant": {
    "signature": "(in_features: int, out_features: int, *, bias: bool = True, offset: bool = False, pertoken_scale: bool = False, device=None, dtype=None, output_dtype=None) -> None"
  },
  "torch_npu.contrib.module.LinearA8W8Quant.forward": {
    "signature": "(self, linear_quant_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.LinearWeightQuant": {
    "signature": "(in_features, out_features, bias: bool = True, device=None, dtype=None, antiquant_offset: bool = False, quant_scale: bool = False, quant_offset: bool = False, antiquant_group_size: int = 0) -> None"
  },
  "torch_npu.contrib.module.LinearWeightQuant.forward": {
    "signature": "(self, x: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.LinearQuant": {
    "signature": "(in_features: int, out_features: int, *, bias: bool = True, offset: bool = False, pertoken_scale: bool = False, device=None, dtype=None, output_dtype=None) -> None"
  },
  "torch_npu.contrib.module.LinearQuant.forward": {
    "signature": "(self, linear_quant_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.Mish": {
    "signature": "()"
  },
  "torch_npu.contrib.module.Mish.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.ModulatedDeformConv": {
    "signature": "(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, bias=True, pack=True)"
  },
  "torch_npu.contrib.module.ModulatedDeformConv.init_param": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.ModulatedDeformConv.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.MultiheadAttention": {
    "signature": "(embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8)"
  },
  "torch_npu.contrib.module.MultiheadAttention.prepare_for_onnx_export_": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.MultiheadAttention.prepare_for_tpu_": {
    "signature": "(self, **kwargs)"
  },
  "torch_npu.contrib.module.MultiheadAttention.reset_parameters": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.MultiheadAttention.transpose_for_scores": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.MultiheadAttention.forward": {
    "signature": "(self, query, key: Union[torch.Tensor, NoneType], value: Union[torch.Tensor, NoneType], bsz, tgt_len, s_len, key_padding_mask: Union[torch.Tensor, NoneType] = None, incremental_state: Union[Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], NoneType] = None, need_weights: bool = True, static_kv: bool = False, attn_mask: Union[torch.Tensor, NoneType] = None, before_softmax: bool = False, need_head_weights: bool = False) -> Tuple[torch.Tensor, Union[torch.Tensor, NoneType]]"
  },
  "torch_npu.contrib.module.MultiheadAttention.multi_attn": {
    "signature": "(self, query, key, value, key_padding_mask, bsz, tgt_len)"
  },
  "torch_npu.contrib.module.MultiheadAttention._append_prev_key_padding_mask": {
    "signature": "(key_padding_mask: Union[torch.Tensor, NoneType], prev_key_padding_mask: Union[torch.Tensor, NoneType], batch_size: int, src_len: int, static_kv: bool) -> Union[torch.Tensor, NoneType]"
  },
  "torch_npu.contrib.module.MultiheadAttention._get_input_buffer": {
    "signature": "(self, incremental_state: Union[Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], NoneType]) -> Dict[str, Union[torch.Tensor, NoneType]]"
  },
  "torch_npu.contrib.module.MultiheadAttention._set_input_buffer": {
    "signature": "(self, incremental_state: Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], buffer: Dict[str, Union[torch.Tensor, NoneType]])"
  },
  "torch_npu.contrib.module.MultiheadAttention.apply_sparse_mask": {
    "signature": "(self, attn_weights, tgt_len: int, src_len: int, bsz: int)"
  },
  "torch_npu.contrib.module.MultiheadAttention.upgrade_state_dict_named": {
    "signature": "(self, state_dict, name)"
  },
  "torch_npu.contrib.module.NpuCachedDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.module.NpuCachedDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.NpuCachedDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.NpuDropPath": {
    "signature": "(drop_prob=None)"
  },
  "torch_npu.contrib.module.NpuDropPath.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.NpuDropPath.enable_droppath_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.NpuFairseqDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.module.NpuFairseqDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.NpuFairseqDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.NpuPreGenDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.module.NpuPreGenDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.NpuPreGenDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.PSROIPool": {
    "signature": "(pooled_height=7, pooled_width=7, spatial_scale=0.0625, group_size=7, output_dim=22)"
  },
  "torch_npu.contrib.module.PSROIPool.forward": {
    "signature": "(self, features, rois)"
  },
  "torch_npu.contrib.module.Prefetcher": {
    "signature": "(loader, stream=None)"
  },
  "torch_npu.contrib.module.Prefetcher.preload": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.Prefetcher.next": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.ROIAlign": {
    "signature": "(output_size, spatial_scale, sampling_ratio, aligned=True)"
  },
  "torch_npu.contrib.module.ROIAlign.forward": {
    "signature": "(self, input_tensor, rois)"
  },
  "torch_npu.contrib.module.SiLU": {
    "signature": "()"
  },
  "torch_npu.contrib.module.SiLU.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.Swish": {
    "signature": "()"
  },
  "torch_npu.contrib.module.Swish.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.activations.Mish": {
    "signature": "()"
  },
  "torch_npu.contrib.module.activations.Mish.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.activations.SiLU": {
    "signature": "()"
  },
  "torch_npu.contrib.module.activations.SiLU.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.activations.Swish": {
    "signature": "()"
  },
  "torch_npu.contrib.module.activations.Swish.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastBatchNorm1d": {
    "signature": "(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastBatchNorm1d._check_input_dim": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastBatchNorm2d": {
    "signature": "(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastBatchNorm2d._check_input_dim": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastBatchNorm3d": {
    "signature": "(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastBatchNorm3d._check_input_dim": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastSyncBatchNorm": {
    "signature": "(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, process_group: Union[Any, NoneType] = None, device=None, dtype=None) -> None"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastSyncBatchNorm._check_input_dim": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastSyncBatchNorm._check_non_zero_input_channels": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastSyncBatchNorm.forward": {
    "signature": "(self, input1: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.batchnorm_with_int32_count.FastSyncBatchNorm.convert_sync_batchnorm": {
    "signature": "(module, process_group=None)"
  },
  "torch_npu.contrib.module.bidirectional_lstm.BiLSTM": {
    "signature": "(input_size, hidden_size)"
  },
  "torch_npu.contrib.module.bidirectional_lstm.BiLSTM.forward": {
    "signature": "(self, inputs)"
  },
  "torch_npu.contrib.module.channel_shuffle.ChannelShuffle": {
    "signature": "(in_channels, groups=2, split_shuffle=True)"
  },
  "torch_npu.contrib.module.channel_shuffle.ChannelShuffle.check_self": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.channel_shuffle.ChannelShuffle.forward": {
    "signature": "(self, x1, x2)"
  },
  "torch_npu.contrib.module.channel_shuffle.IndexSelectFullImplementation": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.module.channel_shuffle.IndexSelectFullImplementation.forward": {
    "signature": "(ctx, x1, x2, fp_index, bp_index1, bp_index2)"
  },
  "torch_npu.contrib.module.channel_shuffle.IndexSelectFullImplementation.backward": {
    "signature": "(ctx, grad_output)"
  },
  "torch_npu.contrib.module.channel_shuffle.IndexSelectHalfImplementation": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.module.channel_shuffle.IndexSelectHalfImplementation.forward": {
    "signature": "(ctx, x1, x2, fp_index1, fp_index2, bp_index1, bp_index2)"
  },
  "torch_npu.contrib.module.channel_shuffle.IndexSelectHalfImplementation.backward": {
    "signature": "(ctx, grad_output1, grad_output2)"
  },
  "torch_npu.contrib.module.channel_shuffle.indexselect_full_implementation_forward": {
    "signature": "(x1, x2, fp_index)"
  },
  "torch_npu.contrib.module.channel_shuffle.indexselect_half_implementation_forward": {
    "signature": "(x1, x2, fp_index1, fp_index2)"
  },
  "torch_npu.contrib.module.crossentropy.LabelSmoothingCrossEntropy": {
    "signature": "(num_classes=1000, smooth_factor=0.0)"
  },
  "torch_npu.contrib.module.crossentropy.LabelSmoothingCrossEntropy.forward": {
    "signature": "(self, pred, target)"
  },
  "torch_npu.contrib.module.deform_conv.DCNv2": {
    "signature": "(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, bias=True, pack=True)"
  },
  "torch_npu.contrib.module.deform_conv.DCNv2.init_param": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.deform_conv.DCNv2.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.deform_conv.ModulatedDeformConv": {
    "signature": "(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, bias=True, pack=True)"
  },
  "torch_npu.contrib.module.deform_conv.ModulatedDeformConv.init_param": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.deform_conv.ModulatedDeformConv.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.deform_conv.ModulatedDeformConv2dFunction": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.contrib.module.deform_conv.ModulatedDeformConv2dFunction.forward": {
    "signature": "(ctx, input_tensor, offset_ori, mask, weight, bias=None, with_bias=False, stride=1, padding=0, dilation=1, groups=1, deformable_groups=1, sort_index_for_npu_fp=None, sort_index_for_npu_bp=None)"
  },
  "torch_npu.contrib.module.deform_conv.ModulatedDeformConv2dFunction.backward": {
    "signature": "(ctx, grad_output)"
  },
  "torch_npu.contrib.module.drop_path.NpuDropPath": {
    "signature": "(drop_prob=None)"
  },
  "torch_npu.contrib.module.drop_path.NpuDropPath.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.drop_path.NpuDropPath.enable_droppath_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuCachedDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuCachedDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuCachedDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuFairseqDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuFairseqDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuFairseqDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuPreGenDropout": {
    "signature": "(p, module_name=None)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuPreGenDropout.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.ensemble_dropout.NpuPreGenDropout.enable_dropout_ensemble": {
    "signature": "(model)"
  },
  "torch_npu.contrib.module.focus.Conv": {
    "signature": "(c1, c2, k=1, s=1, p=None, g=1, act=True)"
  },
  "torch_npu.contrib.module.focus.Conv.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.focus.Conv.fuseforward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.focus.Focus": {
    "signature": "(c1, c2, k=1, s=1, p=None, g=1, act=True)"
  },
  "torch_npu.contrib.module.focus.Focus.forward": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.focus.autopad": {
    "signature": "(k, p=None)"
  },
  "torch_npu.contrib.module.focus.fast_slice": {
    "signature": "(x)"
  },
  "torch_npu.contrib.module.fusedcolorjitter.FusedColorJitter": {
    "signature": "(brightness=0, contrast=0, saturation=0, hue=0)"
  },
  "torch_npu.contrib.module.fusedcolorjitter.FusedColorJitter._check_input": {
    "signature": "(self, value, name, center=1, bound=(0, inf), clip_first_on_zero=True)"
  },
  "torch_npu.contrib.module.fusedcolorjitter.FusedColorJitter.forward": {
    "signature": "(self, img)"
  },
  "torch_npu.contrib.module.linear_a8w8_quant.LinearA8W8Quant": {
    "signature": "(in_features: int, out_features: int, *, bias: bool = True, offset: bool = False, pertoken_scale: bool = False, device=None, dtype=None, output_dtype=None) -> None"
  },
  "torch_npu.contrib.module.linear_a8w8_quant.LinearA8W8Quant.forward": {
    "signature": "(self, linear_quant_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.linear_weight_quant.LinearWeightQuant": {
    "signature": "(in_features, out_features, bias: bool = True, device=None, dtype=None, antiquant_offset: bool = False, quant_scale: bool = False, quant_offset: bool = False, antiquant_group_size: int = 0) -> None"
  },
  "torch_npu.contrib.module.linear_weight_quant.LinearWeightQuant.forward": {
    "signature": "(self, x: torch.Tensor) -> torch.Tensor"
  },
   "torch_npu.contrib.module.linear_quant.LinearQuant": {
    "signature": "(in_features: int, out_features: int, *, bias: bool = True, offset: bool = False, pertoken_scale: bool = False, device=None, dtype=None, output_dtype=None) -> None"
  },
  "torch_npu.contrib.module.linear_quant.LinearQuant.forward": {
    "signature": "(self, linear_quant_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.multihead_attention.Matmul_transpose": {
    "signature": "(tensor1, tensor2)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention": {
    "signature": "(embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.prepare_for_onnx_export_": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.prepare_for_tpu_": {
    "signature": "(self, **kwargs)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.reset_parameters": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.transpose_for_scores": {
    "signature": "(self, x)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.forward": {
    "signature": "(self, query, key: Union[torch.Tensor, NoneType], value: Union[torch.Tensor, NoneType], bsz, tgt_len, s_len, key_padding_mask: Union[torch.Tensor, NoneType] = None, incremental_state: Union[Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], NoneType] = None, need_weights: bool = True, static_kv: bool = False, attn_mask: Union[torch.Tensor, NoneType] = None, before_softmax: bool = False, need_head_weights: bool = False) -> Tuple[torch.Tensor, Union[torch.Tensor, NoneType]]"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.multi_attn": {
    "signature": "(self, query, key, value, key_padding_mask, bsz, tgt_len)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention._append_prev_key_padding_mask": {
    "signature": "(key_padding_mask: Union[torch.Tensor, NoneType], prev_key_padding_mask: Union[torch.Tensor, NoneType], batch_size: int, src_len: int, static_kv: bool) -> Union[torch.Tensor, NoneType]"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention._get_input_buffer": {
    "signature": "(self, incremental_state: Union[Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], NoneType]) -> Dict[str, Union[torch.Tensor, NoneType]]"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention._set_input_buffer": {
    "signature": "(self, incremental_state: Dict[str, Dict[str, Union[torch.Tensor, NoneType]]], buffer: Dict[str, Union[torch.Tensor, NoneType]])"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.apply_sparse_mask": {
    "signature": "(self, attn_weights, tgt_len: int, src_len: int, bsz: int)"
  },
  "torch_npu.contrib.module.multihead_attention.MultiheadAttention.upgrade_state_dict_named": {
    "signature": "(self, state_dict, name)"
  },
  "torch_npu.contrib.module.npu_modules.DropoutWithByteMask": {
    "signature": "(p=0.5, inplace=False, max_seed=1023)"
  },
  "torch_npu.contrib.module.npu_modules.DropoutWithByteMask.forward": {
    "signature": "(self, input1)"
  },
  "torch_npu.contrib.module.prefetcher.Prefetcher": {
    "signature": "(loader, stream=None)"
  },
  "torch_npu.contrib.module.prefetcher.Prefetcher.preload": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.prefetcher.Prefetcher.next": {
    "signature": "(self)"
  },
  "torch_npu.contrib.module.ps_roi_pooling.PSROIPool": {
    "signature": "(pooled_height=7, pooled_width=7, spatial_scale=0.0625, group_size=7, output_dim=22)"
  },
  "torch_npu.contrib.module.ps_roi_pooling.PSROIPool.forward": {
    "signature": "(self, features, rois)"
  },
  "torch_npu.contrib.module.quant_conv2d.QuantConv2d": {
    "signature": "(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], output_dtype: torch.dtype, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, offset: bool = False, offset_x: int = 0, round_mode: str = 'rint', device=None, dtype=None) -> None"
  },
  "torch_npu.contrib.module.quant_conv2d.QuantConv2d.forward": {
    "signature": "(self, quant_conv2d_input: torch.Tensor) -> torch.Tensor"
  },
  "torch_npu.contrib.module.roi_align.ROIAlign": {
    "signature": "(output_size, spatial_scale, sampling_ratio, aligned=True)"
  },
  "torch_npu.contrib.module.roi_align.ROIAlign.forward": {
    "signature": "(self, input_tensor, rois)"
  },
  "torch_npu.distributed.batch_isend_irecv": {
    "signature": "(p2p_op_list)"
  },
  "torch_npu.distributed.gather": {
    "signature": "(tensor, gather_list=None, dst=0, group=None, async_op=False)"
  },
  "torch_npu.distributed.gather_object": {
    "signature": "(obj, object_gather_list=None, dst=0, group=None)"
  },
  "torch_npu.distributed.is_available": {
    "signature": "()"
  },
  "torch_npu.distributed.is_hccl_available": {
    "signature": "()"
  },
  "torch_npu.distributed.distributed_c10d.batch_isend_irecv": {
    "signature": "(p2p_op_list)"
  },
  "torch_npu.distributed.distributed_c10d.gather": {
    "signature": "(tensor, gather_list=None, dst=0, group=None, async_op=False)"
  },
  "torch_npu.distributed.distributed_c10d.gather_object": {
    "signature": "(obj, object_gather_list=None, dst=0, group=None)"
  },
  "torch_npu.distributed.distributed_c10d.is_hccl_available": {
    "signature": "()"
  },
  "torch_npu.distributed.distributed_c10d.reinit_process_group": {
    "signature": "(group=None, rebuild_link=True)"
  },
  "torch_npu.distributed.reinit_process_group": {
    "signature": "(group=None, rebuild_link=True)"
  },
  "torch_npu.distributed.rpc.backend_registry.construct_rpc_backend_options": {
    "signature": "(backend, rpc_timeout=60.0, init_method='env://', **kwargs)"
  },
  "torch_npu.distributed.rpc.backend_registry.init_backend": {
    "signature": "(backend, *args, **kwargs)"
  },
  "torch_npu.distributed.rpc.options.NPUTensorPipeRpcBackendOptions": {
    "signature": "(*, num_worker_threads: int = 16, rpc_timeout: float = 60.0, init_method: str = 'env://', device_maps: Union[Dict[str, Dict[Union[int, str, torch.device], Union[int, str, torch.device]]], NoneType] = None, devices: Union[List[Union[int, str, torch.device]], NoneType] = None, _transports: Union[List, NoneType] = None, _channels: Union[List, NoneType] = None)"
  },
  "torch_npu.distributed.rpc.options.NPUTensorPipeRpcBackendOptions.set_device_map": {
    "signature": "(self, to: str, device_map: Dict[Union[int, str, torch.device], Union[int, str, torch.device]])"
  },
  "torch_npu.distributed.rpc.options.NPUTensorPipeRpcBackendOptions.set_devices": {
    "signature": "(self, devices: List[Union[int, str, torch.device]])"
  },
  "torch_npu.dynamo.torchair.CompilerConfig": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.dynamo_export": {
    "signature": "(*args, model: torch.nn.modules.module.Module, export_path: str = 'export_file', export_name: str = 'export', dynamic: bool = False, config=<torchair.configs.compiler_config.CompilerConfig object>, **kwargs)"
  },
  "torch_npu.dynamo.torchair.get_compiler": {
    "signature": "(compiler_config: torchair.configs.compiler_config.CompilerConfig = None)"
  },
  "torch_npu.dynamo.torchair.get_npu_backend": {
    "signature": "(*, compiler_config: torchair.configs.compiler_config.CompilerConfig = None, custom_decompositions: Dict = {})"
  },
  "torch_npu.dynamo.torchair.ops.npu_print": {
    "signature": "(*args, summarize_size=3)"
  },
  "torch_npu.dynamo.torchair.patch_for_hcom": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.register_fx_node_ge_converter": {
    "signature": "(aten_op)"
  },
  "torch_npu.dynamo.torchair.use_internal_format_weight": {
    "signature": "(model: torch.nn.modules.module.Module)"
  },
  "torch_npu.dynamo.torchair.configs.compiler_config.CompilerConfig": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.experimental.inference.use_internal_format_weight": {
    "signature": "(model: torch.nn.modules.module.Module)"
  },
  "torch_npu.dynamo.torchair.ge.Cast": {
    "signature": "(x: torchair.ge.TensorBase, *, dst_type: int, dependencies=[], node_name=None)"
  },
  "torch_npu.dynamo.torchair.ge.Const": {
    "signature": "(v, dtype: int = None, node_name=None, readable=True)"
  },
  "torch_npu.dynamo.torchair.ge.DataType": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.Format": {
    "signature": "(value, names=None, *, module=None, qualname=None, type=None, start=1)"
  },
  "torch_npu.dynamo.torchair.ge.Format._generate_next_value_": {
    "signature": "(name, start, count, last_values)"
  },
  "torch_npu.dynamo.torchair.ge.Format._member_type_": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.Tensor": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.Tensor.index": {
    "signature": "(self)"
  },
  "torch_npu.dynamo.torchair.ge.Tensor.dtype": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.Tensor.rank": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.TensorSpec": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.TensorSpec.dtype": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.TensorSpec.rank": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.TensorSpec.size": {
    "signature": "()"
  },
  "torch_npu.dynamo.torchair.ge.custom_op": {
    "signature": "(op_type: str, *, inputs: Union[Dict[str, Union[ForwardRef('Tensor'), List[ForwardRef('Tensor')], NoneType]], NoneType], outputs: Union[List[Union[str, Tuple[str, int]]], NoneType], attrs: Union[Dict[str, ForwardRef('_Attr')], NoneType] = None, node_name: Union[str, NoneType] = None)"
  },
  "torch_npu.dynamo.torchair.ge.ge_custom.custom_op": {
    "signature": "(op_type: str, *, inputs: Union[Dict[str, Union[ForwardRef('Tensor'), List[ForwardRef('Tensor')], NoneType]], NoneType], outputs: Union[List[Union[str, Tuple[str, int]]], NoneType], attrs: Union[Dict[str, ForwardRef('_Attr')], NoneType] = None, node_name: Union[str, NoneType] = None)"
  },
  "torch_npu.dynamo.torchair.inference.cache_compile": {
    "signature": "(func, *, config: Union[torchair.configs.compiler_config.CompilerConfig, NoneType] = None, dynamic: bool = True, cache_dir: Union[str, NoneType] = None, global_rank: Union[int, NoneType] = None, tp_rank: Union[int, NoneType] = None, pp_rank: Union[int, NoneType] = None, **kwargs) -> Callable"
  },
  "torch_npu.dynamo.torchair.inference.readable_cache": {
    "signature": "(cache_bin, print_output=True, file=None)"
  },
  "torch_npu.dynamo.torchair.inference.set_dim_gears": {
    "signature": "(t: torch.Tensor, dim_gears: Dict[int, List[int]])"
  },
  "torch_npu.dynamo.torchair.llm_datadist.create_npu_tensors": {
    "signature": "(shape: List[int], dtype: torch.dtype, addresses: List[int]) -> List[torch.Tensor]"
  },
  "torch_npu.dynamo.torchair.npu_export.dynamo_export": {
    "signature": "(*args, model: torch.nn.modules.module.Module, export_path: str = 'export_file', export_name: str = 'export', dynamic: bool = False, config=<torchair.configs.compiler_config.CompilerConfig object>, **kwargs)"
  },
  "torch_npu.dynamo.torchair.npu_fx_compiler.get_compiler": {
    "signature": "(compiler_config: torchair.configs.compiler_config.CompilerConfig = None)"
  },
  "torch_npu.dynamo.torchair.npu_fx_compiler.get_npu_backend": {
    "signature": "(*, compiler_config: torchair.configs.compiler_config.CompilerConfig = None, custom_decompositions: Dict = {})"
  },
  "torch_npu.jit.optimize": {
    "signature": "(jit_mod)"
  },
  "torch_npu.jit.fusion_pass.fast_gelu.fast_gelu_pass": {
    "signature": "(jit_mod)"
  },
  "torch_npu.jit.register_fusion_pattern.optimize": {
    "signature": "(jit_mod)"
  },
  "torch_npu.npu.BFloat16Storage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.BoolStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.ByteStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.CharStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.DoubleStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.Event": {
    "signature": "(enable_timing=False, blocking=False, interprocess=False)"
  },
  "torch_npu.npu.Event.record": {
    "signature": "(self, stream=None)"
  },
  "torch_npu.npu.Event.wait": {
    "signature": "(self, stream=None)"
  },
  "torch_npu.npu.Event.query": {
    "signature": "(self)"
  },
  "torch_npu.npu.Event.elapsed_time": {
    "signature": "(self, end_event)"
  },
  "torch_npu.npu.Event.synchronize": {
    "signature": "(self)"
  },
  "torch_npu.npu.FloatStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.HalfStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.IntStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.LongStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.NPUPluggableAllocator": {
    "signature": "(path_to_so_file: str, alloc_fn_name: str, free_fn_name: str)"
  },
  "torch_npu.npu.ShortStorage": {
    "signature": "(*args, wrap_storage=None, dtype=None, device=None, _internal=False)"
  },
  "torch_npu.npu.Stream": {
    "signature": "(device=None, priority=0, **kwargs)"
  },
  "torch_npu.npu.Stream.wait_event": {
    "signature": "(self, event)"
  },
  "torch_npu.npu.Stream.wait_stream": {
    "signature": "(self, stream)"
  },
  "torch_npu.npu.Stream.record_event": {
    "signature": "(self, event=None)"
  },
  "torch_npu.npu.Stream.query": {
    "signature": "(self)"
  },
  "torch_npu.npu.Stream.synchronize": {
    "signature": "(self)"
  },
  "torch_npu.npu.Stream.set_data_preprocess_stream": {
    "signature": "(self, is_data_preprocess_stream=False)"
  },
  "torch_npu.npu.SyncLaunchStream": {
    "signature": "(device=None, priority=0, **kwargs)"
  },
  "torch_npu.npu.SyncLaunchStream.wait_event": {
    "signature": "(self, event)"
  },
  "torch_npu.npu.SyncLaunchStream.wait_stream": {
    "signature": "(self, stream)"
  },
  "torch_npu.npu.SyncLaunchStream.record_event": {
    "signature": "(self, event=None)"
  },
  "torch_npu.npu.SyncLaunchStream.query": {
    "signature": "(self)"
  },
  "torch_npu.npu.SyncLaunchStream.synchronize": {
    "signature": "(self)"
  },
  "torch_npu.npu.SyncLaunchStream.set_data_preprocess_stream": {
    "signature": "(self, is_data_preprocess_stream=False)"
  },
  "torch_npu.npu.caching_allocator_alloc": {
    "signature": "(size, device=None, stream=None)"
  },
  "torch_npu.npu.caching_allocator_delete": {
    "signature": "(mem_ptr)"
  },
  "torch_npu.npu.can_device_access_peer": {
    "signature": "(device_id, peer_device_id)"
  },
  "torch_npu.npu.change_current_allocator": {
    "signature": "(allocator: torch_npu.npu.memory._NPUAllocator) -> None"
  },
  "torch_npu.npu.check_uce_in_memory": {
    "signature": "(device_id)"
  },
  "torch_npu.npu.clear_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.current_blas_handle": {
    "signature": "()"
  },
  "torch_npu.npu.current_device": {
    "signature": "()"
  },
  "torch_npu.npu.current_stream": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.default_stream": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.device": {
    "signature": "(device)"
  },
  "torch_npu.npu.device_count": {
    "signature": "()"
  },
  "torch_npu.npu.device_of": {
    "signature": "(obj)"
  },
  "torch_npu.npu.disable_deterministic_with_backward": {
    "signature": "(tensor: torch.Tensor)"
  },
  "torch_npu.npu.empty_cache": {
    "signature": "()"
  },
  "torch_npu.npu.enable_deterministic_with_backward": {
    "signature": "(tensor: torch.Tensor)"
  },
  "torch_npu.npu.finalize_dump": {
    "signature": "()"
  },
  "torch_npu.npu.get_allocator_backend": {
    "signature": "() -> str"
  },
  "torch_npu.npu.get_amp_supported_dtype": {
    "signature": "()"
  },
  "torch_npu.npu.get_autocast_dtype": {
    "signature": "()"
  },
  "torch_npu.npu.get_device_capability": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.get_device_name": {
    "signature": "(device_name=None)"
  },
  "torch_npu.npu.get_device_properties": {
    "signature": "(device_name=None)"
  },
  "torch_npu.npu.get_mm_bmm_format_nd": {
    "signature": "()"
  },
  "torch_npu.npu.get_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.get_rng_state": {
    "signature": "(device: Union[int, str, torch.device] = 'npu') -> torch.Tensor"
  },
  "torch_npu.npu.get_rng_state_all": {
    "signature": "()"
  },
  "torch_npu.npu.get_sync_debug_mode": {
    "signature": "()"
  },
  "torch_npu.npu.init": {
    "signature": "()"
  },
  "torch_npu.npu.init_dump": {
    "signature": "()"
  },
  "torch_npu.npu.initial_seed": {
    "signature": "()"
  },
  "torch_npu.npu.is_autocast_enabled": {
    "signature": "()"
  },
  "torch_npu.npu.is_available": {
    "signature": "()"
  },
  "torch_npu.npu.is_bf16_supported": {
    "signature": "()"
  },
  "torch_npu.npu.is_initialized": {
    "signature": "()"
  },
  "torch_npu.npu.is_jit_compile_false": {
    "signature": "() -> bool"
  },
  "torch_npu.npu.manual_seed": {
    "signature": "(seed)"
  },
  "torch_npu.npu.manual_seed_all": {
    "signature": "(seed)"
  },
  "torch_npu.npu.max_memory_allocated": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.max_memory_cached": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.max_memory_reserved": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.mem_get_info": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory_allocated": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory_cached": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory_reserved": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory_snapshot": {
    "signature": "()"
  },
  "torch_npu.npu.memory_stats": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory_stats_as_nested_dict": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory_summary": {
    "signature": "(device=None, abbreviated=False)"
  },
  "torch_npu.npu.mstx": {
    "signature": "()"
  },
  "torch_npu.npu.mstx.mark": {
    "signature": "(message: str = '')"
  },
  "torch_npu.npu.mstx.range_start": {
    "signature": "(message: str, stream=None) -> int"
  },
  "torch_npu.npu.mstx.range_end": {
    "signature": "(range_id: int)"
  },
  "torch_npu.npu.mstx.mstx_range": {
    "signature": "(message: str, stream=None)"
  },
  "torch_npu.npu.reset_accumulated_memory_stats": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.reset_max_memory_allocated": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.reset_max_memory_cached": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.reset_peak_memory_stats": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.restart_device": {
    "signature": "(device_id: int, rebuild_all_resources: int = False)"
  },
  "torch_npu.npu.stress_detect": {
    "signature": "()"
  },
  "torch_npu.npu.stop_device": {
    "signature": "(device_id)"
  },
  "torch_npu.npu.seed": {
    "signature": "()"
  },
  "torch_npu.npu.seed_all": {
    "signature": "()"
  },
  "torch_npu.npu.set_aoe": {
    "signature": "(dump_path)"
  },
  "torch_npu.npu.set_autocast_dtype": {
    "signature": "(dtype)"
  },
  "torch_npu.npu.set_autocast_enabled": {
    "signature": "(enable)"
  },
  "torch_npu.npu.set_compile_mode": {
    "signature": "(jit_compile=False)"
  },
  "torch_npu.npu.set_device": {
    "signature": "(device)"
  },
  "torch_npu.npu.set_dump": {
    "signature": "(cfg_file)"
  },
  "torch_npu.npu.set_mm_bmm_format_nd": {
    "signature": "(is_nd=True)"
  },
  "torch_npu.npu.set_option": {
    "signature": "(option)"
  },
  "torch_npu.npu.set_per_process_memory_fraction": {
    "signature": "(fraction, device=None) -> None"
  },
  "torch_npu.npu.set_rng_state": {
    "signature": "(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'npu') -> None"
  },
  "torch_npu.npu.set_rng_state_all": {
    "signature": "(new_states)"
  },
  "torch_npu.npu.set_stream": {
    "signature": "(stream)"
  },
  "torch_npu.npu.set_sync_debug_mode": {
    "signature": "(debug_mode)"
  },
  "torch_npu.npu.stream": {
    "signature": "(stream)"
  },
  "torch_npu.npu.synchronize": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.utilization": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.aclnn.backends.version": {
    "signature": "()"
  },
  "torch_npu.npu.amp.GradScaler": {
    "signature": "(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, dynamic=True, enabled=True)"
  },
  "torch_npu.npu.amp.GradScaler._lazy_init_scale_growth_tracker": {
    "signature": "(self, dev)"
  },
  "torch_npu.npu.amp.GradScaler._lazy_init_dist_flag_and_dist_overflow_count": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.GradScaler.scale": {
    "signature": "(self, outputs)"
  },
  "torch_npu.npu.amp.GradScaler._unscale_grads_": {
    "signature": "(self, optimizer, inv_scale, found_inf, allow_fp16)"
  },
  "torch_npu.npu.amp.GradScaler.unscale_": {
    "signature": "(self, optimizer)"
  },
  "torch_npu.npu.amp.GradScaler._maybe_opt_step": {
    "signature": "(self, optimizer, optimizer_state, *args, **kwargs)"
  },
  "torch_npu.npu.amp.GradScaler.step": {
    "signature": "(self, optimizer, *args, **kwargs)"
  },
  "torch_npu.npu.amp.GradScaler.update": {
    "signature": "(self, new_scale=None)"
  },
  "torch_npu.npu.amp.GradScaler.state_dict": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.GradScaler.load_state_dict": {
    "signature": "(self, state_dict)"
  },
  "torch_npu.npu.amp.GradScaler.get_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.amp.GradScaler.clear_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.amp.GradScaler._sync_dist_overflow_count": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.GradScaler._npu_update_scale": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.ShardedGradScaler": {
    "signature": "(init_scale: float = 65536.0, backoff_factor: float = 0.5, growth_factor: float = 2.0, growth_interval: int = 2000, enabled: bool = True, process_group: Union[torch.distributed.distributed_c10d.ProcessGroup, NoneType] = None)"
  },
  "torch_npu.npu.amp.ShardedGradScaler.scale": {
    "signature": "(self, outputs: Union[torch.Tensor, List[torch.Tensor]]) -> Union[torch.Tensor, List[torch.Tensor]]"
  },
  "torch_npu.npu.amp.ShardedGradScaler._foreach_non_finite_check_and_unscale_cpu_": {
    "signature": "(self, grads: List, found_inf: torch.Tensor, inv_scale: torch.Tensor) -> None"
  },
  "torch_npu.npu.amp.ShardedGradScaler._unscale_grads_": {
    "signature": "(self, optimizer: torch.optim.sgd.SGD, inv_scale: torch.Tensor, found_inf: torch.Tensor, allow_fp16: bool = True) -> Dict[torch.device, torch.Tensor]"
  },
  "torch_npu.npu.amp.ShardedGradScaler.unscale_": {
    "signature": "(self, optimizer: torch.optim.sgd.SGD) -> None"
  },
  "torch_npu.npu.amp.ShardedGradScaler.step": {
    "signature": "(self, optimizer: torch.optim.sgd.SGD, *args, **kwargs) -> Union[float, NoneType]"
  },
  "torch_npu.npu.amp.ShardedGradScaler._amp_update_scale_cpu_": {
    "signature": "(self, found_inf) -> None"
  },
  "torch_npu.npu.amp.ShardedGradScaler._amp_update_scale_npu_": {
    "signature": "(self, found_inf) -> None"
  },
  "torch_npu.npu.amp.ShardedGradScaler.update": {
    "signature": "(self, new_scale=None) -> None"
  },
  "torch_npu.npu.amp.autocast": {
    "signature": "(enabled: bool = True, dtype: torch.dtype = torch.float16, cache_enabled: bool = True)"
  },
  "torch_npu.npu.amp.custom_bwd": {
    "signature": "(bwd)"
  },
  "torch_npu.npu.amp.custom_fwd": {
    "signature": "(fwd=None, **kwargs)"
  },
  "torch_npu.npu.amp.autocast_mode.autocast": {
    "signature": "(enabled: bool = True, dtype: torch.dtype = torch.float16, cache_enabled: bool = True)"
  },
  "torch_npu.npu.amp.autocast_mode.custom_bwd": {
    "signature": "(bwd)"
  },
  "torch_npu.npu.amp.autocast_mode.custom_fwd": {
    "signature": "(fwd=None, **kwargs)"
  },
  "torch_npu.npu.amp.common.amp_definitely_not_available": {
    "signature": "()"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler": {
    "signature": "(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, dynamic=True, enabled=True)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler._lazy_init_scale_growth_tracker": {
    "signature": "(self, dev)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler._lazy_init_dist_flag_and_dist_overflow_count": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.scale": {
    "signature": "(self, outputs)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler._unscale_grads_": {
    "signature": "(self, optimizer, inv_scale, found_inf, allow_fp16)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.unscale_": {
    "signature": "(self, optimizer)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler._maybe_opt_step": {
    "signature": "(self, optimizer, optimizer_state, *args, **kwargs)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.step": {
    "signature": "(self, optimizer, *args, **kwargs)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.update": {
    "signature": "(self, new_scale=None)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.state_dict": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.load_state_dict": {
    "signature": "(self, state_dict)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.get_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler.clear_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler._sync_dist_overflow_count": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.grad_scaler.GradScaler._npu_update_scale": {
    "signature": "(self)"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler": {
    "signature": "(init_scale: float = 65536.0, backoff_factor: float = 0.5, growth_factor: float = 2.0, growth_interval: int = 2000, enabled: bool = True, process_group: Union[torch.distributed.distributed_c10d.ProcessGroup, NoneType] = None)"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler.scale": {
    "signature": "(self, outputs: Union[torch.Tensor, List[torch.Tensor]]) -> Union[torch.Tensor, List[torch.Tensor]]"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler._foreach_non_finite_check_and_unscale_cpu_": {
    "signature": "(self, grads: List, found_inf: torch.Tensor, inv_scale: torch.Tensor) -> None"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler._unscale_grads_": {
    "signature": "(self, optimizer: torch.optim.sgd.SGD, inv_scale: torch.Tensor, found_inf: torch.Tensor, allow_fp16: bool = True) -> Dict[torch.device, torch.Tensor]"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler.unscale_": {
    "signature": "(self, optimizer: torch.optim.sgd.SGD) -> None"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler.step": {
    "signature": "(self, optimizer: torch.optim.sgd.SGD, *args, **kwargs) -> Union[float, NoneType]"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler._amp_update_scale_cpu_": {
    "signature": "(self, found_inf) -> None"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler._amp_update_scale_npu_": {
    "signature": "(self, found_inf) -> None"
  },
  "torch_npu.npu.amp.sharded_grad_scaler.ShardedGradScaler.update": {
    "signature": "(self, new_scale=None) -> None"
  },
  "torch_npu.npu.autocast_utils.get_amp_supported_dtype": {
    "signature": "()"
  },
  "torch_npu.npu.autocast_utils.get_autocast_dtype": {
    "signature": "()"
  },
  "torch_npu.npu.autocast_utils.is_autocast_enabled": {
    "signature": "()"
  },
  "torch_npu.npu.autocast_utils.set_autocast_dtype": {
    "signature": "(dtype)"
  },
  "torch_npu.npu.autocast_utils.set_autocast_enabled": {
    "signature": "(enable)"
  },
  "torch_npu.npu.deterministic.disable_deterministic_with_backward": {
    "signature": "(tensor: torch.Tensor)"
  },
  "torch_npu.npu.deterministic.enable_deterministic_with_backward": {
    "signature": "(tensor: torch.Tensor)"
  },
  "torch_npu.npu.memory.NPUPluggableAllocator": {
    "signature": "(path_to_so_file: str, alloc_fn_name: str, free_fn_name: str)"
  },
  "torch_npu.npu.memory.caching_allocator_alloc": {
    "signature": "(size, device=None, stream=None)"
  },
  "torch_npu.npu.memory.caching_allocator_delete": {
    "signature": "(mem_ptr)"
  },
  "torch_npu.npu.memory.change_current_allocator": {
    "signature": "(allocator: torch_npu.npu.memory._NPUAllocator) -> None"
  },
  "torch_npu.npu.memory.empty_cache": {
    "signature": "()"
  },
  "torch_npu.npu.memory.get_allocator_backend": {
    "signature": "() -> str"
  },
  "torch_npu.npu.memory.max_memory_allocated": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.max_memory_cached": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.max_memory_reserved": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.memory_allocated": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.memory_cached": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.memory_reserved": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.memory_snapshot": {
    "signature": "()"
  },
  "torch_npu.npu.memory.memory_stats": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.memory_stats_as_nested_dict": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.memory_summary": {
    "signature": "(device=None, abbreviated=False)"
  },
  "torch_npu.npu.memory.reset_accumulated_memory_stats": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.reset_max_memory_allocated": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.reset_max_memory_cached": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.reset_peak_memory_stats": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.memory.set_per_process_memory_fraction": {
    "signature": "(fraction, device=None) -> None"
  },
  "torch_npu.npu.mstx.mstx": {
    "signature": "()"
  },
  "torch_npu.npu.mstx.mstx.mark": {
    "signature": "(message: str = '')"
  },
  "torch_npu.npu.mstx.mstx.range_start": {
    "signature": "(message: str, stream=None) -> int"
  },
  "torch_npu.npu.mstx.mstx.range_end": {
    "signature": "(range_id: int)"
  },
  "torch_npu.npu.mstx.mstx.mstx_range": {
    "signature": "(message: str, stream=None)"
  },
  "torch_npu.npu.npu_config.finalize_dump": {
    "signature": "()"
  },
  "torch_npu.npu.npu_config.get_mm_bmm_format_nd": {
    "signature": "()"
  },
  "torch_npu.npu.npu_config.init_dump": {
    "signature": "()"
  },
  "torch_npu.npu.npu_config.is_jit_compile_false": {
    "signature": "() -> bool"
  },
  "torch_npu.npu.npu_config.set_aoe": {
    "signature": "(dump_path)"
  },
  "torch_npu.npu.npu_config.set_compile_mode": {
    "signature": "(jit_compile=False)"
  },
  "torch_npu.npu.npu_config.set_dump": {
    "signature": "(cfg_file)"
  },
  "torch_npu.npu.npu_config.set_mm_bmm_format_nd": {
    "signature": "(is_nd=True)"
  },
  "torch_npu.npu.npu_config.set_option": {
    "signature": "(option)"
  },
  "torch_npu.npu.random.get_rng_state": {
    "signature": "(device: Union[int, str, torch.device] = 'npu') -> torch.Tensor"
  },
  "torch_npu.npu.random.get_rng_state_all": {
    "signature": "()"
  },
  "torch_npu.npu.random.initial_seed": {
    "signature": "()"
  },
  "torch_npu.npu.random.manual_seed": {
    "signature": "(seed)"
  },
  "torch_npu.npu.random.manual_seed_all": {
    "signature": "(seed)"
  },
  "torch_npu.npu.random.seed": {
    "signature": "()"
  },
  "torch_npu.npu.random.seed_all": {
    "signature": "()"
  },
  "torch_npu.npu.random.set_rng_state": {
    "signature": "(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'npu') -> None"
  },
  "torch_npu.npu.random.set_rng_state_all": {
    "signature": "(new_states)"
  },
  "torch_npu.npu.streams.Event": {
    "signature": "(enable_timing=False, blocking=False, interprocess=False)"
  },
  "torch_npu.npu.streams.Event.record": {
    "signature": "(self, stream=None)"
  },
  "torch_npu.npu.streams.Event.wait": {
    "signature": "(self, stream=None)"
  },
  "torch_npu.npu.streams.Event.query": {
    "signature": "(self)"
  },
  "torch_npu.npu.streams.Event.elapsed_time": {
    "signature": "(self, end_event)"
  },
  "torch_npu.npu.streams.Event.synchronize": {
    "signature": "(self)"
  },
  "torch_npu.npu.streams.Stream": {
    "signature": "(device=None, priority=0, **kwargs)"
  },
  "torch_npu.npu.streams.Stream.wait_event": {
    "signature": "(self, event)"
  },
  "torch_npu.npu.streams.Stream.wait_stream": {
    "signature": "(self, stream)"
  },
  "torch_npu.npu.streams.Stream.record_event": {
    "signature": "(self, event=None)"
  },
  "torch_npu.npu.streams.Stream.query": {
    "signature": "(self)"
  },
  "torch_npu.npu.streams.Stream.synchronize": {
    "signature": "(self)"
  },
  "torch_npu.npu.streams.Stream.set_data_preprocess_stream": {
    "signature": "(self, is_data_preprocess_stream=False)"
  },
  "torch_npu.npu.streams.SyncLaunchStream": {
    "signature": "(device=None, priority=0, **kwargs)"
  },
  "torch_npu.npu.streams.SyncLaunchStream.wait_event": {
    "signature": "(self, event)"
  },
  "torch_npu.npu.streams.SyncLaunchStream.wait_stream": {
    "signature": "(self, stream)"
  },
  "torch_npu.npu.streams.SyncLaunchStream.record_event": {
    "signature": "(self, event=None)"
  },
  "torch_npu.npu.streams.SyncLaunchStream.query": {
    "signature": "(self)"
  },
  "torch_npu.npu.streams.SyncLaunchStream.synchronize": {
    "signature": "(self)"
  },
  "torch_npu.npu.streams.SyncLaunchStream.set_data_preprocess_stream": {
    "signature": "(self, is_data_preprocess_stream=False)"
  },
  "torch_npu.npu.utils.can_device_access_peer": {
    "signature": "(device_id, peer_device_id)"
  },
  "torch_npu.npu.utils.check_uce_in_memory": {
    "signature": "(device_id)"
  },
  "torch_npu.npu.utils.clear_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.utils.current_blas_handle": {
    "signature": "()"
  },
  "torch_npu.npu.utils.current_device": {
    "signature": "()"
  },
  "torch_npu.npu.utils.current_stream": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.utils.default_stream": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.utils.device": {
    "signature": "(device)"
  },
  "torch_npu.npu.utils.device_count": {
    "signature": "()"
  },
  "torch_npu.npu.utils.device_of": {
    "signature": "(obj)"
  },
  "torch_npu.npu.utils.finalize_dump": {
    "signature": "()"
  },
  "torch_npu.npu.utils.get_device_capability": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.utils.get_device_name": {
    "signature": "(device_name=None)"
  },
  "torch_npu.npu.utils.get_device_properties": {
    "signature": "(device_name=None)"
  },
  "torch_npu.npu.utils.get_npu_overflow_flag": {
    "signature": "()"
  },
  "torch_npu.npu.utils.get_sync_debug_mode": {
    "signature": "()"
  },
  "torch_npu.npu.utils.init_dump": {
    "signature": "()"
  },
  "torch_npu.npu.utils.is_bf16_supported": {
    "signature": "()"
  },
  "torch_npu.npu.utils.is_support_inf_nan": {
    "signature": "()"
  },
  "torch_npu.npu.utils.mem_get_info": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.utils.npu_check_overflow": {
    "signature": "(grad)"
  },
  "torch_npu.npu.utils.set_device": {
    "signature": "(device)"
  },
  "torch_npu.npu.utils.set_dump": {
    "signature": "(cfg_file)"
  },
  "torch_npu.npu.utils.set_stream": {
    "signature": "(stream)"
  },
  "torch_npu.npu.utils.set_sync_debug_mode": {
    "signature": "(debug_mode)"
  },
  "torch_npu.npu.utils.stream": {
    "signature": "(stream)"
  },
  "torch_npu.npu.utils.stress_detect": {
    "signature": "()"
  },
  "torch_npu.npu.utils.synchronize": {
    "signature": "(device=None)"
  },
  "torch_npu.npu.utils.utilization": {
    "signature": "(device=None)"
  },
  "torch_npu.optim.NpuFusedAdadelta": {
    "signature": "(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)"
  },
  "torch_npu.optim.NpuFusedAdadelta._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.NpuFusedAdadelta._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdadelta._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedAdadelta._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdam": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
  },
  "torch_npu.optim.NpuFusedAdam._init_param_state": {
    "signature": "(self, p, amsgrad)"
  },
  "torch_npu.optim.NpuFusedAdam._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdam._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedAdam._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdamP": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False)"
  },
  "torch_npu.optim.NpuFusedAdamP._channel_view": {
    "signature": "(self, x)"
  },
  "torch_npu.optim.NpuFusedAdamP._layer_view": {
    "signature": "(self, x)"
  },
  "torch_npu.optim.NpuFusedAdamP._cosine_similarity": {
    "signature": "(self, x, y, eps, view_func)"
  },
  "torch_npu.optim.NpuFusedAdamP._projection": {
    "signature": "(self, p, grad, perturb, delta, wd_ratio, eps)"
  },
  "torch_npu.optim.NpuFusedAdamP._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.NpuFusedAdamP._combine_middle_vars": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdamP._combine_middle_vars_by_group": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedAdamP._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdamP._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedAdamP._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdamP.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.NpuFusedAdamW": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)"
  },
  "torch_npu.optim.NpuFusedAdamW._init_param_state": {
    "signature": "(self, p, amsgrad)"
  },
  "torch_npu.optim.NpuFusedAdamW._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedAdamW._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedAdamW._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedBertAdam": {
    "signature": "(params, lr=<required parameter>, warmup=-1, t_total=-1, schedule='warmup_linear', b1=0.9, b2=0.999, e=1e-06, weight_decay=0.01, max_grad_norm=1.0)"
  },
  "torch_npu.optim.NpuFusedBertAdam._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.NpuFusedBertAdam._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedBertAdam._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedBertAdam._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedLamb": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-06, weight_decay=0, adam=False, use_global_grad_norm=False)"
  },
  "torch_npu.optim.NpuFusedLamb._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.NpuFusedLamb._combine_middle_vars": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedLamb._combine_middle_vars_by_group": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedLamb._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedLamb._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedLamb._get_global_grad_norm": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedLamb._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedLamb.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase": {
    "signature": "(params, default)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._maybe_init_combined_params_and_grads": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase.zero_grad": {
    "signature": "(self, set_to_none=False)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase.get_combined_params": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase.get_combined_grads": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._clip_grad_norm_fused_": {
    "signature": "(self, combined_grads, combined_grads_masks, max_norm, norm_type)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._combine_grads_mask": {
    "signature": "(self, list_of_params)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._maybe_init_combined_grads_masks": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase._get_combined_grad_masks": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedOptimizerBase.clip_grad_norm_fused_": {
    "signature": "(self, max_norm, norm_type=2)"
  },
  "torch_npu.optim.NpuFusedRMSprop": {
    "signature": "(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)"
  },
  "torch_npu.optim.NpuFusedRMSprop._init_param_state": {
    "signature": "(self, p, momentum, centered)"
  },
  "torch_npu.optim.NpuFusedRMSprop._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedRMSprop._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedRMSprop._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedRMSpropTF": {
    "signature": "(params, lr=0.01, alpha=0.9, eps=1e-10, weight_decay=0, momentum=0.0, centered=False, decoupled_decay=False, lr_in_momentum=True)"
  },
  "torch_npu.optim.NpuFusedRMSpropTF._init_param_state": {
    "signature": "(self, p, momentum, centered)"
  },
  "torch_npu.optim.NpuFusedRMSpropTF._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedRMSpropTF._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedRMSpropTF._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedSGD": {
    "signature": "(params, lr=<required parameter>, momentum=0.0, dampening=0.0, weight_decay=0.0, nesterov=False)"
  },
  "torch_npu.optim.NpuFusedSGD._init_param_state": {
    "signature": "(self, p, weight_decay)"
  },
  "torch_npu.optim.NpuFusedSGD._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedSGD._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.NpuFusedSGD._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.NpuFusedSGD.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.npu_fused_adadelta.NpuFusedAdadelta": {
    "signature": "(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)"
  },
  "torch_npu.optim.npu_fused_adadelta.NpuFusedAdadelta._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.npu_fused_adadelta.NpuFusedAdadelta._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adadelta.NpuFusedAdadelta._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_adadelta.NpuFusedAdadelta._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adam.NpuFusedAdam": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
  },
  "torch_npu.optim.npu_fused_adam.NpuFusedAdam._init_param_state": {
    "signature": "(self, p, amsgrad)"
  },
  "torch_npu.optim.npu_fused_adam.NpuFusedAdam._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adam.NpuFusedAdam._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_adam.NpuFusedAdam._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._channel_view": {
    "signature": "(self, x)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._layer_view": {
    "signature": "(self, x)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._cosine_similarity": {
    "signature": "(self, x, y, eps, view_func)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._projection": {
    "signature": "(self, p, grad, perturb, delta, wd_ratio, eps)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._combine_middle_vars": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._combine_middle_vars_by_group": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adamp.NpuFusedAdamP.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.npu_fused_adamw.NpuFusedAdamW": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)"
  },
  "torch_npu.optim.npu_fused_adamw.NpuFusedAdamW._init_param_state": {
    "signature": "(self, p, amsgrad)"
  },
  "torch_npu.optim.npu_fused_adamw.NpuFusedAdamW._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_adamw.NpuFusedAdamW._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_adamw.NpuFusedAdamW._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_bert_adam.NpuFusedBertAdam": {
    "signature": "(params, lr=<required parameter>, warmup=-1, t_total=-1, schedule='warmup_linear', b1=0.9, b2=0.999, e=1e-06, weight_decay=0.01, max_grad_norm=1.0)"
  },
  "torch_npu.optim.npu_fused_bert_adam.NpuFusedBertAdam._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.npu_fused_bert_adam.NpuFusedBertAdam._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_bert_adam.NpuFusedBertAdam._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_bert_adam.NpuFusedBertAdam._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb": {
    "signature": "(params, lr=0.001, betas=(0.9, 0.999), eps=1e-06, weight_decay=0, adam=False, use_global_grad_norm=False)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._init_param_state": {
    "signature": "(self, p)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._combine_middle_vars": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._combine_middle_vars_by_group": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._get_global_grad_norm": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_lamb.NpuFusedLamb.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase": {
    "signature": "(params, default)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._maybe_init_combined_params_and_grads": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase.zero_grad": {
    "signature": "(self, set_to_none=False)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase.get_combined_params": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase.get_combined_grads": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._clip_grad_norm_fused_": {
    "signature": "(self, combined_grads, combined_grads_masks, max_norm, norm_type)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._combine_grads_mask": {
    "signature": "(self, list_of_params)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._maybe_init_combined_grads_masks": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase._get_combined_grad_masks": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_optim_base.NpuFusedOptimizerBase.clip_grad_norm_fused_": {
    "signature": "(self, max_norm, norm_type=2)"
  },
  "torch_npu.optim.npu_fused_rmsprop.NpuFusedRMSprop": {
    "signature": "(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)"
  },
  "torch_npu.optim.npu_fused_rmsprop.NpuFusedRMSprop._init_param_state": {
    "signature": "(self, p, momentum, centered)"
  },
  "torch_npu.optim.npu_fused_rmsprop.NpuFusedRMSprop._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_rmsprop.NpuFusedRMSprop._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_rmsprop.NpuFusedRMSprop._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_rmsprop_tf.NpuFusedRMSpropTF": {
    "signature": "(params, lr=0.01, alpha=0.9, eps=1e-10, weight_decay=0, momentum=0.0, centered=False, decoupled_decay=False, lr_in_momentum=True)"
  },
  "torch_npu.optim.npu_fused_rmsprop_tf.NpuFusedRMSpropTF._init_param_state": {
    "signature": "(self, p, momentum, centered)"
  },
  "torch_npu.optim.npu_fused_rmsprop_tf.NpuFusedRMSpropTF._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_rmsprop_tf.NpuFusedRMSpropTF._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_rmsprop_tf.NpuFusedRMSpropTF._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_sgd.NpuFusedSGD": {
    "signature": "(params, lr=<required parameter>, momentum=0.0, dampening=0.0, weight_decay=0.0, nesterov=False)"
  },
  "torch_npu.optim.npu_fused_sgd.NpuFusedSGD._init_param_state": {
    "signature": "(self, p, weight_decay)"
  },
  "torch_npu.optim.npu_fused_sgd.NpuFusedSGD._combine_group_param_states": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_sgd.NpuFusedSGD._maybe_init_combined_states": {
    "signature": "(self)"
  },
  "torch_npu.optim.npu_fused_sgd.NpuFusedSGD._group_step": {
    "signature": "(self, group_index)"
  },
  "torch_npu.optim.npu_fused_sgd.NpuFusedSGD.step": {
    "signature": "(self, closure=None)"
  },
  "torch_npu.profiler.AiCMetrics": {
    "signature": "()"
  },
  "torch_npu.profiler.ExportType": {
    "signature": "()"
  },
  "torch_npu.profiler.ProfilerAction": {
    "signature": "(value, names=None, *, module=None, qualname=None, type=None, start=1)"
  },
  "torch_npu.profiler.ProfilerAction._generate_next_value_": {
    "signature": "(name, start, count, last_values)"
  },
  "torch_npu.profiler.ProfilerAction._member_type_": {
    "signature": "()"
  },
  "torch_npu.profiler.ProfilerLevel": {
    "signature": "()"
  },
  "torch_npu.profiler._ExperimentalConfig": {
    "signature": "(profiler_level: int = 'Level0', aic_metrics: int = 'ACL_AICORE_NONE', l2_cache: bool = False, msprof_tx: bool = False, data_simplification: bool = True, record_op_args: bool = False, op_attr: bool = False, gc_detect_threshold: float = None, export_type: str = 'text')"
  },
  "torch_npu.profiler._ExperimentalConfig._check_params": {
    "signature": "(self)"
  },
  "torch_npu.profiler.profile": {
    "signature": "(*, activities: Union[Iterable[torch_npu._C._profiler.ProfilerActivity], NoneType] = None, schedule: Union[Callable[[int], torch_npu.profiler.scheduler.ProfilerAction], NoneType] = None, on_trace_ready: Union[Callable[..., Any], NoneType] = None, record_shapes: bool = False, profile_memory: bool = False, with_stack: bool = False, with_flops: bool = False, with_modules: bool = False, experimental_config: Union[torch_npu.profiler.experimental_config._ExperimentalConfig, NoneType] = None, use_cuda: Union[bool, NoneType] = None)"
  },
  "torch_npu.profiler.profile.start": {
    "signature": "(self)"
  },
  "torch_npu.profiler.profile.stop": {
    "signature": "(self)"
  },
  "torch_npu.profiler.profile.step": {
    "signature": "(self)"
  },
  "torch_npu.profiler.schedule": {
    "signature": "(wait: int, active: int, warmup: int = 0, repeat: int = 0, skip_first: int = 0) -> None"
  },
  "torch_npu.profiler.schedule._check_params": {
    "signature": "(self)"
  },
  "torch_npu.profiler.supported_activities": {
    "signature": "()"
  },
  "torch_npu.profiler.supported_ai_core_metrics": {
    "signature": "()"
  },
  "torch_npu.profiler.supported_export_type": {
    "signature": "()"
  },
  "torch_npu.profiler.supported_profiler_level": {
    "signature": "()"
  },
  "torch_npu.profiler.tensorboard_trace_handler": {
    "signature": "(dir_name: str = None, worker_name: str = None, analyse_flag: bool = True)"
  },
  "torch_npu.profiler.dynamic_profile.init": {
    "signature": "(path: str)"
  },
  "torch_npu.profiler.dynamic_profile.step": {
    "signature": "()"
  },
  "torch_npu.profiler.dynamic_profile.start": {
    "signature": "(config_path: str = None)"
  },
  "torch_npu.profiler.experimental_config.AiCMetrics": {
    "signature": "()"
  },
  "torch_npu.profiler.experimental_config.ExportType": {
    "signature": "()"
  },
  "torch_npu.profiler.experimental_config.ProfilerLevel": {
    "signature": "()"
  },
  "torch_npu.profiler.experimental_config._ExperimentalConfig": {
    "signature": "(profiler_level: int = 'Level0', aic_metrics: int = 'ACL_AICORE_NONE', l2_cache: bool = False, msprof_tx: bool = False, data_simplification: bool = True, record_op_args: bool = False, op_attr: bool = False, gc_detect_threshold: float = None, export_type: str = 'text')"
  },
  "torch_npu.profiler.experimental_config._ExperimentalConfig._check_params": {
    "signature": "(self)"
  },
  "torch_npu.profiler.experimental_config.supported_ai_core_metrics": {
    "signature": "()"
  },
  "torch_npu.profiler.experimental_config.supported_export_type": {
    "signature": "()"
  },
  "torch_npu.profiler.experimental_config.supported_profiler_level": {
    "signature": "()"
  },
  "torch_npu.profiler.profiler.analyse": {
    "signature": "(profiler_path: str, max_process_number: int = 36)"
  },
  "torch_npu.profiler.profiler.profile": {
    "signature": "(*, activities: Union[Iterable[torch_npu._C._profiler.ProfilerActivity], NoneType] = None, schedule: Union[Callable[[int], torch_npu.profiler.scheduler.ProfilerAction], NoneType] = None, on_trace_ready: Union[Callable[..., Any], NoneType] = None, record_shapes: bool = False, profile_memory: bool = False, with_stack: bool = False, with_flops: bool = False, with_modules: bool = False, experimental_config: Union[torch_npu.profiler.experimental_config._ExperimentalConfig, NoneType] = None, use_cuda: Union[bool, NoneType] = None)"
  },
  "torch_npu.profiler.profiler.profile.start": {
    "signature": "(self)"
  },
  "torch_npu.profiler.profiler.profile.stop": {
    "signature": "(self)"
  },
  "torch_npu.profiler.profiler.profile.step": {
    "signature": "(self)"
  },
  "torch_npu.profiler.profiler.supported_activities": {
    "signature": "()"
  },
  "torch_npu.profiler.profiler.tensorboard_trace_handler": {
    "signature": "(dir_name: str = None, worker_name: str = None, analyse_flag: bool = True)"
  },
  "torch_npu.profiler.profiler_interface.supported_activities": {
    "signature": "()"
  },
  "torch_npu.profiler.scheduler.ProfilerAction": {
    "signature": "(value, names=None, *, module=None, qualname=None, type=None, start=1)"
  },
  "torch_npu.profiler.scheduler.ProfilerAction._generate_next_value_": {
    "signature": "(name, start, count, last_values)"
  },
  "torch_npu.profiler.scheduler.ProfilerAction._member_type_": {
    "signature": "()"
  },
  "torch_npu.profiler.scheduler.Schedule": {
    "signature": "(wait: int, active: int, warmup: int = 0, repeat: int = 0, skip_first: int = 0) -> None"
  },
  "torch_npu.profiler.scheduler.Schedule._check_params": {
    "signature": "(self)"
  },
  "torch_npu.testing.common_distributed.TestSkip": {
    "signature": "(exit_code, message)"
  },
  "torch_npu.testing.common_distributed.TestSkip._make": {
    "signature": "(iterable)"
  },
  "torch_npu.testing.common_distributed.TestSkip._replace": {
    "signature": "(self, /, **kwds)"
  },
  "torch_npu.testing.common_distributed.TestSkip._asdict": {
    "signature": "(self)"
  },
  "torch_npu.testing.common_distributed.init_pg": {
    "signature": "(backend: str = 'hccl', world_size=1, rank=0, file_name='file://') -> None"
  },
  "torch_npu.testing.common_distributed.skipIfUnsupportMultiNPU": {
    "signature": "(npu_number_needed)"
  },
  "torch_npu.testing.common_distributed.with_comms": {
    "signature": "(func)"
  },
  "torch_npu.testing.common_methods_invocations.sample_inputs_median_custom": {
    "signature": "(self, device, dtype, requires_grad, **kwargs)"
  },
  "torch_npu.testing.common_methods_invocations.sample_inputs_normal_tensor_second": {
    "signature": "(self, device, dtype, requires_grad, **kwargs)"
  },
  "torch_npu.testing.common_utils.Baseline": {
    "signature": "(baselineFile)"
  },
  "torch_npu.testing.common_utils.Baseline.get_baseline": {
    "signature": "(self, resourceId)"
  },
  "torch_npu.testing.common_utils.Baseline.set_baseline": {
    "signature": "(self, resourceId, baseline)"
  },
  "torch_npu.testing.common_utils.Baseline.save_baseline": {
    "signature": "(self)"
  },
  "torch_npu.testing.common_utils.SkipIfNoLapack": {
    "signature": "()"
  },
  "torch_npu.testing.common_utils.SkipIfNotRegistered": {
    "signature": "()"
  },
  "torch_npu.testing.common_utils.SupportedDevices": {
    "signature": "(supported_devices: List[str]) -> None"
  },
  "torch_npu.testing.common_utils.check_operators_in_prof": {
    "signature": "(expected_operators, prof, unexpected_operators=None)"
  },
  "torch_npu.testing.common_utils.create_common_tensor": {
    "signature": "(item, minValue, maxValue, device=None)"
  },
  "torch_npu.testing.common_utils.create_dtype_tensor": {
    "signature": "(shape, dtype, npu_format=-1, min_value=-5, max_value=5, no_zero=False, device=None)"
  },
  "torch_npu.testing.common_utils.dump_baseline": {
    "signature": "()"
  },
  "torch_npu.testing.common_utils.freeze_rng_state": {
    "signature": "()"
  },
  "torch_npu.testing.common_utils.get_cycles_per_ms": {
    "signature": "() -> float"
  },
  "torch_npu.testing.common_utils.get_npu_device": {
    "signature": "()"
  },
  "torch_npu.testing.common_utils.is_iterable": {
    "signature": "(obj)"
  },
  "torch_npu.testing.common_utils.iter_indices": {
    "signature": "(tensor)"
  },
  "torch_npu.testing.common_utils.set_npu_device": {
    "signature": "()"
  },
  "torch_npu.testing.common_utils.test_2args_broadcast": {
    "signature": "(fn)"
  },
  "torch_npu.testing.decorator.Dtypes": {
    "signature": "(*args)"
  },
  "torch_npu.testing.decorator.Formats": {
    "signature": "(*args)"
  },
  "torch_npu.testing.decorator.feed_data": {
    "signature": "(func, new_name, *args, **kwargs)"
  },
  "torch_npu.testing.decorator.gen_op_input": {
    "signature": "(testcase, func, op_info)"
  },
  "torch_npu.testing.decorator.gen_ops_testcase": {
    "signature": "(cls, func, name, keys, value, op_info)"
  },
  "torch_npu.testing.decorator.instantiate_ops_tests": {
    "signature": "(op_db)"
  },
  "torch_npu.testing.decorator.instantiate_tests": {
    "signature": "(arg=None, **kwargs)"
  },
  "torch_npu.testing.testcase.TestCase": {
    "signature": "(method_name='runTest')"
  },
  "torch_npu.testing.testcase.TestCase.setUpClass": {
    "signature": "()"
  },
  "torch_npu.testing.testcase.TestCase.setUp": {
    "signature": "(self)"
  },
  "torch_npu.testing.testcase.TestCase.assertTensorsSlowEqual": {
    "signature": "(self, x, y, prec=None, message='')"
  },
  "torch_npu.testing.testcase.TestCase.genSparseTensor": {
    "signature": "(self, size, sparse_dim, nnz, is_uncoalesced, device='cpu')"
  },
  "torch_npu.testing.testcase.TestCase.safeToDense": {
    "signature": "(self, t)"
  },
  "torch_npu.testing.testcase.TestCase.safeCoalesce": {
    "signature": "(self, t)"
  },
  "torch_npu.testing.testcase.TestCase.assertRtolEqual": {
    "signature": "(self, x, y, prec=0.0001, prec16=0.001, auto_trans_dtype=False, message=None)"
  },
  "torch_npu.testing.testcase.TestCase._assert_tensor_equal": {
    "signature": "(self, a, b, message, exact_dtype, allow_inf, prec)"
  },
  "torch_npu.testing.testcase.TestCase._assertNumberEqual": {
    "signature": "(self, x, y, prec=None, message='', allow_inf=False, exact_dtype=None)"
  },
  "torch_npu.testing.testcase.TestCase._assertBoolEqual": {
    "signature": "(self, x, y, prec=None, message='', allow_inf=False, exact_dtype=None)"
  },
  "torch_npu.testing.testcase.TestCase._assertTensorsEqual": {
    "signature": "(self, x, y, prec=None, message='', allow_inf=False, exact_dtype=None)"
  },
  "torch_npu.testing.testcase.TestCase.assertEqual": {
    "signature": "(self, x, y, prec=None, message='', allow_inf=False, exact_dtype=None)"
  },
  "torch_npu.testing.testcase.TestCase.assertAlmostEqual": {
    "signature": "(self, x, y, places=None, msg=None, delta=None, allow_inf=None)"
  },
  "torch_npu.testing.testcase.TestCase.assertNotEqual": {
    "signature": "(self, x, y, prec=None, message='')"
  },
  "torch_npu.testing.testcase.TestCase.assertObjectIn": {
    "signature": "(self, obj, iterable)"
  },
  "torch_npu.testing.testcase.TestCase.assertExpectedRaises": {
    "signature": "(self, exc_type, call_fn, *args, **kwargs)"
  },
  "torch_npu.testing.testcase.TestCase.assertNotWarn": {
    "signature": "(self, call_fn, msg='')"
  },
  "torch_npu.testing.testcase.TestCase.assertWarns": {
    "signature": "(self, call_fn, msg='')"
  },
  "torch_npu.testing.testcase.TestCase.maybeWarnsRegex": {
    "signature": "(self, category, regex='')"
  },
  "torch_npu.testing.testcase.TestCase._reset_warning_registry": {
    "signature": "(self)"
  },
  "torch_npu.testing.testcase.TestCase.assertExpectedStripMangled": {
    "signature": "(self, s, subname=None)"
  },
  "torch_npu.testing.testcase.TestCase.run": {
    "signature": "(self, result=None)"
  },
  "torch_npu.testing.testcase.run_tests": {
    "signature": "()"
  },
  "torch_npu.utils.get_part_combined_tensor": {
    "signature": "(combined_tensor, index, size)"
  },
  "torch_npu.utils.is_combined_tensor_valid": {
    "signature": "(combined_tensor, list_of_tensor)"
  },
  "torch_npu.utils.npu_combine_tensors": {
    "signature": "(list_of_tensor, require_copy_value=True)"
  },
  "torch_npu.utils.collect_env.SystemEnv": {
    "signature": "(torch_version, torch_npu_version, is_debug_build, gcc_version, clang_version, cmake_version, os, libc_version, python_version, python_platform, pip_version, pip_packages, conda_packages, caching_allocator_config, is_xnnpack_available, cpu_info, cann_version)"
  },
  "torch_npu.utils.collect_env.SystemEnv._make": {
    "signature": "(iterable)"
  },
  "torch_npu.utils.collect_env.SystemEnv._replace": {
    "signature": "(self, /, **kwds)"
  },
  "torch_npu.utils.collect_env.SystemEnv._asdict": {
    "signature": "(self)"
  },
  "torch_npu.utils.collect_env.check_directory_path_readable": {
    "signature": "(path)"
  },
  "torch_npu.utils.collect_env.check_path_owner_consistent": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.collect_env.get_torch_npu_install_path": {
    "signature": "()"
  },
  "torch_npu.utils.collect_env.get_cann_version": {
    "signature": "()"
  },
  "torch_npu.utils.collect_env.get_env_info": {
    "signature": "()"
  },
  "torch_npu.utils.collect_env.get_pretty_env_info": {
    "signature": "()"
  },
  "torch_npu.utils.collect_env.get_torch_npu_version": {
    "signature": "()"
  },
  "torch_npu.utils.collect_env.main": {
    "signature": "()"
  },
  "torch_npu.utils.collect_env.pretty_str": {
    "signature": "(envinfo)"
  },
  "torch_npu.utils.combine_tensors.get_part_combined_tensor": {
    "signature": "(combined_tensor, index, size)"
  },
  "torch_npu.utils.combine_tensors.is_combined_tensor_valid": {
    "signature": "(combined_tensor, list_of_tensor)"
  },
  "torch_npu.utils.combine_tensors.npu_combine_tensors": {
    "signature": "(list_of_tensor, require_copy_value=True)"
  },
  "torch_npu.utils.cpp_extension.NpuExtension": {
    "signature": "(name, sources, *args, **kwargs)"
  },
  "torch_npu.utils.path_manager.PathManager": {
    "signature": "()"
  },
  "torch_npu.utils.path_manager.PathManager.check_input_directory_path": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.path_manager.PathManager.check_input_file_path": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.path_manager.PathManager.check_path_owner_consistent": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.path_manager.PathManager.check_directory_path_writeable": {
    "signature": "(path)"
  },
  "torch_npu.utils.path_manager.PathManager.check_directory_path_readable": {
    "signature": "(path)"
  },
  "torch_npu.utils.path_manager.PathManager.remove_path_safety": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.path_manager.PathManager.remove_file_safety": {
    "signature": "(file: str)"
  },
  "torch_npu.utils.path_manager.PathManager.make_dir_safety": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.path_manager.PathManager.create_file_safety": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.path_manager.PathManager._input_path_common_check": {
    "signature": "(path: str)"
  },
  "torch_npu.utils.profiler.Profile": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.utils.profiler.Singleton": {
    "signature": "(cls)"
  },
  "torch_npu.utils.serialization.load": {
    "signature": "(f: Union[str, os.PathLike, BinaryIO, IO[bytes]], map_location: Union[Callable[[torch.types.Storage, str], torch.types.Storage], torch.device, str, Dict[str, str], NoneType] = None, pickle_module: Any = None, *, weights_only: bool = False, mmap: Union[bool, NoneType] = None, **pickle_load_args: Any) -> Any"
  },
  "torch_npu.utils.serialization.save": {
    "signature": "(obj: object, f: Union[str, os.PathLike, BinaryIO, IO[bytes]], pickle_module: Any = <module 'pickle'>, pickle_protocol: int = 2, _use_new_zipfile_serialization: bool = True, _disable_byteorder_record: bool = False) -> None"
  },
  "torch_npu.utils.syncbatchnorm.SyncBatchNorm": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.utils.syncbatchnorm.SyncBatchNorm.forward": {
    "signature": "(self, input_tensor, weight, bias, running_mean, running_var, eps, momentum, process_group, world_size)"
  },
  "torch_npu.utils.syncbatchnorm.SyncBatchNorm.backward": {
    "signature": "(self, grad_output)"
  },
  "torch_npu._npu_dropout": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.copy_memory_": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.empty_with_format": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.fast_gelu": {
    "signature": "(self)"
  },
  "torch_npu.npu_alloc_float_status": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_anchor_response_flags": {
    "signature": "(self, featmap_size, stride, num_base_anchors)"
  },
  "torch_npu.npu_anti_quant": {
    "signature": "(x, scale, offset=None, dst_dtype=None, src_dtype=None)"
  },
  "torch_npu.npu_apply_adam": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_batch_nms": {
    "signature": "(self, scores, score_threshold, iou_threshold, max_size_per_class, max_total_size, change_coordinate_frame=False, transpose_box=False)"
  },
  "torch_npu.npu_bert_apply_adam": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_bmmV2": {
    "signature": "(self, mat2, output_sizes)"
  },
  "torch_npu.npu_bounding_box_decode": {
    "signature": "(rois, deltas, means0, means1, means2, means3, stds0, stds1, stds2, stds3, max_shape, wh_ratio_clip)"
  },
  "torch_npu.npu_bounding_box_encode": {
    "signature": "(anchor_box, ground_truth_box, means0, means1, means2, means3, stds0, stds1, stds2, stds3)"
  },
  "torch_npu.npu_broadcast": {
    "signature": "(self, size, out=None)"
  },
  "torch_npu.npu_ciou": {
    "signature": "(self, gtboxes, trans=False, is_cross=True, mode=0, atan_sub_flag=False)"
  },
  "torch_npu.npu_clear_float_status": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_confusion_transpose": {
    "signature": "(self, perm, shape, transpose_first)"
  },
  "torch_npu.npu_conv2d": {
    "signature": "(input_, weight, bias, stride, padding, dilation, groups)"
  },
  "torch_npu.npu_conv3d": {
    "signature": "(input_, weight, bias, stride, padding, dilation, groups)"
  },
  "torch_npu.npu_conv_transpose2d": {
    "signature": "(input_, weight, bias, padding, output_padding, stride, dilation, groups)"
  },
  "torch_npu.npu_convert_weight_to_int4pack": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_convolution": {
    "signature": "(input_, weight, bias, stride, padding, dilation, groups)"
  },
  "torch_npu.npu_convolution_transpose": {
    "signature": "(input_, weight, bias, padding, output_padding, stride, dilation, groups)"
  },
  "torch_npu.npu_deformable_conv2d": {
    "signature": "(inputs, weight, offset, bias, kernel_size, stride, padding, dilation=[1, 1, 1, 1], groups=1, deformable_groups=1, modulated=True)"
  },
  "torch_npu.npu_diou": {
    "signature": "(self, gtboxes, trans=False, is_cross=False, mode=0)"
  },
  "torch_npu.npu_dropout_with_add_softmax": {
    "signature": "(self, x1, alpha, prob, dim)"
  },
  "torch_npu.npu_dtype_cast": {
    "signature": "(self, dtype)"
  },
  "torch_npu.npu_dynamic_quant": {
    "signature": "(input_dummy, smooth_scales=None)"
  },
  "torch_npu.npu_dynamic_quant_asymmetric": {
    "signature": "(input_dummy, smooth_scales=None, group_index=None, dst_type=torch.int8)"
  },
  "torch_npu.npu_group_quant": {
    "signature": "(x, scale, group_index, offset=None, dst_dtype=None)"
  },
  "torch_npu.npu_mm_all_reduce_base": {
    "signature": "(x1, x2, hcom, reduce_op, bias, antiquant_scale, antiquant_offset, x3, dequant_scale, pertoken_scale, comm_quant_scale_1, comm_quant_scale_2, antiquant_group_size, comm_turn)"
  },
  "torch_npu.npu_stride_add": {
    "signature": "(self, other, offset1, offset2, c1_len)"
  },
  "torch_npu.npu_ffn": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_format_cast": {
    "signature": "(self, acl_format)"
  },
  "torch_npu.npu_format_cast_": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_fused_attention_score": {
    "signature": "(query_layer, key_layer, value_layer, attention_mask, scale, keep_prob, query_transpose=False, key_transpose=False, bmm_score_transpose_a=False, bmm_score_transpose_b=False, value_transpose=False, dx_transpose=False)"
  },
  "torch_npu.npu_fusion_attention": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_geglu": {
    "signature": "(self, dim=-1, approximate=1, activate_left=False)"
  },
  "torch_npu.npu_get_float_status": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_giou": {
    "signature": "(self, gtboxes, trans=False, is_cross=False, mode=0)"
  },
  "torch_npu.npu_grid_assign_positive": {
    "signature": "(self, overlaps, box_responsible_flags, max_overlaps, argmax_overlaps, gt_max_overlaps, gt_argmax_overlaps, num_gts, pos_iou_thr, min_pos_iou, gt_max_assign_all)"
  },
  "torch_npu.npu_grouped_matmul": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_gru": {
    "signature": "(inputs, hx, weight_input, weight_hidden, bias_input, bias_hidden, seq_length, has_biases, num_layers, dropout, train, bidirectional, batch_first)"
  },
  "torch_npu.npu_incre_flash_attention": {
    "signature": "(self, query, key, value, padding_mask, atten_mask, pse_shift, actual_seq_lengths, num_heads, scale_value, input_layout, num_key_value_heads)"
  },
  "torch_npu.npu_indexing": {
    "signature": "(self, begin, end, strides, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0)"
  },
  "torch_npu.npu_iou": {
    "signature": "(bboxes, gtboxes, mode=0)"
  },
  "torch_npu.npu_layer_norm_eval": {
    "signature": "(input_, normalized_shape, weight=None, bias=None, eps=1e-05)"
  },
  "torch_npu.npu_linear": {
    "signature": "(input_, weight, bias=None)"
  },
  "torch_npu.npu_lstm": {
    "signature": "(inputs, weight, bias, seqMask, h, c, has_biases, num_layers, dropout, train, bidirectional, batch_first, flagSeq, direction)"
  },
  "torch_npu.npu_max": {
    "signature": "(self, dim, keepdim=False)"
  },
  "torch_npu.npu_min": {
    "signature": "(self, dim, keepdim=False)"
  },
  "torch_npu.npu_mish": {
    "signature": "(self)"
  },
  "torch_npu.npu_multi_head_attention": {
    "signature": "(query, key, value, query_weight, key_weight, value_weight, attn_mask, out_proj_weight, query_bias, key_bias, value_bias, out_proj_bias, dropout_mask, attn_head_num, attn_dim_per_head, src_len, tgt_len, dropout_prob, softmax_use_float)"
  },
  "torch_npu.npu_nms_rotated": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_nms_v4": {
    "signature": "(self, scores, max_output_size, iou_threshold, scores_threshold, pad_to_max_output_size=False)"
  },
  "torch_npu.npu_nms_with_mask": {
    "signature": "(inputs, iou_threshold)"
  },
  "torch_npu.npu_one_hot": {
    "signature": "(self, num_classes=-1, depth=1, on_value=1, off_value=0)"
  },
  "torch_npu.npu_pad": {
    "signature": "(input_, paddings)"
  },
  "torch_npu.npu_prompt_flash_attention": {
    "signature": "(self, query, key, value, padding_mask, atten_mask, pse_shift, actual_seq_lengths, num_heads, scale_value, pre_tokens, next_tokens, input_layout, num_key_value_heads)"
  },
  "torch_npu.npu_ps_roi_pooling": {
    "signature": "(self, rois, spatial_scale, group_size, output_dim)"
  },
  "torch_npu.npu_ptiou": {
    "signature": "(bboxes, gtboxes, mode=0)"
  },
  "torch_npu.npu_quant_matmul": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_quant_scatter": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_random_choice_with_mask": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_reshape": {
    "signature": "(self, shape, can_refresh=False, out=None)"
  },
  "torch_npu.npu_rms_norm": {
    "signature": "(self, gamma, epsilon=1e-06)"
  },
  "torch_npu.npu_roi_align": {
    "signature": "(self, rois, spatial_scale, pooled_height, pooled_width, sample_num, roi_end_mode)"
  },
  "torch_npu.npu_rotary_mul": {
    "signature": "(x, r1, r2)"
  },
  "torch_npu.npu_rotated_iou": {
    "signature": "(self, query_boxes, trans=False, mode=0, is_cross=True, v_threshold=0.0, e_threshold=0.0)"
  },
  "torch_npu.npu_rotated_overlaps": {
    "signature": "(self, query_boxes, trans=False)"
  },
  "torch_npu.npu_scaled_masked_softmax": {
    "signature": "(x, mask, scale=1, fixed_triu_mask=False)"
  },
  "torch_npu.npu_scatter_nd_update": {
    "signature": "(self, indices, updates)"
  },
  "torch_npu.npu_scatter_nd_update_": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_sign_bits_pack": {
    "signature": "(self, size)"
  },
  "torch_npu.npu_sign_bits_unpack": {
    "signature": "(inputs, size, dtype)"
  },
  "torch_npu.npu_silu": {
    "signature": "(self)"
  },
  "torch_npu.npu_slice": {
    "signature": "(self, offsets, size)"
  },
  "torch_npu.npu_softmax_cross_entropy_with_logits": {
    "signature": "(self, labels)"
  },
  "torch_npu.npu_sort_v2": {
    "signature": "(self, dim=-1, descending=False, out=None)"
  },
  "torch_npu.npu_swiglu": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_trans_quant_param": {
    "signature": "(*args, **kwargs)"
  },
  "torch_npu.npu_transpose": {
    "signature": "(self, perm, require_contiguous=True, out=None)"
  },
  "torch_npu.npu_weight_quant_batchmatmul": {
    "signature": "(x, weight, antiquant_scale, antiquant_offset, quant_scale, quant_offset, bias, antiquant_group_size)"
  },
  "torch_npu.npu_yolo_boxes_encode": {
    "signature": "(self, gt_bboxes, weight)"
  },
  "torch_npu_public_env: INF_NAN_MODE_ENABLE": {
    "mode": "std::unordered_map<int32_t, std::string> infNanMode = {{0, \"max\"}, {1, \"inf_nan\"}}"
  },
  "torch_npu_public_env: COMBINED_ENABLE": {
    "mode": "std::unordered_map<int32_t, std::string> combinedEnableMode = {{0, \"close\"}, {1, \"open\"}}"
  },
  "torch_npu_public_env: ASCEND_LAUNCH_BLOCKING": {
    "mode": "std::unordered_map<int32_t, std::string> launchBlockingMode = {{0, \"disable\"}, {1, \"enable\"}}"
  },
  "torch_npu_public_env: HCCL_ASYNC_ERROR_HANDLING": {
    "mode": "std::unordered_map<int32_t, std::string> asyncErrorHandlingMode = {{0, \"close\"}, {1, \"open\"}}"
  },
  "torch_npu_public_env: HCCL_DESYNC_DEBUG": {
    "mode": "std::unordered_map<int32_t, std::string> desyncDebugMode = {{0, \"close\"}, {1, \"open\"}}"
  },
  "torch_npu_public_env: ASCEND_GLOBAL_LOG_LEVEL": {
    "mode": "std::unordered_map<int32_t, std::string> logLevelMode = {{0, \"debug\"}, {1, \"info\"}, {2, \"warning\"}, {3, \"error\"}, {4, \"null\"}}"
  },
  "torch_npu_public_env: PYTORCH_NO_NPU_MEMORY_CACHING": {
    "mode": "std::unordered_map<int32_t, std::string> memoryCacheMode = {{0, \"open\"}, {1, \"close\"}}"
  },
  "torch_npu_public_env: TASK_QUEUE_ENABLE": {
    "mode": "std::unordered_map<int32_t, std::string> taskQueueEnableMode = {{0, \"close\"}, {1, \"level 1\"}, {2, \"level 2\"}}"
  },
  "func: npu_change_data_ptr": {
    "signature": "(Tensor dst, Tensor src, int index) -> int"
  },
  "func: get_npu_format": {
    "signature": "(Tensor self) -> int"
  },
  "func: npu_format_cast.Tensor": {
    "signature": "(Tensor self, Tensor dst) -> Tensor"
  },
  "func: npu_format_cast_.acl_format": {
    "signature": "(Tensor(a!) self, int acl_format) -> Tensor(a!)"
  },
  "func: npu_format_cast_": {
    "signature": "(Tensor(a!) self, Tensor src) -> Tensor(a!)"
  },
  "func: empty_with_format": {
    "signature": "(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor"
  },
  "func: unsafe_empty_with_format": {
    "signature": "(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor"
  },
  "func: empty_with_format.names": {
    "signature": "(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor"
  },
  "func: copy_memory_": {
    "signature": "(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
  },
  "func: get_storage_size": {
    "signature": "(Tensor self) -> int"
  },
  "func: npu_format_cast": {
    "signature": "(Tensor self, int acl_format) -> Tensor"
  },
  "func: _npu_format_cast": {
    "signature": "(Tensor self, int acl_format) -> Tensor"
  },
  "func: repeat_interleave_backward_tensor": {
    "signature": "(Tensor grad, Tensor self, Tensor repeats, int? dim=None) -> Tensor"
  },
  "func: repeat_interleave_backward_int": {
    "signature": "(Tensor grad, Tensor self, SymInt repeats, int? dim=None) -> Tensor"
  },
  "func: npu_view_copy": {
    "signature": "(Tensor(a!) self, Tensor other, bool non_blocking) -> Tensor(a!)"
  },
  "func: npu_transpose": {
    "signature": "(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor"
  },
  "func: npu_transpose.out": {
    "signature": "(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_broadcast": {
    "signature": "(Tensor self, int[] size) -> Tensor"
  },
  "func: npu_broadcast.out": {
    "signature": "(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_dtype_cast_": {
    "signature": "(Tensor(a!) self, Tensor src) -> Tensor(a!)"
  },
  "func: npu_alloc_float_status": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: npu_get_float_status": {
    "signature": "(Tensor self, int mode=0) -> Tensor"
  },
  "func: npu_clear_float_status": {
    "signature": "(Tensor self, int mode=0) -> Tensor"
  },
  "func: one_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: npu_fast_gelu": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: npu_fast_gelu_backward": {
    "signature": "(Tensor grad, Tensor self) -> Tensor"
  },
  "func: _amp_foreach_non_finite_check": {
    "signature": "(Tensor[] scaled_grads) -> bool"
  },
  "func: npu_sign_bits_pack": {
    "signature": "(Tensor self, int size) -> Tensor"
  },
  "func: npu_bert_apply_adam": {
    "signature": "(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)"
  },
  "func: npu_bert_apply_adam.out": {
    "signature": "(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  "func: npu_conv_transpose2d_backward": {
    "signature": "(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_conv_transpose3d_backward": {
    "signature": "(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_convolution_backward": {
    "signature": "(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_conv_transpose2d": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor"
  },
  "func: npu_conv2d": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor"
  },
  "func: npu_conv2d.out": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_conv2d_backward": {
    "signature": "(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_conv3d": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor"
  },
  "func: npu_conv3d.out": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_conv3d_backward": {
    "signature": "(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_stride_add": {
    "signature": "(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor"
  },
  "func: npu_slice": {
    "signature": "(Tensor self, int[] offsets, int[] size) -> Tensor"
  },
  "func: npu_slice.out": {
    "signature": "(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_indexing": {
    "signature": "(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor"
  },
  "func: npu_indexing.out": {
    "signature": "(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_softmax_cross_entropy_with_logits_backward": {
    "signature": "(Tensor grad, Tensor self, Tensor labels) -> Tensor"
  },
  "func: npu_stride_copy": {
    "signature": "(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor"
  },
  "func: npu_stride_copy.out": {
    "signature": "(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_roi_align": {
    "signature": "(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor"
  },
  "func: npu_roi_alignbk": {
    "signature": "(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor"
  },
  "func: npu_sort_v2.out": {
    "signature": "(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_sort_v2": {
    "signature": "(Tensor self, int dim=-1, bool descending=False) -> Tensor"
  },
  "func: npu_one_hot": {
    "signature": "(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor"
  },
  "func: npu_linear_backward": {
    "signature": "(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)"
  },
  "func: npu_anchor_response_flags": {
    "signature": "(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor"
  },
  "func: npu_dropout_backward": {
    "signature": "(Tensor grad_output, Tensor mask, float p) -> Tensor"
  },
  "func: npu_nms_rotated": {
    "signature": "(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)"
  },
  "func: npu_masked_fill_range": {
    "signature": "(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor"
  },
  "func: npu_moe_init_routing": {
    "signature": "(Tensor x, Tensor row_idx, Tensor expert_idx, int active_num) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_moe_gating_top_k_softmax": {
    "signature": "(Tensor x, Tensor? finished=None, int k=1) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_sub_sample": {
    "signature": "(Tensor self, int per_images, float positive_fraction) -> Tensor"
  },
  "func: npu_yolo_boxes_encode": {
    "signature": "(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor"
  },
  "func: npu_scatter": {
    "signature": "(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor"
  },
  "func: npu_layer_norm_eval": {
    "signature": "(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor"
  },
  "func: npu_rotated_box_encode": {
    "signature": "(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor"
  },
  "func: npu_rotated_box_decode": {
    "signature": "(Tensor self, Tensor deltas, Tensor weight) -> Tensor"
  },
  "func: npu_rotated_overlaps": {
    "signature": "(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor"
  },
  "func: npu_silu_backward": {
    "signature": "(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor"
  },
  "func: npu_rotated_iou": {
    "signature": "(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor"
  },
  "func: npu_nms_with_mask": {
    "signature": "(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_gru_backward": {
    "signature": "(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_mish_backward": {
    "signature": "(Tensor grad, Tensor input) -> Tensor"
  },
  "func: npu_reshape": {
    "signature": "(Tensor self, int[] shape, bool can_refresh=False) -> Tensor"
  },
  "func: npu_reshape.out": {
    "signature": "(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: npu_batch_nms": {
    "signature": "(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_bounding_box_encode": {
    "signature": "(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor"
  },
  "func: npu_bounding_box_decode": {
    "signature": "(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor"
  },
  "func: npu_apply_adam": {
    "signature": "(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_apply_adam.out": {
    "signature": "(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  "func: npu_apply_adam_w": {
    "signature": "(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_apply_adam_w.out": {
    "signature": "(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  "func: npu_deformable_conv2dbk": {
    "signature": "(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_giou_backward": {
    "signature": "(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)"
  },
  "func: npu_diou_backward": {
    "signature": "(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)"
  },
  "func: npu_iou": {
    "signature": "(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor"
  },
  "func: npu_nms_v4": {
    "signature": "(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)"
  },
  "func: npu_pad": {
    "signature": "(Tensor input, int[] paddings) -> Tensor"
  },
  "func: npu_geglu": {
    "signature": "(Tensor self, int dim=-1, int approximate=1, bool activate_left=False) -> (Tensor, Tensor)"
  },
  "func: npu_geglu_grad": {
    "signature": "(Tensor grad_output, Tensor self, Tensor gelu, int dim=-1, int approximate=1, bool activate_left=False) -> Tensor"
  },
  "func: npu_add_layer_norm": {
    "signature": "(Tensor x1, Tensor x2, Tensor gamma, Tensor beta, float epsilon=1e-05, bool additional_output=False) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_add_layer_norm_backward": {
    "signature": "(Tensor? dy_opt, Tensor x1, Tensor x2, Tensor rstd, Tensor mean, Tensor gamma, Tensor? dsum_opt) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_add_rms_norm": {
    "signature": "(Tensor x1, Tensor x2, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_rms_norm": {
    "signature": "(Tensor self, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor)"
  },
  "func: npu_rms_norm_backward": {
    "signature": "(Tensor dy, Tensor self, Tensor gamma, Tensor rstd) -> (Tensor, Tensor)"
  },
  "func: npu_swiglu": {
    "signature": "(Tensor self, int dim=-1) -> Tensor"
  },
  "func: npu_swiglu_backward": {
    "signature": "(Tensor grad_output, Tensor self, int dim=-1) -> Tensor"
  },
  "func: npu_deep_norm": {
    "signature": "(Tensor x, Tensor gx, Tensor beta, Tensor gamma, float alpha=0.3, float epsilon=1e-06) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_deep_norm_backward": {
    "signature": "(Tensor dy, Tensor x, Tensor gx, Tensor gamma, Tensor mean, Tensor rstd, float alpha=0.3) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_random_choice_with_mask": {
    "signature": "(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)"
  },
  "func: npu_normalize_batch": {
    "signature": "(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor"
  },
  "func: npu_ptiou": {
    "signature": "(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor"
  },
  "func: npu_lstm_backward": {
    "signature": "(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: _dropout_with_byte_mask_backward": {
    "signature": "(Tensor grad_output, Tensor mask, float p) -> Tensor"
  },
  "func: dropout_with_byte_mask": {
    "signature": "(Tensor self, float p, bool train) -> Tensor"
  },
  "func: npu_dropout_with_add_softmax_backward": {
    "signature": "(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)"
  },
  "func: npu_multi_head_attention_backward": {
    "signature": "(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_dropout_gen_mask": {
    "signature": "(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: npu_ciou_backward": {
    "signature": "(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)"
  },
  "func: npu_sign_bits_unpack": {
    "signature": "(Tensor input, int size, ScalarType dtype) -> Tensor"
  },
  "func: _conv_depthwise2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)"
  },
  "func: slow_conv_dilated2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  },
  "func: slow_conv_transpose2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  },
  "func: npu_lstm_cell_backward": {
    "signature": "(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: batch_norm_reduce": {
    "signature": "(Tensor input, float eps) -> (Tensor, Tensor)"
  },
  "func: batch_norm_gather_stats_update": {
    "signature": "(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)"
  },
  "func: npu_fused_attention_score_backward": {
    "signature": "(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_fused_attention_score_fwd": {
    "signature": "(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_fused_attention_score_grad": {
    "signature": "(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_fused_attention_qkv_grad": {
    "signature": "(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]"
  },
  "func: npu_fused_attention_layernorm_qkv_fwd": {
    "signature": "(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]"
  },
  "func: npu_layernorm_grad": {
    "signature": "(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_ifmr": {
    "signature": "(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)"
  },
  "func: npu_grid_assign_positive": {
    "signature": "(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor"
  },
  "func: npu_rotary_mul_backward": {
    "signature": "(Tensor grad, Tensor self, Tensor r1, Tensor r2) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_convolution": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor"
  },
  "func: npu_convolution_transpose": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor"
  },
  "func: fast_gelu": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: npu_confusion_transpose": {
    "signature": "(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor"
  },
  "func: npu_ps_roi_pooling": {
    "signature": "(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor"
  },
  "func: npu_linear": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
  },
  "func: _npu_dropout": {
    "signature": "(Tensor self, float p) -> (Tensor, Tensor)"
  },
  "func: npu_softmax_cross_entropy_with_logits": {
    "signature": "(Tensor self, Tensor labels) -> Tensor"
  },
  "func: npu_max.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: npu_max.names_dim": {
    "signature": "(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: npu_bmmV2": {
    "signature": "(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor"
  },
  "func: npu_dtype_cast": {
    "signature": "(Tensor self, ScalarType dtype) -> Tensor"
  },
  "func: npu_silu": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: npu_silu_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: npu_gru": {
    "signature": "(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_mish": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: npu_min.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: npu_min.names_dim": {
    "signature": "(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: npu_deformable_conv2d": {
    "signature": "(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)"
  },
  "func: npu_giou": {
    "signature": "(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor"
  },
  "func: npu_diou": {
    "signature": "(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor"
  },
  "func: npu_lstm": {
    "signature": "(Tensor input, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_lstm_data": {
    "signature": "(Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: _dropout_with_byte_mask": {
    "signature": "(Tensor self, float p) -> (Tensor, Tensor)"
  },
  "func: npu_dropout_with_add_softmax": {
    "signature": "(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_scaled_masked_softmax": {
    "signature": "(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor"
  },
  "func: npu_multi_head_attention": {
    "signature": "(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_fusion_attention": {
    "signature": "(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor, int, int, int)"
  },
  "func: npu_fusion_attention_grad": {
    "signature": "(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=0, int seed=0, int offset=0, int numels=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_dropout_do_mask": {
    "signature": "(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)"
  },
  "func: npu_ciou": {
    "signature": "(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor"
  },
  "func: npu_lstm_cell": {
    "signature": "(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: npu_fused_attention_score": {
    "signature": "(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor"
  },
  "func: npu_rotary_mul": {
    "signature": "(Tensor self, Tensor r1, Tensor r2) -> Tensor"
  },
  "func: _npu_ciou": {
    "signature": "(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> (Tensor, Tensor)"
  },
  "func: npu_prompt_flash_attention": {
    "signature": "(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, int[]? actual_seq_lengths=None, Tensor? deq_scale1=None, Tensor? quant_scale1=None, Tensor? deq_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, int pre_tokens=2147473647, int next_tokens=0, str input_layout=\"BSH\", int num_key_value_heads=0, int[]? actual_seq_lengths_kv=None, int sparse_mode=0) -> Tensor"
  },
  "func: npu_incre_flash_attention": {
    "signature": "(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, SymInt[]? actual_seq_lengths=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? block_table=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? kv_padding_size=None, int num_heads=1, float scale_value=1.0, str input_layout=\"BSH\", int num_key_value_heads=0, int block_size=0, int inner_precise=1) -> Tensor"
  },
  "func: npu_convolution_transpose_backward": {
    "signature": "(Tensor input, Tensor grad, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] grad_input_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_scaled_masked_softmax_backward": {
    "signature": "(Tensor y_grad, Tensor y, Tensor mask, Scalar scale, bool fixed_triu_mask) -> Tensor"
  },
  "func: npu_dtype_cast_backward": {
    "signature": "(Tensor grad, ScalarType dtype) -> Tensor"
  },
  "func: npu_binary_cross_entropy_with_logits_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight_opt, Tensor? pos_weight_opt, int reduction) -> Tensor"
  },
  "func: npu_confusion_transpose_backward": {
    "signature": "(Tensor grad, int[] perm, SymInt[] shape, bool transpose_first) -> Tensor"
  },
  "func: npu_max_backward": {
    "signature": "(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor"
  },
  "func: npu_min_backward": {
    "signature": "(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor"
  },
  "func: npu_ps_roi_pooling_backward": {
    "signature": "(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, SymInt[] input_size) -> Tensor"
  },
  "func: npu_bmm_v2_mat1_backward": {
    "signature": "(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor"
  },
  "func: npu_bmm_v2_mat2_backward": {
    "signature": "(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor"
  },
  "func: selu_backward": {
    "signature": "(Tensor grad_output, Tensor result) -> Tensor"
  },
  "func: npu_group_norm_silu": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, int group, float eps=0.00001) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_apply_rotary_pos_emb": {
    "signature": "(Tensor query, Tensor key, Tensor cos, Tensor sin, str layout='BSH') -> (Tensor, Tensor)"
  },
  "func: npu_lstm_data_backward": {
    "signature": "(Tensor? grady_opt, Tensor? gradh_opt, Tensor? gradc_opt, Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor init_h, Tensor init_c, Tensor y, Tensor h, Tensor c, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc, bool flag_direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor)"
  },
  "func: l1_loss_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
  },
  "func: kl_div_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor"
  },
  "func: npu_mm_reduce_scatter_base": {
    "signature": "(Tensor self, Tensor x2, str hcom, int world_size, *, str reduce_op='sum', Tensor? bias=None, int comm_turn=0) -> Tensor"
  },
  "func: npu_all_gather_base_mm": {
    "signature": "(Tensor self, Tensor x2, str hcom, int world_size, *, Tensor? bias=None, int gather_index=0, bool gather_output=True, int comm_turn=0) -> (Tensor, Tensor)"
  },
  "func: npu_mm_all_reduce_base": {
    "signature": "(Tensor x1, Tensor x2, str hcom, *, str reduce_op='sum', Tensor? bias=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? x3=None, Tensor? dequant_scale=None, Tensor? pertoken_scale=None, Tensor? comm_quant_scale_1=None, Tensor? comm_quant_scale_2=None, int antiquant_group_size=0, int comm_turn=0) -> Tensor"
  },
  "func: npu_scatter_list": {
    "signature": "(Tensor[] self, Tensor indices, Tensor updates, Tensor? mask=None, str reduce='update', int axis=-2) -> Tensor[]"
  },
  "func: npu_scatter_list_": {
    "signature": "(Tensor(a!)[] self, Tensor indices, Tensor updates, Tensor? mask=None, str reduce='update', int axis=-2) -> ()"
  },
  "func: npu_scatter_nd_update": {
    "signature": "(Tensor self, Tensor indices, Tensor updates) -> Tensor"
  },
  "func: npu_scatter_nd_update_": {
    "signature": "(Tensor(a!) self, Tensor indices, Tensor updates) -> Tensor(a!)"
  },
  "func: scatter_update": {
    "signature": "(Tensor self, Tensor indices, Tensor updates, int axis) -> Tensor"
  },
  "func: scatter_update_": {
    "signature": "(Tensor(a!) self, Tensor indices, Tensor updates, int axis) -> Tensor(a!)"
  },
  "func: npu_quant_scatter": {
    "signature": "(Tensor self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -> Tensor"
  },
  "func: npu_quant_scatter_": {
    "signature": "(Tensor(a!) self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -> Tensor(a!)"
  },
  "func: npu_multi_head_attention_v2": {
    "signature": "(Tensor query, Tensor key, Tensor value, Tensor? atten_mask=None, Tensor? alibi_mask=None, float scale=1.0, int head_num=1, str input_layout=\"BNSD\", float keep_prob=1., int pre_tokens=2147483647, int next_tokens=1, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, int, int, int)"
  },
  "func: npu_multi_head_attention_v2_grad": {
    "signature": "(Tensor attention_score_grad, Tensor query, Tensor key, Tensor value, Tensor softmax_log_max_sum, Tensor attention_score, Tensor? atten_mask=None, Tensor? alibi_mask=None, float scale=1.0, int head_num=1, str input_layout=\"BNSD\", float keep_prob=1., int pre_tokens=2147483647, int next_tokens=1, int seed=0, int offset=0, int numels=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_masked_softmax_with_rel_pos_bias": {
    "signature": "(Tensor x, Tensor? atten_mask, Tensor relative_pos_bias, float scale_value=1.0, int inner_precision_mode=0) -> Tensor"
  },
  "func: npu_ffn": {
    "signature": "(Tensor x, Tensor weight1, Tensor weight2, str activation, *, int[]? expert_tokens=None, int[]? expert_tokens_index=None, Tensor? bias1=None, Tensor? bias2=None, Tensor? scale=None, Tensor? offset=None, Tensor? deq_scale1=None, Tensor? deq_scale2=None, Tensor? antiquant_scale1=None, Tensor? antiquant_scale2=None, Tensor? antiquant_offset1=None, Tensor? antiquant_offset2=None, int? inner_precise=None, ScalarType? output_dtype=None) -> Tensor"
  },
  "func: npu_moe_compute_expert_tokens": {
    "signature": "(Tensor sorted_expert_for_source_row, int num_expert) -> Tensor"
  },
  "func: npu_grouped_matmul": {
    "signature": "(Tensor[] x, Tensor[] weight, *, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, int[]? group_list=None, int? split_item=0, ScalarType? output_dtype=None) -> Tensor[]"
  },
  "func: npu_weight_quant_batchmatmul": {
    "signature": "(Tensor x, Tensor weight, Tensor antiquant_scale, Tensor? antiquant_offset=None, Tensor? quant_scale=None, Tensor? quant_offset=None, Tensor? bias=None, int antiquant_group_size=0) -> Tensor"
  },
  "func: npu_convert_weight_to_int4pack": {
    "signature": "(Tensor weight, int inner_k_tiles=0) -> Tensor"
  },
  "func: npu_trans_quant_param": {
    "signature": "(Tensor scale, Tensor? offset=None) -> Tensor"
  },
  "func: npu_quant_matmul": {
    "signature": "(Tensor x1, Tensor x2, Tensor scale, *, Tensor? offset=None, Tensor? pertoken_scale=None, Tensor? bias=None, ScalarType? output_dtype=None) -> Tensor"
  },
  "func: _npu_dropout_gen_mask.Tensor": {
    "signature": "(Tensor self, int[] size, float p, int seed, int offset, *, bool? parallel=True, bool? sync=None) -> Tensor"
  },
  "func: npu_quantize": {
    "signature": "(Tensor self, Tensor scales, Tensor? zero_points, ScalarType dtype, int axis=1, bool div_mode=True) -> Tensor"
  },
  "func: npu_anti_quant": {
    "signature": "(Tensor x, Tensor scale, *, Tensor? offset=None, ScalarType? dst_dtype=None, ScalarType? src_dtype=None) -> Tensor"
  },
  "func: _npu_silent_check": {
    "signature": "(Tensor(a!) input_grad, Tensor val, Tensor(b!) pre_val, Tensor(c!) min_val, Tensor(d!) max_val, Tensor val_counter, int c_min_steps, float c_thresh_l1, float c_coeff_l1, float c_thresh_l2, float c_coeff_l2) -> Tensor"
  },
  "func: npu_quant_conv2d": {
    "signature": "(Tensor input, Tensor weight, Tensor scale, int[2] strides=1, int[2] pads=0, int[2] dilations=1, int groups=1, int offset_x=0, str round_mode='rint', ScalarType? output_dtype=None, Tensor? bias=None, Tensor? offset=None) -> Tensor"
  },
  "func: npu_moe_finalize_routing": {
    "signature": "(Tensor expanded_permuted_rows, Tensor? skip1, Tensor? skip2, Tensor? bias, Tensor? scales, Tensor expanded_src_to_dst_row, Tensor? export_for_source_row, int? drop_pad_mode=0) -> Tensor"
  },
  "func: matmul_double_backward": {
    "signature": "(Tensor? grad_self, Tensor? grad_other, Tensor grad_out, Tensor self, Tensor other, bool[3] mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: int_repr": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: dequantize.self": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: _empty_affine_quantized": {
    "signature": "(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor"
  },
  "func: clone": {
    "signature": ""
  },
  "func: copy_": {
    "signature": ""
  },
  "func: __ilshift__.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: __ilshift__.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: __ior__.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: __ior__.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: __irshift__.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: __irshift__.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: __lshift__.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: __lshift__.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: __rshift__.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: __rshift__.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: __xor__.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: __xor__.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: _adaptive_avg_pool2d": {
    "signature": "(Tensor self, SymInt[2] output_size) -> Tensor"
  },
  "func: _adaptive_avg_pool2d_backward": {
    "signature": "(Tensor grad_output, Tensor self) -> Tensor"
  },
  "func: _adaptive_avg_pool3d": {
    "signature": "(Tensor self, SymInt[3] output_size) -> Tensor"
  },
  "func: _adaptive_avg_pool3d_backward": {
    "signature": "(Tensor grad_output, Tensor self) -> Tensor"
  },
  "func: _add_relu.Tensor": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  "func: _add_relu.out": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _add_relu_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: _aminmax": {
    "signature": "(Tensor self) -> (Tensor, Tensor)"
  },
  "func: _aminmax.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)"
  },
  "func: signbit.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: aminmax": {
    "signature": "(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)"
  },
  "func: aminmax.out": {
    "signature": "(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)"
  },
  "func: _amp_foreach_non_finite_check_and_unscale_": {
    "signature": "(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()"
  },
  "func: _batch_norm_impl_index": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)"
  },
  "func: _batch_norm_impl_index_backward": {
    "signature": "(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)"
  },
  "func: _cdist_backward": {
    "signature": "(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor"
  },
  "func: _cdist_forward": {
    "signature": "(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor"
  },
  "func: _conv_depthwise2d": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation) -> Tensor"
  },
  "func: _conv_depthwise2d.out": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _convolution": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor"
  },
  "func: _ctc_loss": {
    "signature": "(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)"
  },
  "func: _ctc_loss_backward": {
    "signature": "(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor"
  },
  "func: _cummax_helper": {
    "signature": "(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()"
  },
  "func: _cummin_helper": {
    "signature": "(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()"
  },
  "func: _dim_arange": {
    "signature": "(Tensor like, int dim) -> Tensor"
  },
  "func: _embedding_bag": {
    "signature": "(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: _embedding_bag_backward": {
    "signature": "(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor"
  },
  "func: _embedding_bag_dense_backward": {
    "signature": "(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor"
  },
  "func: _embedding_bag_forward_only": {
    "signature": "(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: _embedding_bag_per_sample_weights_backward": {
    "signature": "(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor"
  },
  "func: _index_put_impl_": {
    "signature": "(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)"
  },
  "func: _linalg_svd": {
    "signature": "(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)"
  },
  "func: _linalg_svd.U": {
    "signature": "(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)"
  },
  "func: _log_softmax": {
    "signature": "(Tensor self, int dim, bool half_to_float) -> Tensor"
  },
  "func: _log_softmax.out": {
    "signature": "(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _log_softmax_backward_data": {
    "signature": "(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor"
  },
  "func: _log_softmax_backward_data.out": {
    "signature": "(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _native_batch_norm_legit": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  },
  "func: _nnpack_spatial_convolution": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1) -> Tensor"
  },
  "func: _pack_padded_sequence": {
    "signature": "(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)"
  },
  "func: _pad_packed_sequence": {
    "signature": "(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)"
  },
  "func: _pdist_forward": {
    "signature": "(Tensor self, float p=2) -> Tensor"
  },
  "func: _slow_conv2d_backward.output_mask": {
    "signature": "(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  },
  "func: _slow_conv2d_forward": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> Tensor"
  },
  "func: _slow_conv2d_forward.output": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output) -> Tensor(a!)"
  },
  "func: _softmax": {
    "signature": "(Tensor self, int dim, bool half_to_float) -> Tensor"
  },
  "func: _softmax.out": {
    "signature": "(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _softmax_backward_data": {
    "signature": "(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor"
  },
  "func: _softmax_backward_data.out": {
    "signature": "(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: _unique2": {
    "signature": "(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)"
  },
  "func: abs": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: abs.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: abs_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: acos": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: acos.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: acos_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: acosh": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: acosh.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: acosh_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: adaptive_avg_pool1d": {
    "signature": "(Tensor self, int[1] output_size) -> Tensor"
  },
  "func: adaptive_avg_pool2d": {
    "signature": "(Tensor self, SymInt[2] output_size) -> Tensor"
  },
  "func: adaptive_avg_pool2d.out": {
    "signature": "(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: adaptive_avg_pool3d.out": {
    "signature": "(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: adaptive_avg_pool3d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: adaptive_max_pool2d": {
    "signature": "(Tensor self, int[2] output_size) -> (Tensor, Tensor)"
  },
  "func: adaptive_max_pool2d.out": {
    "signature": "(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  "func: adaptive_max_pool2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor indices) -> Tensor"
  },
  "func: adaptive_max_pool2d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: add.Scalar": {
    "signature": "(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  "func: add.Tensor": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  "func: add.out": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: add_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: add_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: addbmm": {
    "signature": "(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  "func: addbmm.out": {
    "signature": "(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: addbmm_": {
    "signature": "(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: addcdiv": {
    "signature": "(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor"
  },
  "func: addcdiv.out": {
    "signature": "(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: addcdiv_": {
    "signature": "(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  },
  "func: addcmul": {
    "signature": "(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor"
  },
  "func: addcmul.out": {
    "signature": "(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: addcmul_": {
    "signature": "(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)"
  },
  "func: addmm": {
    "signature": "(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  "func: addmm.out": {
    "signature": "(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: addmm_": {
    "signature": "(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: addmv": {
    "signature": "(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  "func: addmv.out": {
    "signature": "(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: addmv_": {
    "signature": "(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: addr": {
    "signature": "(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  "func: addr.out": {
    "signature": "(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: addr_": {
    "signature": "(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: affine_grid_generator": {
    "signature": "(Tensor theta, SymInt[] size, bool align_corners) -> Tensor"
  },
  "func: affine_grid_generator_backward": {
    "signature": "(Tensor grad, SymInt[] size, bool align_corners) -> Tensor"
  },
  "func: all": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: all.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> Tensor"
  },
  "func: all.out": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: all.all_out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: amax": {
    "signature": "(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor"
  },
  "func: amax.out": {
    "signature": "(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: amin": {
    "signature": "(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor"
  },
  "func: amin.out": {
    "signature": "(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: angle": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: angle.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: any": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: any.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> Tensor"
  },
  "func: any.out": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: any.all_out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: arange": {
    "signature": "(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: arange.out": {
    "signature": "(Scalar end, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: arange.start": {
    "signature": "(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: arange.start_out": {
    "signature": "(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: arange.start_step": {
    "signature": "(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: argmax": {
    "signature": "(Tensor self, int? dim=None, bool keepdim=False) -> Tensor"
  },
  "func: argmax.out": {
    "signature": "(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: argmin": {
    "signature": "(Tensor self, int? dim=None, bool keepdim=False) -> Tensor"
  },
  "func: argmin.out": {
    "signature": "(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: argsort": {
    "signature": "(Tensor self, int dim=-1, bool descending=False) -> Tensor"
  },
  "func: argsort.dimname": {
    "signature": "(Tensor self, Dimname dim, bool descending=False) -> Tensor"
  },
  "func: asin": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: asin.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: asin_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: asinh": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: asinh.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: asinh_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: atan": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: atan.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: atan2": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: atan2.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: atan2_": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: atan_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: atanh": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: atanh.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: atanh_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: avg_pool2d": {
    "signature": "(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor"
  },
  "func: avg_pool2d.out": {
    "signature": "(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: avg_pool2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor"
  },
  "func: avg_pool2d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: avg_pool3d": {
    "signature": "(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor"
  },
  "func: avg_pool3d.out": {
    "signature": "(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: avg_pool3d_backward": {
    "signature": "(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor"
  },
  "func: avg_pool3d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: baddbmm": {
    "signature": "(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
  },
  "func: baddbmm.out": {
    "signature": "(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: baddbmm_": {
    "signature": "(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: batch_norm": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor"
  },
  "func: batch_norm_backward_elemt": {
    "signature": "(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -> Tensor"
  },
  "func: batch_norm_backward_reduce": {
    "signature": "(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)"
  },
  "func: batch_norm_elemt": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor"
  },
  "func: batch_norm_elemt.out": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: batch_norm_gather_stats_with_counts": {
    "signature": "(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)"
  },
  "func: batch_norm_stats": {
    "signature": "(Tensor input, float eps) -> (Tensor, Tensor)"
  },
  "func: bernoulli": {
    "signature": "(Tensor self, *, Generator? generator=None) -> Tensor"
  },
  "func: bernoulli.out": {
    "signature": "(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bernoulli.p": {
    "signature": "(Tensor self, float p, *, Generator? generator=None) -> Tensor"
  },
  "func: bernoulli_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: bernoulli_.float": {
    "signature": "(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: binary_cross_entropy": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor"
  },
  "func: binary_cross_entropy.out": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: binary_cross_entropy_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor"
  },
  "func: binary_cross_entropy_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: bincount": {
    "signature": "(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor"
  },
  "func: bitwise_and.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: bitwise_and.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_and.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: bitwise_and.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_and_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: bitwise_and_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: bitwise_not": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: bitwise_not.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_not_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: bitwise_or.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: bitwise_or.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_or.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: bitwise_or.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_xor.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: bitwise_xor.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_xor.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: bitwise_xor.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bitwise_xor_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: bitwise_xor_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: bmm": {
    "signature": "(Tensor self, Tensor mat2) -> Tensor"
  },
  "func: bmm.out": {
    "signature": "(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: bucketize.Scalar": {
    "signature": "(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor"
  },
  "func: bucketize.Tensor": {
    "signature": "(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor"
  },
  "func: bucketize.Tensor_out": {
    "signature": "(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cat": {
    "signature": "(Tensor[] tensors, int dim=0) -> Tensor"
  },
  "func: cat.names": {
    "signature": "(Tensor[] tensors, Dimname dim) -> Tensor"
  },
  "func: cat.names_out": {
    "signature": "(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cat.out": {
    "signature": "(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cdist": {
    "signature": "(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor"
  },
  "func: ceil": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: ceil.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ceil_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: complex": {
    "signature": "(Tensor real, Tensor imag) -> Tensor"
  },
  "func: complex.out": {
    "signature": "(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _coalesce": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: channel_shuffle": {
    "signature": "(Tensor self, int groups) -> Tensor"
  },
  "func: clamp": {
    "signature": "(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor"
  },
  "func: clamp.out": {
    "signature": "(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: clamp_": {
    "signature": "(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)"
  },
  "func: clamp_max": {
    "signature": "(Tensor self, Scalar max) -> Tensor"
  },
  "func: clamp_max.out": {
    "signature": "(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: clamp_max_": {
    "signature": "(Tensor(a!) self, Scalar max) -> Tensor(a!)"
  },
  "func: clamp_min": {
    "signature": "(Tensor self, Scalar min) -> Tensor"
  },
  "func: clamp_min.out": {
    "signature": "(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: clamp_min_": {
    "signature": "(Tensor(a!) self, Scalar min) -> Tensor(a!)"
  },
  "func: clamp.Tensor": {
    "signature": "(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor"
  },
  "func: clamp.Tensor_out": {
    "signature": "(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: clamp_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)"
  },
  "func: clamp_max.Tensor": {
    "signature": "(Tensor self, Tensor max) -> Tensor"
  },
  "func: clamp_max.Tensor_out": {
    "signature": "(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: clamp_max_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor max) -> Tensor(a!)"
  },
  "func: clamp_min.Tensor": {
    "signature": "(Tensor self, Tensor min) -> Tensor"
  },
  "func: clamp_min.Tensor_out": {
    "signature": "(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: clamp_min_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor min) -> Tensor(a!)"
  },
  "func: col2im": {
    "signature": "(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor"
  },
  "func: col2im.out": {
    "signature": "(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: constant_pad_nd": {
    "signature": "(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor"
  },
  "func: conv_tbc": {
    "signature": "(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor"
  },
  "func: conv_tbc_backward": {
    "signature": "(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)"
  },
  "func: conv_transpose2d.input": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor"
  },
  "func: conv_transpose3d.input": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor"
  },
  "func: convolution": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor"
  },
  "func: convolution_backward": {
    "signature": "(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: convolution_backward_overrideable": {
    "signature": "(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  },
  "func: convolution_overrideable": {
    "signature": "(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor"
  },
  "func: count_nonzero": {
    "signature": "(Tensor self, int? dim=None) -> Tensor"
  },
  "func: count_nonzero.dim_IntList": {
    "signature": "(Tensor self, int[] dim) -> Tensor"
  },
  "func: cos": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: cos.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cos_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: cosh": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: cosh.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cosh_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: ctc_loss.IntList": {
    "signature": "(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor"
  },
  "func: ctc_loss.Tensor": {
    "signature": "(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor"
  },
  "func: cumprod.dimname_out": {
    "signature": "(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cumprod.out": {
    "signature": "(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cumprod_": {
    "signature": "(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)"
  },
  "func: cumprod_.dimname": {
    "signature": "(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)"
  },
  "func: cumsum": {
    "signature": "(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: cumsum.dimname_out": {
    "signature": "(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: cumsum.out": {
    "signature": "(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: div.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: div.Scalar_mode": {
    "signature": "(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor"
  },
  "func: div.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: div.Tensor_mode": {
    "signature": "(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor"
  },
  "func: div.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: div.out_mode": {
    "signature": "(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: div_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: div_.Scalar_mode": {
    "signature": "(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)"
  },
  "func: div_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: div_.Tensor_mode": {
    "signature": "(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)"
  },
  "func: dot": {
    "signature": "(Tensor self, Tensor tensor) -> Tensor"
  },
  "func: dot.out": {
    "signature": "(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: vdot": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: vdot.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: dropout": {
    "signature": "(Tensor input, float p, bool train) -> Tensor"
  },
  "func: embedding": {
    "signature": "(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor"
  },
  "func: embedding_backward": {
    "signature": "(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor"
  },
  "func: embedding_dense_backward": {
    "signature": "(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor"
  },
  "func: embedding_renorm_": {
    "signature": "(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)"
  },
  "func: eq.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: eq.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: eq.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: eq.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: eq_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: eq_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: equal": {
    "signature": "(Tensor self, Tensor other) -> bool"
  },
  "func: erf": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: erf.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: erf_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: erfc": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: erfc.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: erfc_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: erfinv": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: erfinv.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: erfinv_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: exp": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: exp.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: exp2": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: exp2.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: exp2_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: exp_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: expm1": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: expm1.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: expm1_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: exponential_": {
    "signature": "(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: eye": {
    "signature": "(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: eye.m": {
    "signature": "(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: eye.m_out": {
    "signature": "(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: eye.out": {
    "signature": "(SymInt n, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: fill_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  },
  "func: fill_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor value) -> Tensor(a!)"
  },
  "func: fill_diagonal_": {
    "signature": "(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)"
  },
  "func: flip": {
    "signature": "(Tensor self, int[] dims) -> Tensor"
  },
  "func: floor": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: floor.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: floor_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: floor_divide": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: floor_divide.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: floor_divide.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: floor_divide_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: floor_divide_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: fmod.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: fmod.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: fmod.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: fmod.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: fmod_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: fmod_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: frac": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: frac.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: frac_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: gather": {
    "signature": "(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor"
  },
  "func: gather.dimname": {
    "signature": "(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor"
  },
  "func: gather.dimname_out": {
    "signature": "(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: gather.out": {
    "signature": "(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: gcd.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ge.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: ge.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ge.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: ge.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ge_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: ge_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: gelu": {
    "signature": "(Tensor self, *, str approximate='none') -> Tensor"
  },
  "func: gelu_backward": {
    "signature": "(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor"
  },
  "func: glu": {
    "signature": "(Tensor self, int dim=-1) -> Tensor"
  },
  "func: glu.out": {
    "signature": "(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: glu_backward": {
    "signature": "(Tensor grad_output, Tensor self, int dim) -> Tensor"
  },
  "func: glu_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: grid_sampler_2d": {
    "signature": "(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor"
  },
  "func: grid_sampler_2d_backward": {
    "signature": "(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)"
  },
  "func: grid_sampler_3d": {
    "signature": "(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor"
  },
  "func: grid_sampler_3d_backward": {
    "signature": "(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)"
  },
  "func: gru.input": {
    "signature": "(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)"
  },
  "func: gt.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: gt.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: gt.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: gt.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: gt_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: gt_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: hardshrink": {
    "signature": "(Tensor self, Scalar lambd=0.5) -> Tensor"
  },
  "func: hardshrink.out": {
    "signature": "(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: hardshrink_backward": {
    "signature": "(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor"
  },
  "func: hardshrink_backward.grad_input": {
    "signature": "(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: hardsigmoid": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: hardsigmoid.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: hardsigmoid_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: hardsigmoid_backward": {
    "signature": "(Tensor grad_output, Tensor self) -> Tensor"
  },
  "func: hardswish": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: hardswish.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: hardswish_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: hardswish_backward": {
    "signature": "(Tensor grad_output, Tensor self) -> Tensor"
  },
  "func: hardtanh": {
    "signature": "(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor"
  },
  "func: hardtanh.out": {
    "signature": "(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: hardtanh_": {
    "signature": "(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)"
  },
  "func: hardtanh_backward": {
    "signature": "(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor"
  },
  "func: hardtanh_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: im2col": {
    "signature": "(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor"
  },
  "func: im2col.out": {
    "signature": "(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: index.Tensor": {
    "signature": "(Tensor self, Tensor?[] indices) -> Tensor"
  },
  "func: index_add": {
    "signature": "(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor"
  },
  "func: index_add.dimname": {
    "signature": "(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor"
  },
  "func: index_add.out": {
    "signature": "(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: index_fill.int_Scalar": {
    "signature": "(Tensor self, int dim, Tensor index, Scalar value) -> Tensor"
  },
  "func: index_fill.int_Tensor": {
    "signature": "(Tensor self, int dim, Tensor index, Tensor value) -> Tensor"
  },
  "func: index_fill_.int_Scalar": {
    "signature": "(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)"
  },
  "func: index_fill_.int_Tensor": {
    "signature": "(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)"
  },
  "func: index_put": {
    "signature": "(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor"
  },
  "func: index_put_": {
    "signature": "(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)"
  },
  "func: index_select": {
    "signature": "(Tensor self, int dim, Tensor index) -> Tensor"
  },
  "func: index_select.dimname": {
    "signature": "(Tensor self, Dimname dim, Tensor index) -> Tensor"
  },
  "func: index_select.dimname_out": {
    "signature": "(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: index_select.out": {
    "signature": "(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: inverse": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: inverse.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: isin.Tensor_Scalar": {
    "signature": "(Tensor element, Scalar test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor"
  },
  "func: isin.Tensor_Scalar_out": {
    "signature": "(Tensor element, Scalar test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: isclose": {
    "signature": "(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor"
  },
  "func: isfinite": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: isin.Scalar_Tensor_out": {
    "signature": "(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: isposinf.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: isneginf.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: kthvalue": {
    "signature": "(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: kthvalue.dimname": {
    "signature": "(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: kthvalue.dimname_out": {
    "signature": "(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: kthvalue.values": {
    "signature": "(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: le.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: le.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: le.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: le.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: le_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: le_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: leaky_relu": {
    "signature": "(Tensor self, Scalar negative_slope=0.01) -> Tensor"
  },
  "func: leaky_relu.out": {
    "signature": "(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: leaky_relu_": {
    "signature": "(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)"
  },
  "func: leaky_relu_backward": {
    "signature": "(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor"
  },
  "func: leaky_relu_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: lerp.Scalar": {
    "signature": "(Tensor self, Tensor end, Scalar weight) -> Tensor"
  },
  "func: lerp.Scalar_out": {
    "signature": "(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: lerp.Tensor": {
    "signature": "(Tensor self, Tensor end, Tensor weight) -> Tensor"
  },
  "func: lerp.Tensor_out": {
    "signature": "(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: lerp_.Scalar": {
    "signature": "(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)"
  },
  "func: lerp_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)"
  },
  "func: linalg_cross": {
    "signature": "(Tensor self, Tensor other, *, int dim=-1) -> Tensor"
  },
  "func: linalg_cross.out": {
    "signature": "(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linalg_qr": {
    "signature": "(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)"
  },
  "func: linalg_qr.out": {
    "signature": "(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)"
  },
  "func: linalg_solve_triangular": {
    "signature": "(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor"
  },
  "func: linalg_solve_triangular.out": {
    "signature": "(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linalg_svdvals": {
    "signature": "(Tensor A, *, str? driver=None) -> Tensor"
  },
  "func: linalg_svdvals.out": {
    "signature": "(Tensor A, *, str? driver=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linspace": {
    "signature": "(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: linspace.out": {
    "signature": "(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linalg_norm": {
    "signature": "(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: linalg_norm.ord_str": {
    "signature": "(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: linalg_norm.out": {
    "signature": "(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linalg_norm.ord_str_out": {
    "signature": "(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: log": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: log.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: log10": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: log10.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: log10_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: log1p": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: log1p.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: log1p_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: log2": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: log2.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: log2_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: log_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: log_sigmoid": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: log_sigmoid.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: log_sigmoid_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor"
  },
  "func: log_sigmoid_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: log_sigmoid_forward": {
    "signature": "(Tensor self) -> (Tensor output, Tensor buffer)"
  },
  "func: log_sigmoid_forward.output": {
    "signature": "(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))"
  },
  "func: log_softmax.Dimname": {
    "signature": "(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: log_softmax.int": {
    "signature": "(Tensor self, int dim, ScalarType? dtype=None) -> Tensor"
  },
  "func: logaddexp": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: logaddexp.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logaddexp2": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: logaddexp2.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logical_and": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: logical_and.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logical_and_": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: logical_not": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: logical_not.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logical_not_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: logical_or": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: logical_or.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logical_or_": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: logical_xor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: logical_xor.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logspace": {
    "signature": "(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: logspace.out": {
    "signature": "(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logsumexp": {
    "signature": "(Tensor self, int[1] dim, bool keepdim=False) -> Tensor"
  },
  "func: logsumexp.names": {
    "signature": "(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor"
  },
  "func: logsumexp.names_out": {
    "signature": "(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: logsumexp.out": {
    "signature": "(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: lstm.data": {
    "signature": "(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)"
  },
  "func: lstm.input": {
    "signature": "(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)"
  },
  "func: lstm_cell": {
    "signature": "(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)"
  },
  "func: lt.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: lt.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: lt.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: lt.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: lt_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: lt_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: masked_fill_.Scalar": {
    "signature": "(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)"
  },
  "func: masked_fill_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)"
  },
  "func: masked_scatter_": {
    "signature": "(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)"
  },
  "func: masked_select": {
    "signature": "(Tensor self, Tensor mask) -> Tensor"
  },
  "func: masked_select.out": {
    "signature": "(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: matmul": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: matmul_backward": {
    "signature": "(Tensor grad, Tensor self, Tensor other, bool[2] mask) -> (Tensor, Tensor)"
  },
  "func: matmul.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: max": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: max.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: max.dim_max": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: max.names_dim": {
    "signature": "(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: max.names_dim_max": {
    "signature": "(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: max.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: max_pool2d_with_indices": {
    "signature": "(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)"
  },
  "func: max_pool2d_with_indices.out": {
    "signature": "(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  "func: max_pool2d_with_indices_backward": {
    "signature": "(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor"
  },
  "func: max_pool2d_with_indices_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: max_pool3d_with_indices": {
    "signature": "(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)"
  },
  "func: max_pool3d_with_indices.out": {
    "signature": "(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  },
  "func: max_pool3d_with_indices_backward": {
    "signature": "(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor"
  },
  "func: max_pool3d_with_indices_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: max_unpool2d": {
    "signature": "(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor"
  },
  "func: max_unpool2d.out": {
    "signature": "(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: max_unpool3d": {
    "signature": "(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor"
  },
  "func: max_unpool3d.out": {
    "signature": "(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: maximum": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: maximum.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mean": {
    "signature": "(Tensor self, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: mean.dim": {
    "signature": "(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: mean.names_dim": {
    "signature": "(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: mean.names_out": {
    "signature": "(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mean.out": {
    "signature": "(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: median": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: median.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: median.dim_values": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: min": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: min.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: min.dim_min": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: min.names_dim": {
    "signature": "(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: min.names_dim_min": {
    "signature": "(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: min.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: minimum": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: minimum.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mish": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: mish.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mish_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: mish_backward": {
    "signature": "(Tensor grad_output, Tensor self) -> Tensor"
  },
  "func: mm": {
    "signature": "(Tensor self, Tensor mat2) -> Tensor"
  },
  "func: mm.out": {
    "signature": "(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mse_loss": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean) -> Tensor"
  },
  "func: mse_loss.out": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mse_loss_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
  },
  "func: mse_loss_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: mul.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: mul.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: mul.out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mul_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: mul_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: multilabel_margin_loss": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean) -> Tensor"
  },
  "func: multilabel_margin_loss.out": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: multilabel_margin_loss_forward": {
    "signature": "(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)"
  },
  "func: multilabel_margin_loss_forward.output": {
    "signature": "(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))"
  },
  "func: multinomial": {
    "signature": "(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  },
  "func: multinomial.out": {
    "signature": "(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: mv": {
    "signature": "(Tensor self, Tensor vec) -> Tensor"
  },
  "func: mv.out": {
    "signature": "(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: nanmedian": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: nanmedian.dim": {
    "signature": "(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)"
  },
  "func: nan_to_num": {
    "signature": "(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor"
  },
  "func: nan_to_num_": {
    "signature": "(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)"
  },
  "func: nan_to_num.out": {
    "signature": "(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linalg_vector_norm": {
    "signature": "(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: linalg_vector_norm.out": {
    "signature": "(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: native_batch_norm": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  },
  "func: native_batch_norm_backward": {
    "signature": "(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: native_batch_norm.out": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  },
  "func: native_dropout": {
    "signature": "(Tensor input, float p, bool? train) -> (Tensor, Tensor)"
  },
  "func: native_dropout_backward": {
    "signature": "(Tensor grad_output, Tensor mask, float scale) -> Tensor"
  },
  "func: native_layer_norm": {
    "signature": "(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)"
  },
  "func: native_layer_norm_backward": {
    "signature": "(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: ne.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: ne.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ne.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: ne.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ne_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: ne_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: neg": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: neg.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: neg_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: nll_loss": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor"
  },
  "func: nll_loss.out": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: nll_loss2d": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor"
  },
  "func: nll_loss2d.out": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: nll_loss2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor"
  },
  "func: nll_loss2d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: nll_loss2d_forward": {
    "signature": "(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)"
  },
  "func: nll_loss2d_forward.output": {
    "signature": "(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))"
  },
  "func: nll_loss_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor"
  },
  "func: nll_loss_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: nll_loss_forward": {
    "signature": "(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)"
  },
  "func: nll_loss_forward.output": {
    "signature": "(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))"
  },
  "func: nonzero": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: nonzero.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: norm.Scalar": {
    "signature": "(Tensor self, Scalar p=2) -> Tensor"
  },
  "func: norm.ScalarOpt_dim": {
    "signature": "(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor"
  },
  "func: norm.ScalarOpt_dim_dtype": {
    "signature": "(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor"
  },
  "func: norm.ScalarOpt_dtype": {
    "signature": "(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor"
  },
  "func: norm.dtype_out": {
    "signature": "(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: norm.out": {
    "signature": "(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: normal.Tensor_Tensor": {
    "signature": "(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor"
  },
  "func: normal.Tensor_Tensor_out": {
    "signature": "(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: normal.Tensor_float": {
    "signature": "(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor"
  },
  "func: normal.Tensor_float_out": {
    "signature": "(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: normal.float_Tensor": {
    "signature": "(float mean, Tensor std, *, Generator? generator=None) -> Tensor"
  },
  "func: normal.float_Tensor_out": {
    "signature": "(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: normal.float_float": {
    "signature": "(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: normal.float_float_out": {
    "signature": "(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: normal_": {
    "signature": "(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: one_hot": {
    "signature": "(Tensor self, int num_classes=-1) -> Tensor"
  },
  "func: ones": {
    "signature": "(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: ones.names": {
    "signature": "(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: ones.out": {
    "signature": "(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: ones_like": {
    "signature": "(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  "func: pdist": {
    "signature": "(Tensor self, float p=2) -> Tensor"
  },
  "func: pow.Scalar": {
    "signature": "(Scalar self, Tensor exponent) -> Tensor"
  },
  "func: pow.Scalar_out": {
    "signature": "(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: pow.Tensor_Scalar": {
    "signature": "(Tensor self, Scalar exponent) -> Tensor"
  },
  "func: pow.Tensor_Scalar_out": {
    "signature": "(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: pow.Tensor_Tensor": {
    "signature": "(Tensor self, Tensor exponent) -> Tensor"
  },
  "func: pow.Tensor_Tensor_out": {
    "signature": "(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: pow_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar exponent) -> Tensor(a!)"
  },
  "func: pow_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor exponent) -> Tensor(a!)"
  },
  "func: _prelu_kernel": {
    "signature": "(Tensor self, Tensor weight) -> Tensor"
  },
  "func: _prelu_kernel_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)"
  },
  "func: prod": {
    "signature": "(Tensor self, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: prod.dim_int": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: prod.int_out": {
    "signature": "(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: put_": {
    "signature": "(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)"
  },
  "func: quantize_per_channel": {
    "signature": "(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor"
  },
  "func: quantize_per_tensor": {
    "signature": "(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor"
  },
  "func: random_": {
    "signature": "(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: random_.from": {
    "signature": "(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: random_.to": {
    "signature": "(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: randperm": {
    "signature": "(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: randperm.generator": {
    "signature": "(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: randperm.generator_out": {
    "signature": "(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: randperm.out": {
    "signature": "(SymInt n, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: range": {
    "signature": "(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: range.out": {
    "signature": "(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: range.step": {
    "signature": "(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: reciprocal": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: reciprocal.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: reciprocal_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: reflection_pad1d": {
    "signature": "(Tensor self, SymInt[2] padding) -> Tensor"
  },
  "func: reflection_pad1d.out": {
    "signature": "(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: reflection_pad1d_backward": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor"
  },
  "func: reflection_pad1d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: reflection_pad2d": {
    "signature": "(Tensor self, SymInt[4] padding) -> Tensor"
  },
  "func: reflection_pad2d.out": {
    "signature": "(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: reflection_pad2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor"
  },
  "func: reflection_pad2d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: reflection_pad3d": {
    "signature": "(Tensor self, SymInt[6] padding) -> Tensor"
  },
  "func: reflection_pad3d.out": {
    "signature": "(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: reflection_pad3d_backward": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor"
  },
  "func: reflection_pad3d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: relu": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: relu_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: remainder.Scalar": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: remainder.Scalar_out": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: remainder.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: remainder.Tensor_out": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: remainder_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: remainder_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: remainder.Scalar_Tensor": {
    "signature": "(Scalar self, Tensor other) -> Tensor"
  },
  "func: renorm": {
    "signature": "(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor"
  },
  "func: renorm.out": {
    "signature": "(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: renorm_": {
    "signature": "(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)"
  },
  "func: repeat": {
    "signature": "(Tensor self, SymInt[] "
  },
  "func: repeat_interleave.self_Tensor": {
    "signature": "(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor"
  },
  "func: repeat_interleave.self_int": {
    "signature": "(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor"
  },
  "func: replication_pad1d": {
    "signature": "(Tensor self, SymInt[2] padding) -> Tensor"
  },
  "func: replication_pad1d.out": {
    "signature": "(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: replication_pad1d_backward": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor"
  },
  "func: replication_pad1d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: replication_pad2d": {
    "signature": "(Tensor self, SymInt[4] padding) -> Tensor"
  },
  "func: replication_pad2d.out": {
    "signature": "(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: replication_pad2d_backward": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor"
  },
  "func: replication_pad2d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: replication_pad3d": {
    "signature": "(Tensor self, SymInt[6] padding) -> Tensor"
  },
  "func: replication_pad3d.out": {
    "signature": "(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: replication_pad3d_backward": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor"
  },
  "func: replication_pad3d_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: roll": {
    "signature": "(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor"
  },
  "func: round": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: round.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: round_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: round.decimals": {
    "signature": "(Tensor self, *, int decimals) -> Tensor"
  },
  "func: round_.decimals": {
    "signature": "(Tensor(a!) self, *, int decimals) -> Tensor(a!)"
  },
  "func: round.decimals_out": {
    "signature": "(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: rrelu_with_noise": {
    "signature": "(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor"
  },
  "func: rrelu_with_noise.out": {
    "signature": "(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: rrelu_with_noise_": {
    "signature": "(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)"
  },
  "func: rsqrt": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: rsqrt.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: rsqrt_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: rsub.Scalar": {
    "signature": "(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  "func: rsub.Tensor": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  "func: scaled_dot_product_attention": {
    "signature": "(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> Tensor"
  },
  "func: scatter_.src": {
    "signature": "(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)"
  },
  "func: scatter_.value": {
    "signature": "(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)"
  },
  "func: scatter.src_out": {
    "signature": "(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: scatter.value_out": {
    "signature": "(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: scatter_add": {
    "signature": "(Tensor self, int dim, Tensor index, Tensor src) -> Tensor"
  },
  "func: scatter_add.dimname": {
    "signature": "(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor"
  },
  "func: scatter_add_": {
    "signature": "(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)"
  },
  "func: searchsorted.Scalar": {
    "signature": "(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor"
  },
  "func: searchsorted.Tensor": {
    "signature": "(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor"
  },
  "func: searchsorted.Tensor_out": {
    "signature": "(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sgn": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sgn.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sigmoid": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sigmoid.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sigmoid_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: sigmoid_backward": {
    "signature": "(Tensor grad_output, Tensor output) -> Tensor"
  },
  "func: sigmoid_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: sign": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sign.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sign_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: sin": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sin.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sin_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: sinc": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sinc.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sinc_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: sinh": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sinh.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sinh_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: slogdet": {
    "signature": "(Tensor self) -> (Tensor sign, Tensor logabsdet)"
  },
  "func: slow_conv3d": {
    "signature": "(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor"
  },
  "func: slow_conv3d.out": {
    "signature": "(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: slow_conv3d_forward": {
    "signature": "(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor"
  },
  "func: slow_conv3d_forward.output": {
    "signature": "(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)"
  },
  "func: slow_conv_dilated2d": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor"
  },
  "func: slow_conv_transpose2d": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor"
  },
  "func: slow_conv_transpose2d.out": {
    "signature": "(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: smooth_l1_loss": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor"
  },
  "func: smooth_l1_loss.out": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: smooth_l1_loss_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor"
  },
  "func: smooth_l1_loss_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: soft_margin_loss": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean) -> Tensor"
  },
  "func: soft_margin_loss.out": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: soft_margin_loss_backward": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
  },
  "func: soft_margin_loss_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: softmax.Dimname": {
    "signature": "(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: softmax.int": {
    "signature": "(Tensor self, int dim, ScalarType? dtype=None) -> Tensor"
  },
  "func: softplus": {
    "signature": "(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor"
  },
  "func: softplus.out": {
    "signature": "(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: softplus_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: softshrink": {
    "signature": "(Tensor self, Scalar lambd=0.5) -> Tensor"
  },
  "func: softshrink.out": {
    "signature": "(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: softshrink_backward": {
    "signature": "(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor"
  },
  "func: softshrink_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: sort": {
    "signature": "(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  "func: sort.dimname": {
    "signature": "(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  "func: sort.dimname_values": {
    "signature": "(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: sort.values": {
    "signature": "(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: sort.stable": {
    "signature": "(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  },
  "func: sort.values_stable": {
    "signature": "(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: sqrt": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: sqrt.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sqrt_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: stack": {
    "signature": "(Tensor[] tensors, int dim=0) -> Tensor"
  },
  "func: stack.out": {
    "signature": "(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: std.correction": {
    "signature": "(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor"
  },
  "func: std.correction_out": {
    "signature": "(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: std_mean.correction": {
    "signature": "(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)"
  },
  "func: stft": {
    "signature": "(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor"
  },
  "func: sub.Scalar": {
    "signature": "(Tensor self, Scalar other, Scalar alpha=1) -> Tensor"
  },
  "func: sub.Tensor": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor"
  },
  "func: sub.out": {
    "signature": "(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sub_.Scalar": {
    "signature": "(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: sub_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)"
  },
  "func: sum": {
    "signature": "(Tensor self, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: sum.DimnameList_out": {
    "signature": "(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sum.IntList_out": {
    "signature": "(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: sum.dim_DimnameList": {
    "signature": "(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: sum.dim_IntList": {
    "signature": "(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: nansum": {
    "signature": "(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: nansum.out": {
    "signature": "(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: take": {
    "signature": "(Tensor self, Tensor index) -> Tensor"
  },
  "func: take.out": {
    "signature": "(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: tan": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: tan.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: tan_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: tanh": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: tanh.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: tanh_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: tanh_backward": {
    "signature": "(Tensor grad_output, Tensor output) -> Tensor"
  },
  "func: tanh_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: threshold": {
    "signature": "(Tensor self, Scalar "
  },
  "func: threshold.out": {
    "signature": "(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: threshold_": {
    "signature": "(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)"
  },
  "func: threshold_backward": {
    "signature": "(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor"
  },
  "func: topk": {
    "signature": "(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)"
  },
  "func: topk.values": {
    "signature": "(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  },
  "func: trace": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: triangular_solve.X": {
    "signature": "(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)"
  },
  "func: tril": {
    "signature": "(Tensor self, int diagonal=0) -> Tensor"
  },
  "func: tril.out": {
    "signature": "(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: tril_": {
    "signature": "(Tensor(a!) self, int diagonal=0) -> Tensor(a!)"
  },
  "func: triu": {
    "signature": "(Tensor self, int diagonal=0) -> Tensor"
  },
  "func: triu.out": {
    "signature": "(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: triu_": {
    "signature": "(Tensor(a!) self, int diagonal=0) -> Tensor(a!)"
  },
  "func: trunc": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: trunc.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: trunc_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: uniform_": {
    "signature": "(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)"
  },
  "func: unique_consecutive": {
    "signature": "(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)"
  },
  "func: unique_dim": {
    "signature": "(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)"
  },
  "func: upsample_bicubic2d": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_bicubic2d.out": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_bicubic2d_backward": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_bicubic2d_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: upsample_bilinear2d": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_bilinear2d.out": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_bilinear2d_backward": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_bilinear2d_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: upsample_linear1d": {
    "signature": "(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor"
  },
  "func: upsample_linear1d.out": {
    "signature": "(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_linear1d_backward": {
    "signature": "(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor"
  },
  "func: upsample_nearest1d": {
    "signature": "(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor"
  },
  "func: upsample_nearest1d.out": {
    "signature": "(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_nearest1d_backward": {
    "signature": "(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor"
  },
  "func: upsample_nearest1d_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: upsample_nearest2d": {
    "signature": "(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_nearest2d.out": {
    "signature": "(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_nearest2d_backward": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_nearest2d_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: upsample_nearest3d": {
    "signature": "(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_nearest3d.out": {
    "signature": "(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_nearest3d_backward": {
    "signature": "(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_nearest3d_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: upsample_trilinear3d": {
    "signature": "(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_trilinear3d.out": {
    "signature": "(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: upsample_trilinear3d_backward": {
    "signature": "(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: upsample_trilinear3d_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: var.correction": {
    "signature": "(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor"
  },
  "func: var.correction_out": {
    "signature": "(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: var_mean.correction": {
    "signature": "(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)"
  },
  "func: _weight_norm": {
    "signature": "(Tensor v, Tensor g, int dim=0) -> Tensor"
  },
  "func: where": {
    "signature": "(Tensor condition) -> Tensor[]"
  },
  "func: where.self": {
    "signature": "(Tensor condition, Tensor self, Tensor other) -> Tensor"
  },
  "func: where.self_out": {
    "signature": "(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: xlogy.OutScalar_Other": {
    "signature": "(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: xlogy.OutScalar_Self": {
    "signature": "(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: xlogy.OutTensor": {
    "signature": "(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: xlogy.Scalar_Other": {
    "signature": "(Tensor self, Scalar other) -> Tensor"
  },
  "func: xlogy.Scalar_Self": {
    "signature": "(Scalar self, Tensor other) -> Tensor"
  },
  "func: xlogy.Tensor": {
    "signature": "(Tensor self, Tensor other) -> Tensor"
  },
  "func: xlogy_.Scalar_Other": {
    "signature": "(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  },
  "func: xlogy_.Tensor": {
    "signature": "(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  },
  "func: zero_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: zeros": {
    "signature": "(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: zeros.names": {
    "signature": "(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
  },
  "func: zeros.out": {
    "signature": "(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: zeros_like": {
    "signature": "(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
  },
  "func: celu": {
    "signature": "(Tensor self, Scalar alpha=1.0) -> Tensor"
  },
  "func: celu_": {
    "signature": "(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)"
  },
  "func: elu.out": {
    "signature": "(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: elu": {
    "signature": "(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor"
  },
  "func: elu_": {
    "signature": "(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)"
  },
  "func: elu_backward.grad_input": {
    "signature": "(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: elu_backward": {
    "signature": "(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor"
  },
  "func: silu": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: silu_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: silu.out": {
    "signature": "(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: silu_backward.grad_input": {
    "signature": "(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: silu_backward": {
    "signature": "(Tensor grad_output, Tensor self) -> Tensor"
  },
  "func: binary_cross_entropy_with_logits": {
    "signature": "(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor"
  },
  "func: selu": {
    "signature": "(Tensor self) -> Tensor"
  },
  "func: selu_": {
    "signature": "(Tensor(a!) self) -> Tensor(a!)"
  },
  "func: linalg_matrix_norm": {
    "signature": "(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: linalg_matrix_norm.out": {
    "signature": "(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: linalg_matrix_norm.str_ord": {
    "signature": "(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
  },
  "func: linalg_matrix_norm.str_ord_out": {
    "signature": "(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: l1_loss": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean) -> Tensor"
  },
  "func: kl_div": {
    "signature": "(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor"
  },
  "func: histc": {
    "signature": "(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor"
  },
  "func: histc.out": {
    "signature": "(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: index_copy_": {
    "signature": "(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)"
  },
  "func: index_copy": {
    "signature": "(Tensor self, int dim, Tensor index, Tensor source) -> Tensor"
  },
  "func: _foreach_maximum.Scalar": {
    "signature": "(Tensor[] self, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_maximum_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_maximum.ScalarList": {
    "signature": "(Tensor[] self, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_maximum_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_minimum.Scalar": {
    "signature": "(Tensor[] self, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_minimum_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_minimum.ScalarList": {
    "signature": "(Tensor[] self, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_minimum_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_sqrt": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_sqrt_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_ceil_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_floor_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_trunc_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_round_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_ceil": {
    "signature": "(Tensor(a!)[] self) -> Tensor[]"
  },
  "func: _foreach_floor": {
    "signature": "(Tensor(a!)[] self) -> Tensor[]"
  },
  "func: _foreach_trunc": {
    "signature": "(Tensor(a!)[] self) -> Tensor[]"
  },
  "func: _foreach_round": {
    "signature": "(Tensor(a!)[] self) -> Tensor[]"
  },
  "func: _foreach_frac_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_frac": {
    "signature": "(Tensor(a!)[] self) -> Tensor[]"
  },
  "func: _foreach_mul_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_sub_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_sub.Scalar": {
    "signature": "(Tensor[] self, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_div_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_div.Scalar": {
    "signature": "(Tensor[] self, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_add.List": {
    "signature": "(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]"
  },
  "func: _foreach_add_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()"
  },
  "func: _foreach_sub.List": {
    "signature": "(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]"
  },
  "func: _foreach_sub_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()"
  },
  "func: _foreach_mul.List": {
    "signature": "(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]"
  },
  "func: _foreach_mul_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other) -> ()"
  },
  "func: _foreach_div.List": {
    "signature": "(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]"
  },
  "func: _foreach_div_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other) -> ()"
  },
  "func: _foreach_add.ScalarList": {
    "signature": "(Tensor[] tensors, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_add_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_sub.ScalarList": {
    "signature": "(Tensor[] tensors, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_sub_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_div.ScalarList": {
    "signature": "(Tensor[] tensors, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_div_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_mul.ScalarList": {
    "signature": "(Tensor[] tensors, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_mul_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_mul.Scalar": {
    "signature": "(Tensor[] tensors, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_sigmoid": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_sigmoid_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_add.Scalar": {
    "signature": "(Tensor[] tensors, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_add_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_addcdiv_.Scalar": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()"
  },
  "func: _foreach_addcdiv.Scalar": {
    "signature": "(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]"
  },
  "func: _foreach_addcmul_.Scalar": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()"
  },
  "func: _foreach_addcmul.Scalar": {
    "signature": "(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]"
  },
  "func: _foreach_cos_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_cos": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_pow.Scalar": {
    "signature": "(Tensor[] tensors, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_pow_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_maximum.List": {
    "signature": "(Tensor[] self, Tensor[] other) -> Tensor[]"
  },
  "func: _foreach_minimum.List": {
    "signature": "(Tensor[] self, Tensor[] other) -> Tensor[]"
  },
  "func: _foreach_maximum_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other) -> ()"
  },
  "func: _foreach_minimum_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other) -> ()"
  },
  "func: _foreach_pow.List": {
    "signature": "(Tensor[] self, Tensor[] exponent) -> Tensor[]"
  },
  "func: _foreach_pow_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] exponent) -> ()"
  },
  "func: _foreach_pow.ScalarList": {
    "signature": "(Tensor[] self, Scalar[] exponent) -> Tensor[]"
  },
  "func: _foreach_pow_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] exponent) -> ()"
  },
  "func: _foreach_addcdiv_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()"
  },
  "func: _foreach_addcdiv.ScalarList": {
    "signature": "(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_addcmul_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()"
  },
  "func: _foreach_addcmul.ScalarList": {
    "signature": "(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_addcdiv.Tensor": {
    "signature": "(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]"
  },
  "func: _foreach_addcdiv_.Tensor": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()"
  },
  "func: _foreach_addcmul.Tensor": {
    "signature": "(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]"
  },
  "func: _foreach_addcmul_.Tensor": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()"
  },
  "func: _foreach_exp_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_exp": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_neg": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_neg_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_abs_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_abs": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: native_group_norm": {
    "signature": "(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)"
  },
  "func: native_group_norm_backward": {
    "signature": "(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  },
  "func: view_as_real": {
    "signature": "(Tensor(a) self) -> Tensor(a)"
  },
  "func: view_as_complex": {
    "signature": "(Tensor(a) self) -> Tensor(a)"
  },
  "func: _amp_update_scale_": {
    "signature": "(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float growth_factor, float backoff_factor, int growth_interval) -> Tensor(a!)"
  },
  "func: polar": {
    "signature": "(Tensor abs, Tensor angle) -> Tensor"
  },
  "func: polar.out": {
    "signature": "(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _npu_silent_check_v2": {
    "signature": "(Tensor val, Tensor(a!) input_grad, Tensor(b!) sfda, Tensor(c!) step, int c_min_steps, float c_thresh_l1, float c_coeff_l1, float c_thresh_l2, float c_coeff_l2, int npu_asd_detect) -> Tensor"
  },
  "func: npu_dynamic_quant": {
    "signature": "(Tensor input, *, Tensor? smooth_scales=None) -> (Tensor, Tensor)"
  },
  "func: npu_dynamic_quant_asymmetric": {
    "signature": "(Tensor input, *, Tensor? smooth_scales=None, Tensor? group_index=None, ScalarType? dst_type=None) -> (Tensor, Tensor, Tensor)"
  },
  "func: npu_fused_infer_attention_score": {
    "signature": "(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout=\"BSH\", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> (Tensor, Tensor)"
  },
  "func: npu_prefetch": {
    "signature": "(Tensor self, Tensor? dependency, int max_size, int offset=0) -> ()"
  },
  "func: npu_group_quant": {
    "signature": "(Tensor x, Tensor scale, Tensor group_index, *, Tensor? offset=None, ScalarType? dst_dtype=None) -> Tensor"
  },
  "func: stft_backward": {
    "signature": "(Tensor grad_output, Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor"
  },
  "func: fft_r2c_backward": {
    "signature": "(Tensor grad, int[] dim, int normalization, bool onesided, int last_dim_size) -> Tensor"
  },
  "func: fft_c2r_backward": {
    "signature": "(Tensor grad, int[] dim, int normalization) -> Tensor"
  },
  "func: _fft_c2c": {
    "signature": "(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor"
  },
  "func: _fft_c2c.out": {
    "signature": "(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _fft_c2r": {
    "signature": "(Tensor self, int[] dim, int normalization, SymInt last_dim_size) -> Tensor"
  },
  "func: _fft_c2r.out": {
    "signature": "(Tensor self, int[] dim, int normalization, SymInt last_dim_size, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _fft_r2c": {
    "signature": "(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor"
  },
  "func: _fft_r2c.out": {
    "signature": "(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _foreach_norm.Scalar": {
    "signature": "(Tensor[] tensors, Scalar ord=2, ScalarType? dtype=None) -> Tensor[]"
  },
  "func: _foreach_copy_": {
    "signature": "(Tensor(a!)[] self, Tensor[] src, bool non_blocking=False) -> ()"
  },
  "func: _foreach_sign": {
    "signature": "(Tensor[] self) -> Tensor[]"
  },
  "func: _foreach_sign_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_pow.ScalarAndTensor": {
    "signature": "(Scalar self, Tensor[] exponent) -> Tensor[]"
  },
  "func: _foreach_reciprocal_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_reciprocal": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_expm1_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_expm1": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_erf": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_erf_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_erfc": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_erfc_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_asin": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_asin_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_sin_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_sin": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_sinh_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_sinh": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_acos_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_acos": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_cosh_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_cosh": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_atan_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_atan": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_tan_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_tan": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_tanh": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_tanh_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_clamp_max.ScalarList": {
    "signature": "(Tensor[] self, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_clamp_max_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_clamp_max.List": {
    "signature": "(Tensor[] self, Tensor[] other) -> Tensor[]"
  },
  "func: _foreach_clamp_max_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other) -> ()"
  },
  "func: _foreach_clamp_max.Scalar": {
    "signature": "(Tensor[] self, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_clamp_max_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_clamp_min.ScalarList": {
    "signature": "(Tensor[] self, Scalar[] scalars) -> Tensor[]"
  },
  "func: _foreach_clamp_min_.ScalarList": {
    "signature": "(Tensor(a!)[] self, Scalar[] scalars) -> ()"
  },
  "func: _foreach_clamp_min.List": {
    "signature": "(Tensor[] self, Tensor[] other) -> Tensor[]"
  },
  "func: _foreach_clamp_min_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] other) -> ()"
  },
  "func: _foreach_clamp_min.Scalar": {
    "signature": "(Tensor[] self, Scalar scalar) -> Tensor[]"
  },
  "func: _foreach_clamp_min_.Scalar": {
    "signature": "(Tensor(a!)[] self, Scalar scalar) -> ()"
  },
  "func: _foreach_lerp_.Scalar": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -> ()"
  },
  "func: _foreach_lerp.Scalar": {
    "signature": "(Tensor[] self, Tensor[] tensors1, Scalar weight) -> Tensor[]"
  },
  "func: _foreach_lerp_.List": {
    "signature": "(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -> ()"
  },
  "func: _foreach_lerp.List": {
    "signature": "(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -> Tensor[]"
  },
  "func: _foreach_log1p_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_log1p": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_log_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_log": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_log10_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_log10": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_log2_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _foreach_log2": {
    "signature": "(Tensor[] tensors) -> Tensor[]"
  },
  "func: _foreach_zero_": {
    "signature": "(Tensor(a!)[] self) -> ()"
  },
  "func: _unique": {
    "signature": "(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)"
  },
  "func: argsort.stable": {
    "signature": "(Tensor self, bool stable=False, int dim=-1, bool descending=False) -> Tensor"
  },
  "func: dropout_": {
    "signature": "(Tensor(a!) self, float p, bool train) -> Tensor(a!)"
  },
  "func: fft_fftshift": {
    "signature": "(Tensor self, int[1]? dim=None) -> Tensor"
  },
  "func: fft_ifftshift": {
    "signature": "(Tensor self, int[1]? dim=None) -> Tensor"
  },
  "func: _upsample_bicubic2d_aa_backward": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: _upsample_bicubic2d_aa_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: _upsample_bicubic2d_aa": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: _upsample_bicubic2d_aa.out": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _upsample_bilinear2d_aa": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: _upsample_bilinear2d_aa.out": {
    "signature": "(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _upsample_bilinear2d_aa_backward": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor"
  },
  "func: _upsample_bilinear2d_aa_backward.grad_input": {
    "signature": "(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)"
  },
  "func: isin.Tensor_Tensor": {
    "signature": "(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor"
  },
  "func: isin.Tensor_Tensor_out": {
    "signature": "(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)"
  },
  "func: _fused_adamw_": {
    "signature": "(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()"
  },
  "torch_c_func: torch_npu::init_npu(const c10::DeviceIndex device_index = 0)": {
    "signature": "(const c10::DeviceIndex device_index = 0) -> void",
    "file": "torch_npu/csrc/libs/init_npu.h"
  },
  "torch_c_func: torch_npu::init_npu(const std::string& device_str)": {
    "signature": "(const std::string& device_str) -> void",
    "file": "torch_npu/csrc/libs/init_npu.h"
  },
  "torch_c_func: torch_npu::init_npu(const at::Device& device)": {
    "signature": "(const at::Device& device) -> void",
    "file": "torch_npu/csrc/libs/init_npu.h"
  },
  "torch_c_func: torch_npu::finalize_npu": {
    "signature": "() -> void",
    "file": "torch_npu/csrc/libs/init_npu.h"
  },
  "torch_c_func: torch::npu::synchronize": {
    "signature": "(int64_t device_index = -1) -> void",
    "file": "torch_npu/csrc/libs/init_npu.h"
  },
  "torch_c_func: c10::npu::current_device": {
    "signature": "() -> DeviceIndex",
    "file": "torch_npu/csrc/libs/init_npu.h"
  },
  "torch_c_func: c10_npu::NPUEvent::NPUEvent()": {
    "signature": "()",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::NPUEvent(unsigned int flags)": {
    "signature": "(unsigned int flags)",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::~NPUEvent()": {
    "signature": "()",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::NPUEvent(c10_npu::NPUEvent&& other)": {
    "signature": "(NPUEvent&& other)",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::aclrtEvent": {
    "signature": "() -> operator",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::device": {
    "signature": "() -> c10::optional<at::Device>",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::isCreated": {
    "signature": "() -> bool",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::device_index": {
    "signature": "() -> c10::DeviceIndex",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::event": {
    "signature": "() -> aclrtEvent",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::query": {
    "signature": "() -> bool",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::record()": {
    "signature": "() -> void",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::record(const c10_npu::NPUStream& stream)": {
    "signature": "(const NPUStream& stream) -> void",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::recordOnce": {
    "signature": "(const NPUStream& stream) -> void",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::block": {
    "signature": "(const NPUStream& stream) -> void",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::elapsed_time": {
    "signature": "(const NPUEvent& other) -> float",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: c10_npu::NPUEvent::synchronize": {
    "signature": "() -> void",
    "file": "torch_npu/csrc/core/npu/NPUEvent.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::NPUGeneratorImpl": {
    "signature": "(c10::DeviceIndex device_index = -1)",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::clone": {
    "signature": "() -> std::shared_ptr<NPUGeneratorImpl>",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::set_current_seed": {
    "signature": "(uint64_t seed) -> void",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::set_offset": {
    "signature": "(uint64_t offset) -> void",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::current_seed": {
    "signature": "() -> uint64_t",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::get_offset": {
    "signature": "() -> uint64_t",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::seed": {
    "signature": "() -> uint64_t",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::set_state": {
    "signature": "(const c10::TensorImpl& new_state) -> void",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::get_state": {
    "signature": "() -> c10::intrusive_ptr<c10::TensorImpl>",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::set_philox_offset_per_thread": {
    "signature": "(uint64_t offset) -> void",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::philox_offset_per_thread": {
    "signature": "() -> uint64_t",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::capture_prologue": {
    "signature": "(int64_t* offset_extragraph) -> void",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::capture_epilogue": {
    "signature": "() -> uint64_t",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::philox_npu_state": {
    "signature": "(uint64_t increment) -> PhiloxNpuState",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::philox_engine_inputs": {
    "signature": "(uint64_t increment) -> std::pair<uint64_t, uint64_t>",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::NPUGeneratorImpl::device_type": {
    "signature": "() -> c10::DeviceType",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::detail::getDefaultNPUGenerator": {
    "signature": "(c10::DeviceIndex device_index = -1) -> at::Generator&",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: at_npu::detail::createNPUGenerator": {
    "signature": "(c10::DeviceIndex device_index = -1) -> at::Generator",
    "file": "torch_npu/csrc/aten/NPUGeneratorImpl.h"
  },
  "torch_c_func: c10_npu::NPUStream::NPUStream(c10::Stream stream)": {
    "signature": "(c10::Stream stream)",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::NPUStream(Unchecked, c10::Stream stream)": {
    "signature": "(Unchecked, c10::Stream stream)",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::~NPUStream": {
    "signature": "()",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::operator==": {
    "signature": "(const c10_npu::NPUStream& other) -> bool",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::operator!=": {
    "signature": "(const c10_npu::NPUStream& other) -> bool",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: aclrtStream": {
    "signature": "() -> operator",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10::Stream": {
    "signature": "() -> operator",
    "namespace": "c10::",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::device_type": {
    "signature": "() -> c10::DeviceType",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::device_index": {
    "signature": "() -> c10::DeviceIndex",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::device": {
    "signature": "() -> c10::Device",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::id": {
    "signature": "() -> c10::StreamId",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::query": {
    "signature": "() -> bool",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::synchronize": {
    "signature": "() -> void",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::stream()": {
    "signature": "() -> aclrtStream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::unwrap": {
    "signature": "() -> c10::Stream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::pack3": {
    "signature": "() -> c10::StreamData3",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::unpack3": {
    "signature": "(c10::StreamId stream_id, c10::DeviceIndex device_index, c10::DeviceType device_type) -> c10_npu::NPUStream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::setDataPreprocessStream": {
    "signature": "(bool is_data_preprocess_stream) -> void",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::isDataPreprocessStream": {
    "signature": "() -> bool",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::NPUStream::stream(const bool need_empty)": {
    "signature": "(const bool need_empty) -> aclrtStream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::getNPUStreamFromPool": {
    "signature": "(c10::DeviceIndex device = -1) -> c10_npu::NPUStream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::getDefaultNPUStream": {
    "signature": "(c10::DeviceIndex device_index = -1) -> c10_npu::NPUStream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::getCurrentNPUStream": {
    "signature": "(c10::DeviceIndex device_index = -1) -> c10_npu::NPUStream",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: c10_npu::setCurrentNPUStream": {
    "signature": "(c10_npu::NPUStream stream) -> void",
    "file": "torch_npu/csrc/core/npu/NPUStream.h"
  },
  "torch_c_func: at_npu::native::OpCommand::OpCommand": {
    "signature": "()",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::~OpCommand": {
    "signature": "()",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Name": {
    "signature": "(const string &name) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::SetCustomHandler": {
    "signature": "(PROC_FUNC func) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::DynamicInputReg": {
    "signature": "(DynamicInputRegFunc func, DyNumAndIndex num_and_index) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Expect": {
    "signature": "(UnifiedResult unified_result) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Input()": {
    "signature": "() -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Input(const at::Tensor &input, const string &descName = \"\", const c10::optional<aclFormat> &sensitive_format = c10::nullopt, const string &realData = \"\")": {
    "signature": "(const at::Tensor &input, const string &descName = \"\", const c10::optional<aclFormat> &sensitive_format = c10::nullopt, const string &realData = \"\") -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::InputWithoutContiguous": {
    "signature": "(const at::Tensor &input, const string &descName = \"\", const string &realData = \"\") -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Input(const c10::ArrayRef<T> &dimListRef, at::IntArrayRef realShape, at::ScalarType toType, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_DEPENDENT, const string& realDtype = \"\", const string& descName = \"\")": {
    "signature": "(const c10::ArrayRef<T> &dimListRef, at::IntArrayRef realShape, at::ScalarType toType, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_DEPENDENT, const string& realDtype = \"\", const string& descName = \"\") -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Input(const c10::IntArrayRef &dimListRef, at::ScalarType toType = at::kLong, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_DEPENDENT, const string& realDtype = \"\", const string& descName = \"\")": {
    "signature": "(const c10::IntArrayRef &dimListRef, at::ScalarType toType = at::kLong, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_DEPENDENT, const string& realDtype = \"\", const string& descName = \"\") -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Input(const c10::ArrayRef<double> &dimListRef, at::IntArrayRef realShape, at::ScalarType toType = at::kDouble, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_DEPENDENT, const string& realDtype = \"\")": {
    "signature": "(const c10::ArrayRef<double> &dimListRef, at::IntArrayRef realShape, at::ScalarType toType = at::kDouble, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_DEPENDENT, const string& realDtype = \"\") -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Input(const c10::Scalar &input, const at::ScalarType type, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_INDEPENDENT)": {
    "signature": "(const c10::Scalar &input, const at::ScalarType type, CompileType compileType = CompileType::MEMORY_HOST_COMPILE_INDEPENDENT) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Inputs": {
    "signature": "(const at::TensorList &inputs) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::InputScalarToNPUTensor": {
    "signature": "(const c10::Scalar& input, const at::ScalarType type) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Output": {
    "signature": "(at::Tensor &output, const string &descName = \"\", const c10::optional<aclFormat> &sensitive_format = c10::nullopt, const string &realType = \"\") -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Attr(const string &name, dataType value)": {
    "signature": "(const string &name, dataType value) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Attr(const string &name, dataType value, bool cond)": {
    "signature": "(const string &name, dataType value, bool cond) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Run": {
    "signature": "() -> void",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Sync(c10::SmallVector<int64_t, N> &sync_index)": {
    "signature": "(c10::SmallVector<int64_t, N> &sync_index) -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: at_npu::native::OpCommand::Sync()": {
    "signature": "() -> OpCommand&",
    "file": "torch_npu/csrc/framework/OpCommand.h"
  },
  "torch_c_func: c10_npu::NPUHooksInterface::getDefaultGenerator": {
    "signature": "(c10::DeviceIndex device_index) -> at::Generator&",
    "file": "torch_npu/csrc/core/npu/NPUHooksInterface.h"
  },
  "torch_c_func: c10_npu::device_count": {
    "signature": "() -> c10::DeviceIndex",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: c10_npu::GetDevice": {
    "signature": "(int32_t *device) -> aclError",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: c10_npu::SetDevice": {
    "signature": "(c10::DeviceIndex device) -> aclError",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: c10_npu::current_device()": {
    "signature": "() -> c10::DeviceIndex",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: c10_npu::set_device": {
    "signature": "(c10::DeviceIndex device) -> void",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: c10_npu::warning_state": {
    "signature": "() -> c10_npu::WarningState&",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: c10_npu::warn_or_error_on_sync": {
    "signature": "() -> void",
    "file": "torch_npu/csrc/core/npu/NPUFunctions.h"
  },
  "torch_c_func: at_npu::native::get_npu_format": {
    "signature": "(const at::Tensor& self) -> int64_t",
    "file": "torch_npu/csrc/core/npu/NPUFormat.h"
  },
  "torch_c_func: at_npu::native::get_npu_storage_sizes": {
    "signature": "(const at::Tensor& self) -> std::vector<int64_t>",
    "file": "torch_npu/csrc/core/npu/NPUFormat.h"
  },
  "torch_c_func: at_npu::native::npu_format_cast": {
    "signature": "(const at::Tensor& self, int64_t acl_format) -> at::Tensor",
    "file": "torch_npu/csrc/core/npu/NPUFormat.h"
  },
  "torch_c_func: at_npu::native::empty_with_format": {
    "signature": "(c10::IntArrayRef sizes, const c10::TensorOptions& options, int64_t format, bool keep_format = false) -> at::Tensor",
    "file": "torch_npu/csrc/core/npu/NPUFormat.h"
  },
  "torch_c_func: c10_npu::c10_npu_get_error_message": {
    "signature": "() -> char *",
    "file": "torch_npu/csrc/core/npu/NPUException.h"
  }
}